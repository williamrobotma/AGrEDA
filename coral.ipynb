{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # CORAL for ST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Creating something like CellDART but just using coral loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from src.da_models.coral import CORAL\n",
    "from src.da_models.datasets import SpotDataset\n",
    "from src.da_models.utils import initialize_weights\n",
    "from src.utils.output_utils import DupStdout, TempFolderHolder\n",
    "from src.utils import data_loading\n",
    "\n",
    "# datetime object containing current date and time\n",
    "script_start_time = datetime.datetime.now(datetime.timezone.utc)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Creating something like CellDART but just using coral loss\"\n",
    ")\n",
    "parser.add_argument(\"--config_fname\", \"-f\", type=str, help=\"Name of the config file to use\")\n",
    "parser.add_argument(\n",
    "    \"--num_workers\", type=int, default=0, help=\"Number of workers to use for dataloaders.\"\n",
    ")\n",
    "parser.add_argument(\"--cuda\", \"-c\", default=None, help=\"gpu index to use\")\n",
    "parser.add_argument(\"--tmpdir\", \"-d\", default=None, help=\"optional temporary model directory\")\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG_FNAME = \"coral.yml\"\n",
    "# NUM_WORKERS = 16\n",
    "# CUDA_INDEX = None\n",
    "\n",
    "args = parser.parse_args()\n",
    "CONFIG_FNAME = args.config_fname\n",
    "CUDA_INDEX = args.cuda\n",
    "NUM_WORKERS = args.num_workers\n",
    "TMP_DIR = args.tmpdir\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_params = {}\n",
    "\n",
    "# torch_params[\"manual_seed\"] = 3583\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_params = {}\n",
    "# # Data path and parameters\n",
    "# data_params[\"data_dir\"] = \"data\"\n",
    "# data_params[\"train_using_all_st_samples\"] = False\n",
    "# data_params[\"n_markers\"] = 20\n",
    "# data_params[\"all_genes\"] = False\n",
    "\n",
    "# # Pseudo-spot parameters\n",
    "# data_params[\"n_spots\"] = 20000\n",
    "# data_params[\"n_mix\"] = 8\n",
    "\n",
    "# # ST spot parameters\n",
    "# data_params[\"st_split\"] = False\n",
    "# data_params[\"sample_id_n\"] = \"151673\"\n",
    "\n",
    "# # Scaler parameter\n",
    "# data_params[\"scaler_name\"] = \"standard\"\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"CORAL\"\n",
    "\n",
    "# model_params = {}\n",
    "\n",
    "# # Model parameters\n",
    "\n",
    "# model_params[\"model_version\"] = \"v1\"\n",
    "\n",
    "# model_params[\"coral_kwargs\"] = {\n",
    "#     \"enc_hidden_layer_sizes\": (1024, 512, 64),\n",
    "#     \"hidden_act\": \"leakyrelu\",\n",
    "#     \"dropout\": 0.5,\n",
    "#     \"batchnorm\": True,\n",
    "#     \"batchnorm_after_act\": True,\n",
    "# }\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_params = {}\n",
    "\n",
    "# train_params[\"batch_size\"] = 512\n",
    "\n",
    "# # Pretraining parameters\n",
    "# # SAMPLE_ID_N = \"151673\"\n",
    "\n",
    "# # train_params[\"initial_train_epochs\"] = 100\n",
    "\n",
    "# # train_params[\"early_stop_crit\"] = 100\n",
    "# # train_params[\"min_epochs\"] = 0.4 * train_params[\"initial_train_epochs\"]\n",
    "\n",
    "# # Adversarial training parameters\n",
    "# train_params[\"epochs\"] = 100\n",
    "# train_params[\"early_stop_crit_adv\"] = train_params[\"epochs\"]\n",
    "# train_params[\"min_epochs_adv\"] = 0.4 * train_params[\"epochs\"]\n",
    "\n",
    "\n",
    "# # train_params[\"enc_lr\"] = 0.0002\n",
    "# # train_params[\"alpha\"] = 2\n",
    "# # train_params[\"dis_loop_factor\"] = 5\n",
    "# # train_params[\"adam_beta1\"] = 0.5\n",
    "# train_params[\"lambda\"] = 100\n",
    "# train_params[\"opt_kwargs\"] = {\"lr\": 0.001, \"weight_decay\": 0.3}\n",
    "# train_params[\"two_step\"] = True\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\n",
    "#     \"torch_params\": torch_params,\n",
    "#     \"data_params\": data_params,\n",
    "#     \"model_params\": model_params,\n",
    "#     \"train_params\": train_params,\n",
    "# }\n",
    "\n",
    "# if not os.path.exists(os.path.join(\"configs\", MODEL_NAME)):\n",
    "#     os.makedirs(os.path.join(\"configs\", MODEL_NAME))\n",
    "\n",
    "# with open(os.path.join(\"configs\", MODEL_NAME, CONFIG_FNAME), \"w\") as f:\n",
    "#     yaml.safe_dump(config, f)\n",
    "\n",
    "with open(os.path.join(\"configs\", MODEL_NAME, CONFIG_FNAME), \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(yaml.safe_dump(config))\n",
    "\n",
    "torch_params = config[\"torch_params\"]\n",
    "data_params = config[\"data_params\"]\n",
    "model_params = config[\"model_params\"]\n",
    "train_params = config[\"train_params\"]\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA_INDEX is not None:\n",
    "    device = torch.device(f\"cuda:{CUDA_INDEX}\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cpu\":\n",
    "    warnings.warn(\"Using CPU\", stacklevel=2)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"manual_seed\" in torch_params:\n",
    "    torch_seed = torch_params[\"manual_seed\"]\n",
    "    torch_seed_path = str(torch_params[\"manual_seed\"])\n",
    "else:\n",
    "    torch_seed = int(script_start_time.timestamp())\n",
    "    # torch_seed_path = script_start_time.strftime(\"%Y-%m-%d_%Hh%Mm%Ss\")\n",
    "    torch_seed_path = \"random\"\n",
    "\n",
    "torch.manual_seed(torch_seed)\n",
    "np.random.seed(torch_seed)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = data_loading.get_model_rel_path(\n",
    "    MODEL_NAME,\n",
    "    model_params[\"model_version\"],\n",
    "    dset=data_params.get(\"dset\", \"dlpfc\"),\n",
    "    sc_id=data_params.get(\"sc_id\", data_loading.DEF_SC_ID),\n",
    "    st_id=data_params.get(\"st_id\", data_loading.DEF_ST_ID),\n",
    "    n_markers=data_params[\"n_markers\"],\n",
    "    all_genes=data_params[\"all_genes\"],\n",
    "    n_mix=data_params[\"n_mix\"],\n",
    "    n_spots=data_params[\"n_spots\"],\n",
    "    st_split=data_params[\"st_split\"],\n",
    "    scaler_name=data_params[\"scaler_name\"],\n",
    "    torch_seed_path=torch_seed_path,\n",
    ")\n",
    "model_folder = os.path.join(\"model\", model_folder)\n",
    "\n",
    "\n",
    "temp_folder_holder = TempFolderHolder()\n",
    "model_folder = temp_folder_holder.set_output_folder(TMP_DIR, model_folder)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_dir = data_loading.get_selected_dir(\n",
    "    data_loading.get_dset_dir(\n",
    "        data_params[\"data_dir\"], dset=data_params.get(\"dset\", \"dlpfc\")\n",
    "    ),\n",
    "    sc_id=data_params.get(\"sc_id\", data_loading.DEF_SC_ID),\n",
    "    st_id=data_params.get(\"st_id\", data_loading.DEF_ST_ID),\n",
    "    n_markers=data_params[\"n_markers\"],\n",
    "    all_genes=data_params[\"all_genes\"],\n",
    ")\n",
    "\n",
    "# Load spatial data\n",
    "mat_sp_d, mat_sp_train, st_sample_id_l = data_loading.load_spatial(\n",
    "    selected_dir,\n",
    "    data_params[\"scaler_name\"],\n",
    "    train_using_all_st_samples=data_params[\"train_using_all_st_samples\"],\n",
    "    st_split=data_params[\"st_split\"],\n",
    ")\n",
    "\n",
    "# Load sc data\n",
    "sc_mix_d, lab_mix_d, sc_sub_dict, sc_sub_dict2 = data_loading.load_sc(\n",
    "    selected_dir,\n",
    "    data_params[\"scaler_name\"],\n",
    "    n_mix=data_params[\"n_mix\"],\n",
    "    n_spots=data_params[\"n_spots\"],\n",
    ")\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Training: Adversarial domain adaptation for cell fraction estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### source dataloaders\n",
    "source_train_set = SpotDataset(sc_mix_d[\"train\"], lab_mix_d[\"train\"])\n",
    "source_val_set = SpotDataset(sc_mix_d[\"val\"], lab_mix_d[\"val\"])\n",
    "source_test_set = SpotDataset(sc_mix_d[\"test\"], lab_mix_d[\"test\"])\n",
    "\n",
    "dataloader_source_train = torch.utils.data.DataLoader(\n",
    "    source_train_set,\n",
    "    batch_size=train_params[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False,\n",
    ")\n",
    "dataloader_source_val = torch.utils.data.DataLoader(\n",
    "    source_val_set,\n",
    "    batch_size=train_params[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False,\n",
    ")\n",
    "dataloader_source_test = torch.utils.data.DataLoader(\n",
    "    source_test_set,\n",
    "    batch_size=train_params[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "### target dataloaders\n",
    "target_train_set_d = {}\n",
    "dataloader_target_train_d = {}\n",
    "if data_params[\"st_split\"]:\n",
    "    target_val_set_d = {}\n",
    "    target_test_set_d = {}\n",
    "\n",
    "    dataloader_target_val_d = {}\n",
    "    dataloader_target_test_d = {}\n",
    "    for sample_id in st_sample_id_l:\n",
    "        target_train_set_d[sample_id] = SpotDataset(mat_sp_d[sample_id][\"train\"])\n",
    "        target_val_set_d[sample_id] = SpotDataset(mat_sp_d[sample_id][\"val\"])\n",
    "        target_test_set_d[sample_id] = SpotDataset(mat_sp_d[sample_id][\"test\"])\n",
    "\n",
    "        dataloader_target_train_d[sample_id] = torch.utils.data.DataLoader(\n",
    "            target_train_set_d[sample_id],\n",
    "            batch_size=train_params[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "        dataloader_target_val_d[sample_id] = torch.utils.data.DataLoader(\n",
    "            target_val_set_d[sample_id],\n",
    "            batch_size=train_params[\"batch_size\"],\n",
    "            shuffle=False,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "        dataloader_target_test_d[sample_id] = torch.utils.data.DataLoader(\n",
    "            target_test_set_d[sample_id],\n",
    "            batch_size=train_params[\"batch_size\"],\n",
    "            shuffle=False,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "\n",
    "else:\n",
    "    target_test_set_d = {}\n",
    "    dataloader_target_test_d = {}\n",
    "\n",
    "    for sample_id in st_sample_id_l:\n",
    "        target_train_set_d[sample_id] = SpotDataset(mat_sp_d[sample_id][\"train\"])\n",
    "        dataloader_target_train_d[sample_id] = torch.utils.data.DataLoader(\n",
    "            target_train_set_d[sample_id],\n",
    "            batch_size=train_params[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "\n",
    "        target_test_set_d[sample_id] = SpotDataset(deepcopy(mat_sp_d[sample_id][\"test\"]))\n",
    "        dataloader_target_test_d[sample_id] = torch.utils.data.DataLoader(\n",
    "            target_test_set_d[sample_id],\n",
    "            batch_size=train_params[\"batch_size\"],\n",
    "            shuffle=False,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "\n",
    "\n",
    "if data_params[\"train_using_all_st_samples\"]:\n",
    "    target_train_set = SpotDataset(mat_sp_train)\n",
    "    dataloader_target_train = torch.utils.data.DataLoader(\n",
    "        target_train_set,\n",
    "        batch_size=train_params[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_clf = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(x, y_true, model):\n",
    "    x = x.to(torch.float32).to(device)\n",
    "    y_true = y_true.to(torch.float32).to(device)\n",
    "\n",
    "    y_pred, _ = model(x)\n",
    "\n",
    "    loss = criterion_clf(y_pred, y_true)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def run_pretrain_epoch(model, dataloader, optimizer=None, scheduler=None, inner=None):\n",
    "    loss_running = []\n",
    "    lr_running = []\n",
    "    mean_weights = []\n",
    "\n",
    "    is_training = model.training and optimizer\n",
    "\n",
    "    for _, batch in enumerate(dataloader):\n",
    "        loss = model_loss(*batch, model)\n",
    "        loss_running.append(loss.item())\n",
    "        mean_weights.append(len(batch))  # we will weight average by batch size later\n",
    "\n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler:\n",
    "                lr_running.append(scheduler.get_last_lr()[-1])\n",
    "                scheduler.step()\n",
    "        if inner:\n",
    "            inner.update(1)\n",
    "    return loss_running, mean_weights, lr_running\n",
    "\n",
    "\n",
    "def compute_acc(dataloader, model):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_running, mean_weights, _ = run_pretrain_epoch(model, dataloader)\n",
    "\n",
    "    return np.average(loss_running, weights=mean_weights)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_params.get(\"pretraining\", False):\n",
    "\n",
    "    pretrain_folder = os.path.join(model_folder, \"pretrain\")\n",
    "\n",
    "    model = CORAL(\n",
    "        inp_dim=sc_mix_d[\"train\"].shape[1],\n",
    "        ncls_source=lab_mix_d[\"train\"].shape[1],\n",
    "        **model_params[\"coral_kwargs\"],\n",
    "    )\n",
    "    model.apply(initialize_weights)\n",
    "    model.to(device)\n",
    "\n",
    "    pre_optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=train_params[\"initial_train_lr\"],\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-07,\n",
    "    )\n",
    "\n",
    "    pre_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        pre_optimizer,\n",
    "        max_lr=train_params[\"initial_train_lr\"],\n",
    "        steps_per_epoch=len(dataloader_source_train),\n",
    "        epochs=train_params[\"initial_train_epochs\"],\n",
    "    )\n",
    "\n",
    "    model.pretraining()\n",
    "\n",
    "    if not os.path.isdir(pretrain_folder):\n",
    "        os.makedirs(pretrain_folder)\n",
    "    # Initialize lists to store loss and accuracy values\n",
    "    loss_history = []\n",
    "    loss_history_val = []\n",
    "\n",
    "    loss_history_running = []\n",
    "\n",
    "    lr_history_running = []\n",
    "\n",
    "    # Early Stopping\n",
    "    best_loss_val = np.inf\n",
    "    early_stop_count = 0\n",
    "\n",
    "    # Train\n",
    "    with DupStdout().dup_to_file(os.path.join(pretrain_folder, \"log.txt\"), \"w\") as f_log:\n",
    "        print(\"Start pretrain...\")\n",
    "        outer = tqdm(total=train_params[\"initial_train_epochs\"], desc=\"Epochs\", position=0)\n",
    "        inner = tqdm(total=len(dataloader_source_train), desc=f\"Batch\", position=1)\n",
    "\n",
    "        tqdm.write(\" Epoch | Train Loss | Val Loss   | Next LR    \")\n",
    "        tqdm.write(\"----------------------------------------------\")\n",
    "        checkpoint = {\n",
    "            \"epoch\": -1,\n",
    "            \"model\": model,\n",
    "            \"optimizer\": pre_optimizer,\n",
    "            \"scheduler\": pre_scheduler,\n",
    "            # 'scaler': scaler\n",
    "        }\n",
    "        for epoch in range(train_params[\"initial_train_epochs\"]):\n",
    "            inner.refresh()  # force print final state\n",
    "            inner.reset()  # reuse bar\n",
    "            checkpoint[\"epoch\"] = epoch\n",
    "\n",
    "            # Train mode\n",
    "            model.train()\n",
    "\n",
    "            loss_running, mean_weights, lr_running = run_pretrain_epoch(\n",
    "                model,\n",
    "                dataloader_source_train,\n",
    "                optimizer=pre_optimizer,\n",
    "                scheduler=pre_scheduler,\n",
    "                inner=inner,\n",
    "            )\n",
    "\n",
    "            loss_history.append(np.average(loss_running, weights=mean_weights))\n",
    "            loss_history_running.append(loss_running)\n",
    "            lr_history_running.append(lr_running)\n",
    "\n",
    "            # Evaluate mode\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                curr_loss_val = compute_acc(dataloader_source_val, model)\n",
    "                loss_history_val.append(curr_loss_val)\n",
    "\n",
    "            # Print the results\n",
    "            outer.update(1)\n",
    "\n",
    "            out_string = (\n",
    "                f\" {epoch:5d} \"\n",
    "                f\"| {loss_history[-1]:<10.8f} \"\n",
    "                f\"| {curr_loss_val:<10.8f} \"\n",
    "                f\"| {pre_scheduler.get_last_lr()[-1]:<10.5} \"\n",
    "            )\n",
    "\n",
    "            # Save the best weights\n",
    "            if curr_loss_val < best_loss_val:\n",
    "                best_loss_val = curr_loss_val\n",
    "                torch.save(checkpoint, os.path.join(pretrain_folder, f\"best_model.pth\"))\n",
    "                early_stop_count = 0\n",
    "\n",
    "                out_string += \"<-- new best val loss\"\n",
    "\n",
    "            tqdm.write(out_string)\n",
    "\n",
    "            # Save checkpoint every 10\n",
    "            if epoch % 10 == 0 or epoch >= train_params[\"initial_train_epochs\"] - 1:\n",
    "                torch.save(checkpoint, os.path.join(pretrain_folder, f\"checkpt{epoch}.pth\"))\n",
    "\n",
    "            # check to see if validation loss has plateau'd\n",
    "            if (\n",
    "                early_stop_count >= train_params[\"early_stop_crit\"]\n",
    "                and epoch >= train_params[\"min_epochs\"] - 1\n",
    "            ):\n",
    "                tqdm.write(f\"Validation loss plateaued after {early_stop_count} at epoch {epoch}\")\n",
    "                torch.save(checkpoint, os.path.join(pretrain_folder, f\"earlystop{epoch}.pth\"))\n",
    "                break\n",
    "\n",
    "            early_stop_count += 1\n",
    "\n",
    "    lr_history_running[-1].append(pre_scheduler.get_last_lr()[-1])\n",
    "\n",
    "    # Save final model\n",
    "    best_checkpoint = torch.load(os.path.join(pretrain_folder, f\"best_model.pth\"))\n",
    "    torch.save(best_checkpoint, os.path.join(pretrain_folder, f\"final_model.pth\"))\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_params.get(\"pretraining\", False):\n",
    "\n",
    "    best_checkpoint = torch.load(os.path.join(pretrain_folder, f\"final_model.pth\"))\n",
    "\n",
    "    best_epoch = best_checkpoint[\"epoch\"]\n",
    "    best_loss_val = loss_history_val[best_epoch]\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, sharex=True, figsize=(6, 4), layout=\"constrained\")\n",
    "\n",
    "    axs[0].plot(*format_iters(loss_history_running), label=\"Training\", linewidth=0.5)\n",
    "    axs[0].plot(loss_history_val, label=\"Validation\")\n",
    "    axs[0].axvline(best_epoch, color=\"tab:green\")\n",
    "\n",
    "    axs[0].set_ylim(bottom=0)\n",
    "    axs[0].grid(which=\"major\")\n",
    "    axs[0].minorticks_on()\n",
    "    axs[0].grid(which=\"minor\", alpha=0.2)\n",
    "\n",
    "    axs[0].text(\n",
    "        x=best_epoch + (2 if best_epoch < len(loss_history) * 0.75 else -2),\n",
    "        y=max(loss_history + loss_history_val) * 0.5,\n",
    "        s=f\"Best val. loss:\\n{best_loss_val:.4f} at epoch {best_epoch}\",\n",
    "        ha=\"left\" if best_epoch < len(loss_history) * 0.75 else \"right\",\n",
    "        size=\"x-small\",\n",
    "    )\n",
    "\n",
    "    # axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_title(\"Cross-Entropy Loss\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    # lr history\n",
    "    iters_by_epoch, lr_history_running_flat = format_iters(lr_history_running, startpoint=True)\n",
    "    axs[1].plot(iters_by_epoch, lr_history_running_flat)\n",
    "    axs[1].axvline(best_checkpoint[\"epoch\"], ymax=2, clip_on=False, color=\"tab:green\")\n",
    "\n",
    "    axs[1].set_ylim(bottom=0)\n",
    "    axs[1].grid(which=\"major\")\n",
    "    axs[1].minorticks_on()\n",
    "    axs[1].grid(which=\"minor\", alpha=0.2)\n",
    "\n",
    "    best_epoch_idx = np.where(iters_by_epoch == best_epoch)[0][0]\n",
    "    axs[1].text(\n",
    "        x=best_epoch + (2 if best_epoch < len(loss_history) * 0.75 else -2),\n",
    "        y=np.median(lr_history_running_flat),\n",
    "        s=f\"LR:\\n{lr_history_running_flat[best_epoch_idx]:.4} at epoch {best_epoch}\",\n",
    "        ha=\"left\" if best_epoch < len(loss_history) * 0.75 else \"right\",\n",
    "        size=\"x-small\",\n",
    "    )\n",
    "\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_title(\"Learning Rate\")\n",
    "\n",
    "    plt.savefig(os.path.join(pretrain_folder, \"train_plots.png\"), bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show(block=False)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Adversarial Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advtrain_folder = os.path.join(model_folder, \"advtrain\")\n",
    "\n",
    "if not os.path.isdir(advtrain_folder):\n",
    "    os.makedirs(advtrain_folder)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_dis = coral_loss  # lambda s, t: coral_loss(torch.exp(s), torch.exp(t))\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_adv_loss(\n",
    "    x_source,\n",
    "    x_target,\n",
    "    y_source,\n",
    "    model,\n",
    "    two_step=False,\n",
    "    source_first=True,\n",
    "    optimizer=None,\n",
    "):\n",
    "\n",
    "    if two_step:\n",
    "        if source_first:\n",
    "            y_pred_source, logits_source = model(x_source)\n",
    "            y_pred_target, logits_target = model(x_target)\n",
    "        else:\n",
    "            y_pred_target, logits_source = model(x_target)\n",
    "            y_pred_source, logits_target = model(x_source)\n",
    "    else:\n",
    "        y_pred, logits = model(torch.cat([x_source, x_target], dim=0))\n",
    "        y_pred_source, y_pred_target = torch.split(y_pred, [len(x_source), len(x_target)])\n",
    "        logits_source, logits_target = torch.split(logits, [len(x_source), len(x_target)])\n",
    "\n",
    "    loss_clf = criterion_clf(y_pred_source, y_source)\n",
    "    loss_dis = criterion_dis(logits_source, logits_target)\n",
    "    loss = loss_clf + loss_dis * train_params[\"lambda\"]\n",
    "    update_weights(optimizer, loss)\n",
    "\n",
    "    return loss, loss_dis, loss_clf\n",
    "\n",
    "\n",
    "# def target_step(x_target, model, optimizer):\n",
    "#     if optimizer is not None:\n",
    "#         clf_rq_bak = dict(\n",
    "#             (\n",
    "#                 (name, param.requires_grad)\n",
    "#                 for name, param in model.clf.named_parameters()\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#     y_dis_target, y_dis_target_pred, loss_dis_target = target_pass(x_target, model)\n",
    "#     loss = loss_dis_target * train_params[\"lambda\"]\n",
    "\n",
    "#     if optimizer is not None:\n",
    "#         update_weights(optimizer, loss)\n",
    "#         for name, param in model.clf.named_parameters():\n",
    "#             param.requires_grad = clf_rq_bak[name]\n",
    "\n",
    "#     return y_dis_target, y_dis_target_pred, loss_dis_target\n",
    "\n",
    "\n",
    "# def source_step(x_source, y_source, model, optimizer):\n",
    "#     y_dis_source, y_dis_source_pred, loss_clf, loss_dis_source = source_pass(\n",
    "#         x_source, y_source, model\n",
    "#     )\n",
    "#     loss = loss_clf + loss_dis_source * train_params[\"lambda\"]\n",
    "#     update_weights(optimizer, loss)\n",
    "#     return y_dis_source, y_dis_source_pred, loss_clf, loss_dis_source\n",
    "\n",
    "\n",
    "def update_weights(optimizer, loss):\n",
    "    if optimizer is not None:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "# def target_pass(x_target, model):\n",
    "#     y_dis_target = torch.ones(\n",
    "#         x_target.shape[0], device=device, dtype=x_target.dtype\n",
    "#     ).view(-1, 1)\n",
    "#     _, y_dis_target_pred = model(x_target, clf=False)\n",
    "#     loss_dis_target = criterion_dis(y_dis_target_pred, y_dis_target)\n",
    "#     return y_dis_target, y_dis_target_pred, loss_dis_target\n",
    "\n",
    "\n",
    "# def source_pass(x_source, y_source, model):\n",
    "#     y_dis_source = torch.zeros(\n",
    "#         x_source.shape[0], device=device, dtype=x_source.dtype\n",
    "#     ).view(-1, 1)\n",
    "#     y_clf, y_dis_source_pred = model(x_source, clf=True)\n",
    "#     loss_clf = criterion_clf(y_clf, y_source)\n",
    "#     loss_dis_source = criterion_dis(y_dis_source_pred, y_dis_source)\n",
    "#     return y_dis_source, y_dis_source_pred, loss_clf, loss_dis_source\n",
    "\n",
    "\n",
    "def run_epoch(\n",
    "    dataloader_source,\n",
    "    dataloader_target,\n",
    "    model,\n",
    "    tqdm_bar=None,\n",
    "    scheduler=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    results_running = {\n",
    "        \"clf\": {\"loss\": [], \"weights\": []},\n",
    "        \"dis\": {\"loss\": [], \"weights\": []},\n",
    "        \"ovr\": {\"loss\": [], \"weights\": []},\n",
    "    }\n",
    "\n",
    "    if scheduler is not None:\n",
    "        results_running[\"ovr\"][\"lr\"] = []\n",
    "    n_iters = max(len(dataloader_source), len(dataloader_target))\n",
    "\n",
    "    s_iter = iter(dataloader_source)\n",
    "    t_iter = iter(dataloader_target)\n",
    "    for i in range(n_iters):\n",
    "        try:\n",
    "            x_source, y_source = next(s_iter)\n",
    "        except StopIteration:\n",
    "            s_iter = iter(dataloader_source)\n",
    "            x_source, y_source = next(s_iter)\n",
    "        try:\n",
    "            x_target, _ = next(t_iter)\n",
    "        except StopIteration:\n",
    "            t_iter = iter(dataloader_target)\n",
    "            x_target, _ = next(t_iter)\n",
    "\n",
    "        x_source = x_source.to(torch.float32).to(device)\n",
    "        x_target = x_target.to(torch.float32).to(device)\n",
    "        y_source = y_source.to(torch.float32).to(device)\n",
    "\n",
    "        loss, loss_dis, loss_clf = model_adv_loss(x_source, x_target, y_source, model, **kwargs)\n",
    "\n",
    "        results_running[\"dis\"][\"loss\"].append(loss_dis.item())\n",
    "        results_running[\"clf\"][\"loss\"].append(loss_clf.item())\n",
    "        results_running[\"ovr\"][\"loss\"].append(loss.item())\n",
    "\n",
    "        results_running[\"dis\"][\"weights\"].append((len(x_source) + len(x_target)) / 2)\n",
    "        results_running[\"clf\"][\"weights\"].append(len(x_source))\n",
    "        results_running[\"ovr\"][\"weights\"].append(\n",
    "            (len(x_source) + len(x_target)) / 2 + len(x_source)\n",
    "        )\n",
    "        if scheduler is not None:\n",
    "            results_running[\"ovr\"][\"lr\"].append(scheduler.get_last_lr()[-1])\n",
    "            scheduler.step()\n",
    "\n",
    "        if tqdm_bar is not None:\n",
    "            tqdm_bar.update(1)\n",
    "\n",
    "    return results_running\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversarial_iters(\n",
    "    model,\n",
    "    save_folder,\n",
    "    dataloader_source_train,\n",
    "    dataloader_source_val,\n",
    "    dataloader_target_train,\n",
    "):\n",
    "    model.to(device)\n",
    "    model.advtraining()\n",
    "\n",
    "    max_len_dataloader = max(len(dataloader_source_train), len(dataloader_target_train))\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), **train_params[\"opt_kwargs\"])\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    #     optimizer, **train_params[\"plateau_kwargs\"]\n",
    "    # )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=train_params[\"opt_kwargs\"][\"lr\"],\n",
    "        steps_per_epoch=max_len_dataloader,\n",
    "        epochs=train_params[\"epochs\"],\n",
    "    )\n",
    "\n",
    "    # iters = -(max_len_dataloader // -(1 + DIS_LOOP_FACTOR))  # ceiling divide\n",
    "\n",
    "    iters_val = max(len(dataloader_source_val), len(dataloader_target_train))\n",
    "\n",
    "    results_template = {\n",
    "        \"clf\": {\"loss\": [], \"weights\": []},\n",
    "        \"dis\": {\"loss\": [], \"weights\": []},\n",
    "        \"ovr\": {\"loss\": [], \"lr\": [], \"weights\": []},\n",
    "    }\n",
    "    results_history = deepcopy(results_template)\n",
    "    results_history_val = deepcopy(results_template)\n",
    "    results_history_running = deepcopy(results_template)\n",
    "\n",
    "    # Early Stopping\n",
    "    best_loss_val = np.inf\n",
    "    early_stop_count = 0\n",
    "    with DupStdout().dup_to_file(os.path.join(save_folder, \"log.txt\"), \"w\") as f_log:\n",
    "        # Train\n",
    "        print(\"Start adversarial training...\")\n",
    "        outer = tqdm(total=train_params[\"epochs\"], desc=\"Epochs\", position=0)\n",
    "        inner1 = tqdm(total=max_len_dataloader, desc=f\"Batch\", position=1)\n",
    "\n",
    "        tqdm.write(\" Epoch || KLDiv.          || Coral           || Overall         || Next LR    \")\n",
    "        tqdm.write(\"       || Train  | Val.   || Train  | Val.   || Train  | Val.   ||            \")\n",
    "        tqdm.write(\"------------------------------------------------------------------------------\")\n",
    "        checkpoint = {\n",
    "            \"epoch\": -1,\n",
    "            \"model\": model,\n",
    "            \"optimizer\": optimizer,\n",
    "            \"scheduler\": scheduler,\n",
    "        }\n",
    "        for epoch in range(train_params[\"epochs\"]):\n",
    "            inner1.refresh()  # force print final state\n",
    "            inner1.reset()  # reuse bar\n",
    "\n",
    "            checkpoint[\"epoch\"] = epoch\n",
    "\n",
    "            results_running = run_epoch(\n",
    "                dataloader_source_train,\n",
    "                dataloader_target_train,\n",
    "                model,\n",
    "                tqdm_bar=inner1,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                two_step=train_params.get(\"two_step\", False),\n",
    "                source_first=train_params.get(\"source_first\", True),\n",
    "            )\n",
    "\n",
    "            for goal_k in results_running:\n",
    "                for metric_k in results_running[goal_k]:\n",
    "                    results_history[goal_k][metric_k].append(\n",
    "                        np.average(\n",
    "                            results_running[goal_k][metric_k],\n",
    "                            weights=results_running[goal_k][\"weights\"],\n",
    "                        )\n",
    "                    )\n",
    "            for goal_k in results_running:\n",
    "                for metric_k in results_running[goal_k]:\n",
    "                    results_history_running[goal_k][metric_k].append(\n",
    "                        results_running[goal_k][metric_k]\n",
    "                    )\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                results_val = run_epoch(dataloader_source_val, dataloader_target_train, model)\n",
    "            for goal_k in results_val:\n",
    "                for metric_k in results_val[goal_k]:\n",
    "                    results_history_val[goal_k][metric_k].append(\n",
    "                        np.average(\n",
    "                            results_val[goal_k][metric_k],\n",
    "                            weights=results_val[goal_k][\"weights\"],\n",
    "                        )\n",
    "                    )\n",
    "            # Print the results\n",
    "            outer.update(1)\n",
    "\n",
    "            out_string = (\n",
    "                f\" {epoch:5d} \"\n",
    "                f\"|| {results_history['clf']['loss'][-1]:6.4f} \"\n",
    "                f\"| {results_history_val['clf']['loss'][-1]:6.4f} \"\n",
    "                f\"|| {results_history['dis']['loss'][-1]:6.4f} \"\n",
    "                f\"| {results_history_val['dis']['loss'][-1]:6.4f} \"\n",
    "                f\"|| {results_history['ovr']['loss'][-1]:6.4f} \"\n",
    "                f\"| {results_history_val['ovr']['loss'][-1]:6.4f} \"\n",
    "                f\"|| {scheduler.get_last_lr()[-1]:<10.5} \"\n",
    "            )\n",
    "\n",
    "            better_val_loss = results_history_val[\"ovr\"][\"loss\"][-1] < best_loss_val\n",
    "            if better_val_loss:\n",
    "                best_loss_val = results_history_val[\"ovr\"][\"loss\"][-1]\n",
    "                torch.save(checkpoint, os.path.join(save_folder, f\"best_model.pth\"))\n",
    "                early_stop_count = 0\n",
    "                out_string += f\"<-- new best val loss\"\n",
    "\n",
    "            tqdm.write(out_string)\n",
    "\n",
    "            early_stop_count += 1\n",
    "\n",
    "    results_history_running[\"ovr\"][\"lr\"][-1].append(scheduler.get_last_lr()[-1])\n",
    "    # Save final model\n",
    "    best_checkpoint = torch.load(os.path.join(save_folder, f\"best_model.pth\"))\n",
    "    torch.save(best_checkpoint, os.path.join(save_folder, f\"final_model.pth\"))\n",
    "\n",
    "    results_history_out = {\n",
    "        \"train\": {\"avg\": results_history, \"running\": results_history_running},\n",
    "        \"val\": {\"avg\": results_history_val},\n",
    "    }\n",
    "    # Save results\n",
    "    with open(os.path.join(save_folder, f\"results_history_out.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(results_history_out, f)\n",
    "\n",
    "    return results_history_out\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(save_folder, results_history_out=None):\n",
    "    if results_history_out is None:\n",
    "        with open(os.path.join(save_folder, f\"results_history_out.pkl\"), \"rb\") as f:\n",
    "            results_history_out = pickle.load(results_history_out, f)\n",
    "    results_history = results_history_out[\"train\"][\"avg\"]\n",
    "    results_history_running = results_history_out[\"train\"][\"running\"]\n",
    "    results_history_val = results_history_out[\"val\"][\"avg\"]\n",
    "\n",
    "    n_epochs = len(results_history[\"ovr\"][\"loss\"])\n",
    "    best_checkpoint = torch.load(os.path.join(save_folder, f\"final_model.pth\"))\n",
    "    best_epoch = best_checkpoint[\"epoch\"]\n",
    "\n",
    "    best_coral_loss = results_history_val[\"dis\"][\"loss\"][best_epoch]\n",
    "    best_kld_loss = results_history_val[\"clf\"][\"loss\"][best_epoch]\n",
    "    best_overall_loss = results_history_val[\"ovr\"][\"loss\"][best_epoch]\n",
    "\n",
    "    fig, axs = plt.subplots(4, 1, sharex=True, figsize=(9, 12), layout=\"constrained\")\n",
    "\n",
    "    x_text = best_epoch + (2 if best_epoch < n_epochs * 0.75 else -2)\n",
    "    ha_text = \"left\" if best_epoch < n_epochs * 0.75 else \"right\"\n",
    "\n",
    "    # Coral\n",
    "    axs[0].plot(\n",
    "        *(x_y_coral_iters := format_iters(results_history_running[\"dis\"][\"loss\"])),\n",
    "        label=\"training\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    axs[0].plot(results_history_val[\"dis\"][\"loss\"], label=\"validation\")\n",
    "    axs[1].axvline(best_epoch, color=\"tab:green\")\n",
    "\n",
    "    axs[0].set_ylim(bottom=0, top=max(results_history_val[\"dis\"][\"loss\"]))\n",
    "    axs[0].grid(which=\"major\")\n",
    "    axs[0].minorticks_on()\n",
    "    axs[0].grid(which=\"minor\", alpha=0.2)\n",
    "    axs[0].text(\n",
    "        x=x_text,\n",
    "        y=max(results_history_val[\"dis\"][\"loss\"]) * 0.5,\n",
    "        s=f\"CORAL val. loss:\\n{best_coral_loss:.4f} at epoch {best_epoch}\",\n",
    "        ha=ha_text,\n",
    "        size=\"x-small\",\n",
    "    )\n",
    "    axs[0].set_title(\"CORAL Loss\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    # KLDiv\n",
    "    axs[1].plot(\n",
    "        *(x_y_kld_iters := format_iters(results_history_running[\"clf\"][\"loss\"])),\n",
    "        label=\"training\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    axs[1].plot(results_history_val[\"clf\"][\"loss\"], label=\"validation\")\n",
    "    axs[1].axvline(best_epoch, ymax=2, clip_on=False, color=\"tab:green\")\n",
    "\n",
    "    axs[1].set_ylim(bottom=0, top=max(results_history_val[\"clf\"][\"loss\"]))\n",
    "    axs[1].grid(which=\"major\")\n",
    "    axs[1].minorticks_on()\n",
    "    axs[1].grid(which=\"minor\", alpha=0.2)\n",
    "    axs[1].text(\n",
    "        x=x_text,\n",
    "        y=max(results_history_val[\"clf\"][\"loss\"]) * 0.5,\n",
    "        s=f\"KLD val. loss:\\n{best_kld_loss:.4f} at epoch {best_epoch}\",\n",
    "        ha=ha_text,\n",
    "        size=\"x-small\",\n",
    "    )\n",
    "    axs[1].set_title(\"KL-Divergence Loss\")\n",
    "    axs[1].legend()\n",
    "\n",
    "    # Overall\n",
    "    axs[2].plot(\n",
    "        *(x_y_ovr_iters := format_iters(results_history_running[\"ovr\"][\"loss\"])),\n",
    "        label=\"training\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    axs[2].plot(results_history_val[\"ovr\"][\"loss\"], label=\"validation\")\n",
    "    axs[2].axvline(best_epoch, ymax=2, clip_on=False, color=\"tab:green\")\n",
    "\n",
    "    axs[2].set_ylim(bottom=0, top=max(results_history_val[\"ovr\"][\"loss\"]))\n",
    "    axs[2].grid(which=\"major\")\n",
    "    axs[2].minorticks_on()\n",
    "    axs[2].grid(which=\"minor\", alpha=0.2)\n",
    "    axs[2].text(\n",
    "        x=x_text,\n",
    "        y=max(results_history_val[\"ovr\"][\"loss\"]) * 0.5,\n",
    "        s=f\"Best overall val. loss:\\n{best_overall_loss:.4f} at epoch {best_epoch}\",\n",
    "        ha=ha_text,\n",
    "        size=\"x-small\",\n",
    "    )\n",
    "    axs[2].set_title(\"Overall Loss\")\n",
    "    axs[2].legend()\n",
    "\n",
    "    # lr history\n",
    "    iters_by_epoch, lr_history_running_flat = format_iters(\n",
    "        results_history_running[\"ovr\"][\"lr\"], startpoint=True\n",
    "    )\n",
    "    axs[3].plot(iters_by_epoch, lr_history_running_flat)\n",
    "    axs[3].axvline(best_epoch, ymax=2, clip_on=False, color=\"tab:green\")\n",
    "\n",
    "    axs[3].set_ylim(bottom=0)\n",
    "    axs[3].grid(which=\"major\")\n",
    "    axs[3].minorticks_on()\n",
    "    axs[3].grid(which=\"minor\", alpha=0.2)\n",
    "\n",
    "    best_epoch_idx = np.where(iters_by_epoch == best_epoch)[0][0]\n",
    "    axs[3].text(\n",
    "        x=x_text,\n",
    "        y=np.median(lr_history_running_flat),\n",
    "        s=f\"LR:\\n{lr_history_running_flat[best_epoch_idx]:.4} at epoch {best_epoch}\",\n",
    "        ha=ha_text,\n",
    "        size=\"x-small\",\n",
    "    )\n",
    "\n",
    "    axs[3].set_xlabel(\"Epoch\")\n",
    "    axs[3].set_title(\"Learning Rate\")\n",
    "\n",
    "    if save_folder is not None:\n",
    "        if not os.path.exists(save_folder):\n",
    "            os.makedirs(save_folder)\n",
    "        plt.savefig(os.path.join(save_folder, \"adv_train.png\"))\n",
    "\n",
    "    plt.show(block=False)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# st_sample_id_l = [SAMPLE_ID_N]\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_params[\"train_using_all_st_samples\"]:\n",
    "    print(f\"Adversarial training for all ST slides\")\n",
    "    save_folder = advtrain_folder\n",
    "\n",
    "    model = CORAL(\n",
    "        sc_mix_d[\"train\"].shape[1],\n",
    "        ncls_source=lab_mix_d[\"train\"].shape[1],\n",
    "        **model_params[\"coral_kwargs\"],\n",
    "    )\n",
    "    model.apply(initialize_weights)\n",
    "    if train_params.get(\"pretraining\", False):\n",
    "        best_pre_checkpoint = torch.load(os.path.join(pretrain_folder, f\"final_model.pth\"))\n",
    "        model.load_state_dict(best_pre_checkpoint[\"model\"].state_dict())\n",
    "    model.to(device)\n",
    "\n",
    "    model.advtraining()\n",
    "\n",
    "    train_adversarial_iters(\n",
    "        model,\n",
    "        save_folder,\n",
    "        dataloader_source_train,\n",
    "        dataloader_source_val,\n",
    "        dataloader_target_train,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    for sample_id in st_sample_id_l:\n",
    "        print(f\"Adversarial training for ST slide {sample_id}: \")\n",
    "\n",
    "        save_folder = os.path.join(advtrain_folder, sample_id)\n",
    "        if not os.path.isdir(save_folder):\n",
    "            os.makedirs(save_folder)\n",
    "\n",
    "        model = CORAL(\n",
    "            sc_mix_d[\"train\"].shape[1],\n",
    "            ncls_source=lab_mix_d[\"train\"].shape[1],\n",
    "            **model_params[\"coral_kwargs\"],\n",
    "        )\n",
    "        model.apply(initialize_weights)\n",
    "\n",
    "        if train_params.get(\"pretraining\", False):\n",
    "            best_pre_checkpoint = torch.load(os.path.join(pretrain_folder, f\"final_model.pth\"))\n",
    "            model.load_state_dict(best_pre_checkpoint[\"model\"].state_dict())\n",
    "        model.to(device)\n",
    "\n",
    "        model.advtraining()\n",
    "\n",
    "        print(model)\n",
    "        results = train_adversarial_iters(\n",
    "            model,\n",
    "            save_folder,\n",
    "            dataloader_source_train,\n",
    "            dataloader_source_val,\n",
    "            dataloader_target_train_d[sample_id],\n",
    "        )\n",
    "        plot_results(save_folder, results)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(model_folder, \"config.yml\"), \"w\") as f:\n",
    "    yaml.safe_dump(config, f)\n",
    "\n",
    "temp_folder_holder.copy_out()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Script run time:\", datetime.datetime.now(datetime.timezone.utc) - script_start_time)\n",
    ""
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 }
}