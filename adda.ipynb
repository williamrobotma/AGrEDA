{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # ADDA for ST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Creating something like CellDART but it actually follows Adda in PyTorch as a first step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from src.da_models.adda import ADDAST\n",
    "from src.da_models.datasets import SpotDataset\n",
    "from src.da_models.utils import initialize_weights, set_requires_grad\n",
    "from src.utils.data_loading import (\n",
    "    get_model_rel_path,\n",
    "    get_selected_dir,\n",
    "    load_sc,\n",
    "    load_spatial,\n",
    ")\n",
    "from src.utils.output_utils import DupStdout, TempFolderHolder\n",
    "from src.utils.evaluation import format_iters\n",
    "\n",
    "script_start_time = datetime.datetime.now(datetime.timezone.utc)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Creating something like CellDART but it actually follows Adda in PyTorch as a first step\"\n",
    ")\n",
    "parser.add_argument(\"--config_fname\", \"-f\", help=\"Name of the config file to use\")\n",
    "parser.add_argument(\n",
    "    \"--num_workers\", type=int, default=0, help=\"Number of workers to use for dataloaders.\"\n",
    ")\n",
    "parser.add_argument(\"--cuda\", \"-c\", default=None, help=\"gpu index to use\")\n",
    "parser.add_argument(\"--tmpdir\", \"-d\", default=None, help=\"optional temporary model directory\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args()\n",
    "CONFIG_FNAME = args.config_fname\n",
    "CUDA_INDEX = args.cuda\n",
    "NUM_WORKERS = args.num_workers\n",
    "TMP_DIR = args.tmpdir\n",
    "\n",
    "# CONFIG_FNAME = \"celldart1_bnfix.yml\"\n",
    "# NUM_WORKERS = 0\n",
    "# CUDA_INDEX = None\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_params = {}\n",
    "\n",
    "# torch_params[\"manual_seed\"] = 72\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_params = {}\n",
    "# # Data path and parameters\n",
    "# data_params[\"data_dir\"] = \"data\"\n",
    "# data_params[\"train_using_all_st_samples\"] = False\n",
    "# data_params[\"n_markers\"] = 20\n",
    "# data_params[\"all_genes\"] = False\n",
    "\n",
    "# # Pseudo-spot parameters\n",
    "# data_params[\"n_spots\"] = 20000\n",
    "# data_params[\"n_mix\"] = 8\n",
    "\n",
    "# # ST spot parameters\n",
    "# data_params[\"st_split\"] = False\n",
    "# data_params[\"sample_id_n\"] = \"151673\"\n",
    "\n",
    "# # Scaler parameter\n",
    "# data_params[\"scaler_name\"] = \"celldart\"\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_params = {}\n",
    "\n",
    "# # Model parameters\n",
    "MODEL_NAME = \"ADDA\"\n",
    "# model_params[\"model_version\"] = \"celldart_bnfix\"\n",
    "\n",
    "# model_params[\"adda_kwargs\"] = {\n",
    "#     \"emb_dim\": 64,\n",
    "#     \"bn_momentum\": 0.01,\n",
    "# }\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_params = {}\n",
    "\n",
    "# train_params[\"batch_size\"] = 1024\n",
    "\n",
    "# # Pretraining parameters\n",
    "# # SAMPLE_ID_N = \"151673\"\n",
    "\n",
    "# train_params[\"initial_train_epochs\"] = 100\n",
    "\n",
    "# train_params[\"early_stop_crit\"] = 100\n",
    "# train_params[\"min_epochs\"] = 0.4 * train_params[\"initial_train_epochs\"]\n",
    "\n",
    "# # Adversarial training parameters\n",
    "# train_params[\"epochs\"] = 200\n",
    "# train_params[\"early_stop_crit_adv\"] = train_params[\"epochs\"]\n",
    "# train_params[\"min_epochs_adv\"] =  0.4 * train_params[\"epochs\"]\n",
    "\n",
    "\n",
    "# train_params[\"enc_lr\"] = 0.0002\n",
    "# train_params[\"alpha\"] = 2\n",
    "# train_params[\"dis_loop_factor\"] = 5\n",
    "# train_params[\"adam_beta1\"] = 0.5\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\n",
    "#     \"torch_params\": torch_params,\n",
    "#     \"data_params\": data_params,\n",
    "#     \"model_params\": model_params,\n",
    "#     \"train_params\": train_params,\n",
    "# }\n",
    "\n",
    "# if not os.path.exists(os.path.join(\"configs\", MODEL_NAME)):\n",
    "#     os.makedirs(os.path.join(\"configs\", MODEL_NAME))\n",
    "\n",
    "# with open(os.path.join(\"configs\", MODEL_NAME, CONFIG_FNAME), \"w\") as f:\n",
    "#     yaml.safe_dump(config, f)\n",
    "\n",
    "with open(os.path.join(\"configs\", MODEL_NAME, CONFIG_FNAME), \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "torch_params = config[\"torch_params\"]\n",
    "data_params = config[\"data_params\"]\n",
    "model_params = config[\"model_params\"]\n",
    "train_params = config[\"train_params\"]\n",
    "\n",
    "if not \"pretraining\" in train_params:\n",
    "    train_params[\"pretraining\"] = True\n",
    "    with open(os.path.join(\"configs\", MODEL_NAME, CONFIG_FNAME), \"w\") as f:\n",
    "        yaml.safe_dump(config, f)\n",
    "\n",
    "print(yaml.safe_dump(config))\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA_INDEX is not None:\n",
    "    device = torch.device(f\"cuda:{CUDA_INDEX}\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cpu\":\n",
    "    warnings.warn(\"Using CPU\", stacklevel=2)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"manual_seed\" in torch_params:\n",
    "    torch_seed = torch_params[\"manual_seed\"]\n",
    "    torch_seed_path = str(torch_params[\"manual_seed\"])\n",
    "else:\n",
    "    torch_seed = int(script_start_time.timestamp())\n",
    "    # torch_seed_path = script_start_time.strftime(\"%Y-%m-%d_%Hh%Mm%Ss\")\n",
    "    torch_seed_path = \"random\"\n",
    "\n",
    "torch.manual_seed(torch_seed)\n",
    "np.random.seed(torch_seed)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = get_model_rel_path(\n",
    "    MODEL_NAME,\n",
    "    model_params[\"model_version\"],\n",
    "    scaler_name=data_params[\"scaler_name\"],\n",
    "    n_markers=data_params[\"n_markers\"],\n",
    "    all_genes=data_params[\"all_genes\"],\n",
    "    n_mix=data_params[\"n_mix\"],\n",
    "    n_spots=data_params[\"n_spots\"],\n",
    "    st_split=data_params[\"st_split\"],\n",
    "    torch_seed_path=torch_seed_path,\n",
    ")\n",
    "model_folder = os.path.join(\"model\", model_folder)\n",
    "\n",
    "temp_folder_holder = TempFolderHolder()\n",
    "model_folder = temp_folder_holder.set_output_folder(TMP_DIR, model_folder)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_dir = get_selected_dir(\n",
    "    data_params[\"data_dir\"], data_params[\"n_markers\"], data_params[\"all_genes\"]\n",
    ")\n",
    "\n",
    "# Load spatial data\n",
    "mat_sp_d, mat_sp_train, st_sample_id_l = load_spatial(\n",
    "    selected_dir,\n",
    "    data_params[\"scaler_name\"],\n",
    "    train_using_all_st_samples=data_params[\"train_using_all_st_samples\"],\n",
    "    st_split=data_params[\"st_split\"],\n",
    ")\n",
    "\n",
    "# Load sc data\n",
    "sc_mix_d, lab_mix_d, sc_sub_dict, sc_sub_dict2 = load_sc(\n",
    "    selected_dir,\n",
    "    data_params[\"scaler_name\"],\n",
    "    n_mix=data_params[\"n_mix\"],\n",
    "    n_spots=data_params[\"n_spots\"],\n",
    ")\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Training: Adversarial domain adaptation for cell fraction estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### source dataloaders\n",
    "source_train_set = SpotDataset(sc_mix_d[\"train\"], lab_mix_d[\"train\"])\n",
    "source_val_set = SpotDataset(sc_mix_d[\"val\"], lab_mix_d[\"val\"])\n",
    "source_test_set = SpotDataset(sc_mix_d[\"test\"], lab_mix_d[\"test\"])\n",
    "\n",
    "dataloader_source_train = torch.utils.data.DataLoader(\n",
    "    source_train_set,\n",
    "    batch_size=train_params[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False,\n",
    ")\n",
    "dataloader_source_val = torch.utils.data.DataLoader(\n",
    "    source_val_set,\n",
    "    batch_size=train_params[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False,\n",
    ")\n",
    "dataloader_source_test = torch.utils.data.DataLoader(\n",
    "    source_test_set,\n",
    "    batch_size=train_params[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "### target dataloaders\n",
    "target_train_set_d = {}\n",
    "dataloader_target_train_d = {}\n",
    "if data_params[\"st_split\"]:\n",
    "    target_val_set_d = {}\n",
    "    target_test_set_d = {}\n",
    "\n",
    "    dataloader_target_val_d = {}\n",
    "    dataloader_target_test_d = {}\n",
    "    for sample_id in st_sample_id_l:\n",
    "        target_train_set_d[sample_id] = SpotDataset(mat_sp_d[sample_id][\"train\"])\n",
    "        target_val_set_d[sample_id] = SpotDataset(mat_sp_d[sample_id][\"val\"])\n",
    "        target_test_set_d[sample_id] = SpotDataset(mat_sp_d[sample_id][\"test\"])\n",
    "\n",
    "        dataloader_target_train_d[sample_id] = torch.utils.data.DataLoader(\n",
    "            target_train_set_d[sample_id],\n",
    "            batch_size=train_params[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "        dataloader_target_val_d[sample_id] = torch.utils.data.DataLoader(\n",
    "            target_val_set_d[sample_id],\n",
    "            batch_size=train_params[\"batch_size\"],\n",
    "            shuffle=False,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "        dataloader_target_test_d[sample_id] = torch.utils.data.DataLoader(\n",
    "            target_test_set_d[sample_id],\n",
    "            batch_size=train_params[\"batch_size\"],\n",
    "            shuffle=False,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "\n",
    "else:\n",
    "    target_test_set_d = {}\n",
    "    dataloader_target_test_d = {}\n",
    "\n",
    "    target_train_set_dis_d = {}\n",
    "    dataloader_target_train_dis_d = {}\n",
    "    for sample_id in st_sample_id_l:\n",
    "        target_train_set_d[sample_id] = SpotDataset(mat_sp_d[sample_id][\"train\"])\n",
    "        dataloader_target_train_d[sample_id] = torch.utils.data.DataLoader(\n",
    "            target_train_set_d[sample_id],\n",
    "            batch_size=train_params[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "\n",
    "        target_test_set_d[sample_id] = SpotDataset(deepcopy(mat_sp_d[sample_id][\"test\"]))\n",
    "        dataloader_target_test_d[sample_id] = torch.utils.data.DataLoader(\n",
    "            target_test_set_d[sample_id],\n",
    "            batch_size=train_params[\"batch_size\"],\n",
    "            shuffle=False,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "\n",
    "        target_train_set_dis_d[sample_id] = SpotDataset(deepcopy(mat_sp_d[sample_id][\"train\"]))\n",
    "        dataloader_target_train_dis_d[sample_id] = torch.utils.data.DataLoader(\n",
    "            target_train_set_dis_d[sample_id],\n",
    "            batch_size=train_params[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "\n",
    "if data_params[\"train_using_all_st_samples\"]:\n",
    "    target_train_set = SpotDataset(mat_sp_train)\n",
    "    dataloader_target_train = torch.utils.data.DataLoader(\n",
    "        target_train_set,\n",
    "        batch_size=train_params[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ADDAST(\n",
    "    inp_dim=sc_mix_d[\"train\"].shape[1],\n",
    "    ncls_source=lab_mix_d[\"train\"].shape[1],\n",
    "    is_adda=True,\n",
    "    **model_params[\"adda_kwargs\"],\n",
    ")\n",
    "model.apply(initialize_weights)\n",
    "model.to(device)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_folder = os.path.join(model_folder, \"pretrain\")\n",
    "\n",
    "if not os.path.isdir(pretrain_folder):\n",
    "    os.makedirs(pretrain_folder)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_optimizer = torch.optim.Adam(model.parameters(), lr=0.002, betas=(0.9, 0.999), eps=1e-07)\n",
    "\n",
    "pre_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    pre_optimizer,\n",
    "    max_lr=0.002,\n",
    "    steps_per_epoch=len(dataloader_source_train),\n",
    "    epochs=train_params[\"initial_train_epochs\"],\n",
    ")\n",
    "\n",
    "criterion_clf = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(x, y_true, model):\n",
    "    x = x.to(torch.float32).to(device)\n",
    "    y_true = y_true.to(torch.float32).to(device)\n",
    "\n",
    "    y_pred = model(x)\n",
    "\n",
    "    loss = criterion_clf(y_pred, y_true)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def run_pretrain_epoch(model, dataloader, optimizer=None, scheduler=None, inner=None):\n",
    "    loss_running = []\n",
    "    mean_weights = []\n",
    "\n",
    "    is_training = model.training and optimizer\n",
    "\n",
    "    for _, batch in enumerate(dataloader):\n",
    "        loss = model_loss(*batch, model)\n",
    "        loss_running.append(loss.item())\n",
    "        mean_weights.append(len(batch))  # we will weight average by batch size later\n",
    "\n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "        if inner:\n",
    "            inner.update(1)\n",
    "    return loss_running, mean_weights\n",
    "\n",
    "\n",
    "def compute_acc(dataloader, model):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_running, mean_weights = run_pretrain_epoch(model, dataloader)\n",
    "\n",
    "    return np.average(loss_running, weights=mean_weights)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pretraining()\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store loss and accuracy values\n",
    "loss_history = []\n",
    "loss_history_val = []\n",
    "\n",
    "loss_history_running = []\n",
    "\n",
    "# Early Stopping\n",
    "best_loss_val = np.inf\n",
    "early_stop_count = 0\n",
    "\n",
    "with DupStdout().dup_to_file(os.path.join(pretrain_folder, \"log.txt\"), \"w\") as f_log:\n",
    "    # Train\n",
    "    print(\"Start pretrain...\")\n",
    "    outer = tqdm(total=train_params[\"initial_train_epochs\"], desc=\"Epochs\", position=0)\n",
    "    inner = tqdm(total=len(dataloader_source_train), desc=f\"Batch\", position=1)\n",
    "\n",
    "    print(\" Epoch | Train Loss | Val Loss   \")\n",
    "    print(\"---------------------------------\")\n",
    "    checkpoint = {\n",
    "        \"epoch\": -1,\n",
    "        \"model\": model,\n",
    "        \"optimizer\": pre_optimizer,\n",
    "        \"scheduler\": pre_scheduler,\n",
    "        # 'scaler': scaler\n",
    "    }\n",
    "    for epoch in range(train_params[\"initial_train_epochs\"]):\n",
    "        inner.refresh()  # force print final state\n",
    "        inner.reset()  # reuse bar\n",
    "\n",
    "        checkpoint[\"epoch\"] = epoch\n",
    "\n",
    "        # Train mode\n",
    "        model.train()\n",
    "\n",
    "        loss_running, mean_weights = run_pretrain_epoch(\n",
    "            model,\n",
    "            dataloader_source_train,\n",
    "            optimizer=pre_optimizer,\n",
    "            scheduler=pre_scheduler,\n",
    "            inner=inner,\n",
    "        )\n",
    "\n",
    "        loss_history.append(np.average(loss_running, weights=mean_weights))\n",
    "        loss_history_running.append(loss_running)\n",
    "\n",
    "        # Evaluate mode\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            curr_loss_val = compute_acc(dataloader_source_val, model)\n",
    "            loss_history_val.append(curr_loss_val)\n",
    "\n",
    "        # Print the results\n",
    "        outer.update(1)\n",
    "        print(\n",
    "            f\" {epoch:5d}\",\n",
    "            f\"| {loss_history[-1]:<10.8f}\",\n",
    "            f\"| {curr_loss_val:<10.8f}\",\n",
    "            end=\" \",\n",
    "        )\n",
    "        # Save the best weights\n",
    "        if curr_loss_val < best_loss_val:\n",
    "            best_loss_val = curr_loss_val\n",
    "            torch.save(checkpoint, os.path.join(pretrain_folder, f\"best_model.pth\"))\n",
    "            early_stop_count = 0\n",
    "\n",
    "            print(\"<-- new best val loss\")\n",
    "        else:\n",
    "            print(\"\")\n",
    "\n",
    "        # Save checkpoint every 10\n",
    "        # if epoch % 10 == 0 or epoch >= train_params[\"initial_train_epochs\"] - 1:\n",
    "        #     torch.save(checkpoint, os.path.join(pretrain_folder, f\"checkpt{epoch}.pth\"))\n",
    "\n",
    "        # check to see if validation loss has plateau'd\n",
    "        # if (\n",
    "        #     early_stop_count >= train_params[\"early_stop_crit\"]\n",
    "        #     and epoch >= train_params[\"min_epochs\"] - 1\n",
    "        # ):\n",
    "        #     print(\n",
    "        #         f\"Validation loss plateaued after {early_stop_count} at epoch {epoch}\"\n",
    "        #     )\n",
    "        #     torch.save(\n",
    "        #         checkpoint, os.path.join(pretrain_folder, f\"earlystop{epoch}.pth\")\n",
    "        #     )\n",
    "        #     break\n",
    "\n",
    "        early_stop_count += 1\n",
    "\n",
    "    # Save final model\n",
    "    best_checkpoint = torch.load(os.path.join(pretrain_folder, f\"best_model.pth\"))\n",
    "    torch.save(best_checkpoint, os.path.join(pretrain_folder, f\"final_model.pth\"))\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Adversarial Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advtrain_folder = os.path.join(model_folder, \"advtrain\")\n",
    "\n",
    "if not os.path.isdir(advtrain_folder):\n",
    "    os.makedirs(advtrain_folder)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cycle_iter(iter):\n",
    "#     while True:\n",
    "#         yield from iter\n",
    "\n",
    "\n",
    "# def iter_skip(iter, n=1):\n",
    "#     for i in range(len(iter) * n):\n",
    "#         if (i % n) == n - 1:\n",
    "#             yield next(iter)\n",
    "#         else:\n",
    "#             yield None, None\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_dis = nn.BCEWithLogitsLoss()\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrim_loss_accu(x, domain, model):\n",
    "    x = x.to(device)\n",
    "\n",
    "    if domain == \"source\":\n",
    "        y_dis = torch.zeros(x.shape[0], device=device, dtype=x.dtype).view(-1, 1)\n",
    "        emb = model.source_encoder(x)  # .view(x.shape[0], -1)\n",
    "    elif domain == \"target\":\n",
    "        y_dis = torch.ones(x.shape[0], device=device, dtype=x.dtype).view(-1, 1)\n",
    "        emb = model.target_encoder(x)  # .view(x.shape[0], -1)\n",
    "    else:\n",
    "        raise (\n",
    "            ValueError,\n",
    "            f\"invalid domain {domain} given, must be 'source' or 'target'\",\n",
    "        )\n",
    "\n",
    "    y_pred = model.dis(emb)\n",
    "\n",
    "    loss = criterion_dis(y_pred, y_dis)\n",
    "    accu = (\n",
    "        (torch.round(torch.sigmoid(y_pred)).to(torch.long) == y_dis).to(torch.float32).mean().cpu()\n",
    "    )\n",
    "\n",
    "    return loss, accu\n",
    "\n",
    "\n",
    "# def discrim_loss_accu(x_source, x_target, model):\n",
    "#     # x = x.to(device)\n",
    "\n",
    "#     x_source, x_target = x_source.to(device), x_target.to(device)\n",
    "\n",
    "#     # if domain == 'source':\n",
    "#     #     y_dis = torch.zeros(x.shape[0], device=device, dtype=x.dtype).view(-1, 1)\n",
    "#     #     emb = model.source_encoder(x) #.view(x.shape[0], -1)\n",
    "#     # elif domain == 'target':\n",
    "#     #     y_dis = torch.ones(x.shape[0], device=device, dtype=x.dtype).view(-1, 1)\n",
    "#     #     emb = model.target_encoder(x) #.view(x.shape[0], -1)\n",
    "#     # else:\n",
    "#     #     raise(ValueError, f\"invalid domain {domain} given, must be 'source' or 'target'\")\n",
    "\n",
    "#     y_dis = torch.cat(\n",
    "#         [\n",
    "#             torch.zeros(x_source.shape[0], device=device, dtype=x_source.dtype).view(\n",
    "#                 -1, 1\n",
    "#             ),\n",
    "#             torch.ones(x_target.shape[0], device=device, dtype=x_target.dtype).view(\n",
    "#                 -1, 1\n",
    "#             ),\n",
    "#         ]\n",
    "#     )\n",
    "#     x = torch.cat([x_source, x_target])\n",
    "#     emb = model.source_encoder(x)  # .view(x.shape[0], -1)\n",
    "#     y_pred = model.dis(emb)\n",
    "\n",
    "#     loss = criterion_dis(y_pred, y_dis)\n",
    "#     accu = torch.mean(\n",
    "#         (torch.round(y_pred).to(torch.long) == y_dis).to(torch.float32)\n",
    "#     ).cpu()\n",
    "\n",
    "#     return loss, accu\n",
    "\n",
    "\n",
    "def compute_acc_dis(dataloader_source, dataloader_target, model):\n",
    "    results_history = {\n",
    "        \"dis\": {\n",
    "            \"source\": {},\n",
    "            \"target\": {},\n",
    "        }\n",
    "    }\n",
    "\n",
    "    model.eval()\n",
    "    model.dis.eval()\n",
    "    model.target_encoder.eval()\n",
    "    model.source_encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_running, accu_running, mean_weights = run_adv_epoch_dis(\n",
    "            model, dataloader_source, \"source\"\n",
    "        )\n",
    "        results_history[\"dis\"][\"source\"][\"loss\"] = np.average(loss_running, weights=mean_weights)\n",
    "        results_history[\"dis\"][\"source\"][\"accu\"] = np.average(accu_running, weights=mean_weights)\n",
    "\n",
    "        loss_running, accu_running, mean_weights = run_adv_epoch_dis(\n",
    "            model, dataloader_target, \"target\"\n",
    "        )\n",
    "        results_history[\"dis\"][\"target\"][\"loss\"] = np.average(loss_running, weights=mean_weights)\n",
    "        results_history[\"dis\"][\"target\"][\"accu\"] = np.average(accu_running, weights=mean_weights)\n",
    "    return results_history\n",
    "\n",
    "\n",
    "def run_adv_epoch_dis(model, dataloader, domain):\n",
    "    loss_running = []\n",
    "    accu_running = []\n",
    "    mean_weights = []\n",
    "    for _, (X, _) in enumerate(dataloader):\n",
    "        loss, accu = discrim_loss_accu(X, domain, model)\n",
    "        loss_running.append(loss.item())\n",
    "        accu_running.append(accu)\n",
    "        mean_weights.append(len(X))\n",
    "    return loss_running, accu_running, mean_weights\n",
    "\n",
    "\n",
    "def encoder_loss(x_target, model):\n",
    "    x_target = x_target.to(device)\n",
    "\n",
    "    # flip label\n",
    "    y_dis = torch.zeros(x_target.shape[0], device=device, dtype=x_target.dtype).view(-1, 1)\n",
    "\n",
    "    emb_target = model.target_encoder(x_target)  # .view(x_target.shape[0], -1)\n",
    "    y_pred = model.dis(emb_target)\n",
    "    loss = criterion_dis(y_pred, y_dis)\n",
    "    accu = (\n",
    "        (torch.round(torch.sigmoid(y_pred)).to(torch.long) == y_dis).to(torch.float32).mean().cpu()\n",
    "    )\n",
    "\n",
    "    return loss, accu\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversarial_iters(\n",
    "    model,\n",
    "    save_folder,\n",
    "    dataloader_source_train,\n",
    "    dataloader_source_val,\n",
    "    dataloader_target_train,\n",
    "    dataloader_target_train_dis,\n",
    "):\n",
    "    model.to(device)\n",
    "    model.advtraining()\n",
    "\n",
    "    target_optimizer = torch.optim.Adam(\n",
    "        model.target_encoder.parameters(),\n",
    "        lr=train_params[\"enc_lr\"],\n",
    "        betas=(train_params[\"adam_beta1\"], 0.999),\n",
    "        eps=1e-07,\n",
    "    )\n",
    "    dis_optimizer = torch.optim.Adam(\n",
    "        model.dis.parameters(),\n",
    "        lr=train_params[\"alpha\"] * train_params[\"enc_lr\"],\n",
    "        betas=(train_params[\"adam_beta1\"], 0.999),\n",
    "        eps=1e-07,\n",
    "    )\n",
    "\n",
    "    # iters = -(max_len_dataloader // -(1 + DIS_LOOP_FACTOR))  # ceiling divide\n",
    "\n",
    "    dataloader_lengths = [\n",
    "        len(dataloader_source_train),\n",
    "        len(dataloader_target_train),\n",
    "        len(dataloader_target_train_dis) * train_params[\"dis_loop_factor\"],\n",
    "    ]\n",
    "    max_len_dataloader = np.amax(dataloader_lengths)\n",
    "    longest = np.argmax(dataloader_lengths)\n",
    "\n",
    "    # dis_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    #     dis_optimizer, max_lr=0.0005, steps_per_epoch=iters, epochs=EPOCHS\n",
    "    # )\n",
    "    # target_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    #     target_optimizer, max_lr=0.0005, steps_per_epoch=iters, epochs=EPOCHS\n",
    "    # )\n",
    "\n",
    "    # Initialize lists to store loss and accuracy values\n",
    "    # loss_history = []\n",
    "    # accu_history = []\n",
    "    # loss_history_val = []\n",
    "    # accu_history_val = []\n",
    "    # loss_history_running = []\n",
    "\n",
    "    # loss_history_gen = []\n",
    "    # loss_history_gen_running = []\n",
    "    # mean_weights_gen = []\n",
    "\n",
    "    results_template = {\n",
    "        \"dis\": {\n",
    "            \"source\": {\"loss\": [], \"accu\": [], \"weights\": []},\n",
    "            \"target\": {\"loss\": [], \"accu\": [], \"weights\": []},\n",
    "        },\n",
    "        \"gen\": {\n",
    "            \"target\": {\"loss\": [], \"accu\": [], \"weights\": []},\n",
    "        },\n",
    "    }\n",
    "    results_history = deepcopy(results_template)\n",
    "    results_history_val = deepcopy(results_template)\n",
    "    results_history_running = deepcopy(results_template)\n",
    "\n",
    "    # Early Stopping\n",
    "    best_loss_val = np.inf\n",
    "    early_stop_count = 0\n",
    "    with DupStdout().dup_to_file(os.path.join(save_folder, \"log.txt\"), \"w\") as f_log:\n",
    "        # Train\n",
    "        print(\"Start adversarial training...\")\n",
    "        outer = tqdm(total=train_params[\"epochs\"], desc=\"Epochs\", position=0)\n",
    "        inner1 = tqdm(total=max_len_dataloader, desc=f\"Batch\", position=1)\n",
    "\n",
    "        print(\" Epoch ||| Generator       ||| Discriminator \")\n",
    "        print(\"       ||| Train           ||| Train                             || Validation    \")\n",
    "        print(\n",
    "            \"       ||| Loss   | Accu   ||| Loss            | Accu            || Loss            | Accu  \"\n",
    "        )\n",
    "        print(\n",
    "            \"       ||| Target - Target ||| Source - Target | Source - Target || Source - Target | Source - Target \"\n",
    "        )\n",
    "        print(\n",
    "            \"------------------------------------------------------------------------------------------------------\"\n",
    "        )\n",
    "        checkpoint = {\n",
    "            \"epoch\": -1,\n",
    "            \"model\": model,\n",
    "            \"dis_optimizer\": dis_optimizer,\n",
    "            \"target_optimizer\": target_optimizer,\n",
    "            # \"dis_scheduler\": dis_scheduler,\n",
    "            # \"target_scheduler\": target_scheduler,\n",
    "        }\n",
    "        for epoch in range(train_params[\"epochs\"]):\n",
    "            inner1.refresh()  # force print final state\n",
    "            inner1.reset()  # reuse bar\n",
    "\n",
    "            checkpoint[\"epoch\"] = epoch\n",
    "\n",
    "            # Train mode\n",
    "            model.train()\n",
    "            model.target_encoder.train()\n",
    "            model.source_encoder.eval()\n",
    "            model.dis.train()\n",
    "\n",
    "            results_running = deepcopy(results_template)\n",
    "\n",
    "            s_train_iter = iter(dataloader_source_train)\n",
    "            t_train_iter = iter(dataloader_target_train)\n",
    "            t_train_dis_iter = iter(dataloader_target_train_dis)\n",
    "            for i in range(max_len_dataloader):\n",
    "                try:\n",
    "                    x_source, _ = next(s_train_iter)\n",
    "                except StopIteration:\n",
    "                    s_train_iter = iter(dataloader_source_train)\n",
    "                    x_source, _ = next(s_train_iter)\n",
    "                try:\n",
    "                    x_target, _ = next(t_train_iter)\n",
    "                except StopIteration:\n",
    "                    t_train_iter = iter(dataloader_target_train)\n",
    "                    x_target, _ = next(t_train_iter)\n",
    "\n",
    "                train_encoder_step = (i % train_params[\"dis_loop_factor\"]) == train_params[\n",
    "                    \"dis_loop_factor\"\n",
    "                ] - 1\n",
    "\n",
    "                model.train_discriminator()\n",
    "                # model.target_encoder.train()\n",
    "                # model.source_encoder.train()\n",
    "                # model.dis.train()\n",
    "\n",
    "                set_requires_grad(model.target_encoder, False)\n",
    "                set_requires_grad(model.source_encoder, False)\n",
    "                set_requires_grad(model.dis, True)\n",
    "\n",
    "                # lr_history_running.append(scheduler.get_last_lr())\n",
    "                dis_optimizer.zero_grad()\n",
    "\n",
    "                loss, accu = discrim_loss_accu(x_source, \"source\", model)\n",
    "                results_running[\"dis\"][\"source\"][\"loss\"].append(loss.item())\n",
    "                results_running[\"dis\"][\"source\"][\"accu\"].append(accu)\n",
    "                results_running[\"dis\"][\"source\"][\"weights\"].append(len(x_source))\n",
    "\n",
    "                # scaler.scale(loss).backward()\n",
    "                # scaler.step(optimizer)\n",
    "                # scaler.update()\n",
    "\n",
    "                loss.backward()\n",
    "                dis_optimizer.step()\n",
    "\n",
    "                dis_optimizer.zero_grad()\n",
    "\n",
    "                loss, accu = discrim_loss_accu(x_target, \"target\", model)\n",
    "                results_running[\"dis\"][\"target\"][\"loss\"].append(loss.item())\n",
    "                results_running[\"dis\"][\"target\"][\"accu\"].append(accu)\n",
    "                results_running[\"dis\"][\"target\"][\"weights\"].append(len(x_target))\n",
    "\n",
    "                # scaler.scale(loss).backward()\n",
    "                # scaler.step(optimizer)\n",
    "                # scaler.update()\n",
    "\n",
    "                loss.backward()\n",
    "                dis_optimizer.step()\n",
    "                # dis_scheduler.step()\n",
    "\n",
    "                # print(i % DIS_LOOP_FACTOR)\n",
    "                if train_encoder_step:\n",
    "                    try:\n",
    "                        x_target_enc, _ = next(t_train_dis_iter)\n",
    "                    except StopIteration:\n",
    "                        t_train_dis_iter = iter(dataloader_target_train_dis)\n",
    "                        x_target_enc, _ = next(t_train_dis_iter)\n",
    "                    model.train_target_encoder()\n",
    "                    # model.target_encoder.train()\n",
    "                    # model.source_encoder.train()\n",
    "                    # model.dis.train()\n",
    "\n",
    "                    set_requires_grad(model.target_encoder, True)\n",
    "                    set_requires_grad(model.source_encoder, False)\n",
    "                    set_requires_grad(model.dis, False)\n",
    "\n",
    "                    target_optimizer.zero_grad()\n",
    "\n",
    "                    loss, accu = encoder_loss(x_target_enc, model)\n",
    "\n",
    "                    results_running[\"gen\"][\"target\"][\"loss\"].append(loss.item())\n",
    "                    results_running[\"gen\"][\"target\"][\"accu\"].append(accu)\n",
    "                    results_running[\"gen\"][\"target\"][\"weights\"].append(len(x_target_enc))\n",
    "\n",
    "                    loss.backward()\n",
    "                    target_optimizer.step()\n",
    "                # target_scheduler.step()\n",
    "\n",
    "                inner1.update(1)\n",
    "            for module_k in results_running:\n",
    "                for domain_k in results_running[module_k]:\n",
    "                    for metric_k in results_running[module_k][domain_k]:\n",
    "                        results_history[module_k][domain_k][metric_k].append(\n",
    "                            np.average(\n",
    "                                results_running[module_k][domain_k][metric_k],\n",
    "                                weights=results_running[module_k][domain_k][\"weights\"],\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "            for module_k in results_running:\n",
    "                for domain_k in results_running[module_k]:\n",
    "                    for metric_k in results_running[module_k][domain_k]:\n",
    "                        results_history_running[module_k][domain_k][metric_k].append(\n",
    "                            results_running[module_k][domain_k][metric_k],\n",
    "                        )\n",
    "\n",
    "            model.eval()\n",
    "            model.dis.eval()\n",
    "            model.target_encoder.eval()\n",
    "            model.source_encoder.eval()\n",
    "\n",
    "            set_requires_grad(model, True)\n",
    "            set_requires_grad(model.target_encoder, True)\n",
    "            set_requires_grad(model.source_encoder, True)\n",
    "            set_requires_grad(model.dis, True)\n",
    "\n",
    "            # del batch_cycler\n",
    "            with torch.no_grad():\n",
    "                results_val = compute_acc_dis(dataloader_source_val, dataloader_target_train, model)\n",
    "            for module_k in results_val:\n",
    "                for domain_k in results_val[module_k]:\n",
    "                    for metric_k in results_val[module_k][domain_k]:\n",
    "                        results_history_val[module_k][domain_k][metric_k].append(\n",
    "                            results_val[module_k][domain_k][metric_k]\n",
    "                        )\n",
    "            # Print the results\n",
    "            outer.update(1)\n",
    "            print(\n",
    "                f\" {epoch:5d}\",\n",
    "                f\"||| {results_history['gen']['target']['loss'][-1]:6.4f}\",\n",
    "                f\"- {results_history['gen']['target']['accu'][-1]:6.4f}\",\n",
    "                f\"||| {results_history['dis']['source']['loss'][-1]:6.4f}\",\n",
    "                f\"- {results_history['dis']['target']['loss'][-1]:6.4f}\",\n",
    "                f\"| {results_history['dis']['source']['accu'][-1]:6.4f}\",\n",
    "                f\"- {results_history['dis']['target']['accu'][-1]:6.4f}\",\n",
    "                f\"|| {results_history_val['dis']['source']['loss'][-1]:6.4f}\",\n",
    "                f\"- {results_history_val['dis']['target']['loss'][-1]:6.4f}\",\n",
    "                f\"| {results_history_val['dis']['source']['accu'][-1]:6.4f}\",\n",
    "                f\"- {results_history_val['dis']['target']['accu'][-1]:6.4f}\",\n",
    "                end=\" \",\n",
    "            )\n",
    "\n",
    "            # # Save the best weights\n",
    "            # if diff_from_rand < best_loss_val:\n",
    "            #     best_loss_val = diff_from_rand\n",
    "            #     torch.save(checkpoint, os.path.join(save_folder, f\"best_model.pth\"))\n",
    "            #     early_stop_count = 0\n",
    "\n",
    "            #     print(\"<-- new best difference from random loss\")\n",
    "            # else:\n",
    "            #     print(\"\")\n",
    "\n",
    "            print(\"\")\n",
    "\n",
    "            # Save checkpoint every 10\n",
    "            # if epoch % 10 == 0 or epoch >= train_params[\"epochs\"] - 1:\n",
    "            #     torch.save(checkpoint, os.path.join(save_folder, f\"checkpt{epoch}.pth\"))\n",
    "\n",
    "            # # check to see if validation loss has plateau'd\n",
    "            # if (\n",
    "            #     early_stop_count >= train_params[\"early_stop_crit_adv\"]\n",
    "            #     and epoch > train_params[\"min_epochs_adv\"] - 1\n",
    "            # ):\n",
    "            #     print(\n",
    "            #         f\"Discriminator loss plateaued after {early_stop_count} at epoch {epoch}\"\n",
    "            #     )\n",
    "            #     torch.save(\n",
    "            #         checkpoint, os.path.join(save_folder, f\"earlystop_{epoch}.pth\")\n",
    "            #     )\n",
    "            #     break\n",
    "\n",
    "            early_stop_count += 1\n",
    "\n",
    "    # Save final model\n",
    "    torch.save(checkpoint, os.path.join(save_folder, f\"final_model.pth\"))\n",
    "\n",
    "    return results_history, results_history_running, results_history_val\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results_history, results_history_running, results_history_val, save_folder):\n",
    "\n",
    "    fig, axs = plt.subplots(4, 1, sharex=True, figsize=(9, 12), layout=\"constrained\")\n",
    "\n",
    "    # loss\n",
    "    axs[0].plot(\n",
    "        *format_iters(results_history_running[\"dis\"][\"source\"][\"loss\"]),\n",
    "        label=\"d-source\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    axs[0].plot(\n",
    "        *format_iters(results_history_running[\"dis\"][\"target\"][\"loss\"]),\n",
    "        label=\"d-target\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    axs[0].plot(\n",
    "        *format_iters(results_history_running[\"gen\"][\"target\"][\"loss\"]),\n",
    "        label=\"g-target\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "\n",
    "    axs[0].set_ylim(bottom=0, top=2)\n",
    "    axs[0].grid(which=\"major\")\n",
    "    axs[0].minorticks_on()\n",
    "    axs[0].grid(which=\"minor\", alpha=0.2)\n",
    "\n",
    "    axs[0].set_title(\"Training BCE Loss\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    # accuracy\n",
    "    axs[1].plot(\n",
    "        *format_iters(results_history_running[\"dis\"][\"source\"][\"accu\"]),\n",
    "        label=\"d-source\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    axs[1].plot(\n",
    "        *format_iters(results_history_running[\"dis\"][\"target\"][\"accu\"]),\n",
    "        label=\"d-target\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    axs[1].plot(\n",
    "        *format_iters(results_history_running[\"gen\"][\"target\"][\"accu\"]),\n",
    "        label=\"g-target\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "\n",
    "    axs[1].set_ylim(bottom=0, top=1)\n",
    "    axs[1].grid(which=\"major\")\n",
    "    axs[1].minorticks_on()\n",
    "    axs[1].grid(which=\"minor\", alpha=0.2)\n",
    "\n",
    "    axs[1].set_title(\"Training Accuracy\")\n",
    "    axs[1].legend()\n",
    "\n",
    "    # val loss\n",
    "    axs[2].plot(results_history[\"dis\"][\"source\"][\"loss\"], label=\"d-source\")\n",
    "    axs[2].plot(results_history[\"dis\"][\"target\"][\"loss\"], label=\"d-target\")\n",
    "\n",
    "    axs[2].set_ylim(bottom=0, top=2)\n",
    "    axs[2].grid(which=\"major\")\n",
    "    axs[2].minorticks_on()\n",
    "    axs[2].grid(which=\"minor\", alpha=0.2)\n",
    "\n",
    "    axs[2].set_title(\"Validation BCE Loss\")\n",
    "    axs[2].legend()\n",
    "\n",
    "    # val accuracy\n",
    "    axs[3].plot(results_history[\"dis\"][\"source\"][\"accu\"], label=\"d-source\")\n",
    "    axs[3].plot(results_history[\"dis\"][\"target\"][\"accu\"], label=\"d-target\")\n",
    "\n",
    "    axs[3].set_ylim(bottom=0, top=1)\n",
    "    axs[3].grid(which=\"major\")\n",
    "    axs[3].minorticks_on()\n",
    "    axs[3].grid(which=\"minor\", alpha=0.2)\n",
    "\n",
    "    axs[3].set_title(\"Valdiation Accuracy\")\n",
    "    axs[3].legend()\n",
    "\n",
    "    plt.savefig(os.path.join(save_folder, \"adv_train.png\"))\n",
    "\n",
    "    plt.show(block=False)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# st_sample_id_l = [SAMPLE_ID_N]\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_params[\"train_using_all_st_samples\"]:\n",
    "    print(f\"Adversarial training for all ST slides\")\n",
    "    save_folder = advtrain_folder\n",
    "\n",
    "    best_checkpoint = torch.load(os.path.join(pretrain_folder, f\"final_model.pth\"))\n",
    "    model = ADDAST(\n",
    "        sc_mix_d[\"train\"].shape[1],\n",
    "        ncls_source=lab_mix_d[\"train\"].shape[1],\n",
    "        is_adda=True,\n",
    "        **model_params[\"adda_kwargs\"],\n",
    "    )\n",
    "\n",
    "    model.source_encoder.load_state_dict(best_checkpoint[\"model\"].source_encoder.state_dict())\n",
    "    model.clf.load_state_dict(best_checkpoint[\"model\"].clf.state_dict())\n",
    "\n",
    "    model.init_adv()\n",
    "    model.dis.apply(initialize_weights)\n",
    "    model.to(device)\n",
    "\n",
    "    model.advtraining()\n",
    "\n",
    "    train_adversarial_iters(\n",
    "        model,\n",
    "        save_folder,\n",
    "        dataloader_source_train,\n",
    "        dataloader_source_val,\n",
    "        dataloader_target_train,\n",
    "        dataloader_target_train_dis,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    for sample_id in st_sample_id_l:\n",
    "        print(f\"Adversarial training for ST slide {sample_id}: \")\n",
    "\n",
    "        save_folder = os.path.join(advtrain_folder, sample_id)\n",
    "        if not os.path.isdir(save_folder):\n",
    "            os.makedirs(save_folder)\n",
    "\n",
    "        best_checkpoint = torch.load(os.path.join(pretrain_folder, f\"final_model.pth\"))\n",
    "\n",
    "        model = ADDAST(\n",
    "            sc_mix_d[\"train\"].shape[1],\n",
    "            ncls_source=lab_mix_d[\"train\"].shape[1],\n",
    "            is_adda=True,\n",
    "            **model_params[\"adda_kwargs\"],\n",
    "        )\n",
    "\n",
    "        model.apply(initialize_weights)\n",
    "\n",
    "        # load state dicts\n",
    "        # this makes it easier, if, say, the discriminator changes\n",
    "        model.source_encoder.load_state_dict(best_checkpoint[\"model\"].source_encoder.state_dict())\n",
    "\n",
    "        model.clf.load_state_dict(best_checkpoint[\"model\"].clf.state_dict())\n",
    "\n",
    "        model.init_adv()\n",
    "        model.dis.apply(initialize_weights)\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        model.advtraining()\n",
    "\n",
    "        results = train_adversarial_iters(\n",
    "            model,\n",
    "            save_folder,\n",
    "            dataloader_source_train,\n",
    "            dataloader_source_val,\n",
    "            dataloader_target_train_d[sample_id],\n",
    "            dataloader_target_train_dis_d[sample_id],\n",
    "        )\n",
    "        plot_results(*results, save_folder)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(model_folder, \"config.yml\"), \"w\") as f:\n",
    "    yaml.safe_dump(config, f)\n",
    "\n",
    "temp_folder_holder.copy_out()\n",
    ""
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 }
}