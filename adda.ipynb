{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADDA for ST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating something like CellDART but it actually follows Adda in PyTorch as a first step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import ceil\n",
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "\n",
    "import scipy.stats as ss\n",
    "from scipy.sparse import csr_matrix\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "script_start_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%Hh%Mm%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/2022-09-11_01h10m52\n"
     ]
    }
   ],
   "source": [
    "results_folder = os.path.join('results', script_start_time)\n",
    "model_folder = os.path.join('model', script_start_time)\n",
    "\n",
    "if not os.path.isdir(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "    print(model_folder)\n",
    "\n",
    "if not os.path.isdir(results_folder):\n",
    "    os.makedirs(results_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_markers = 20\n",
    "n_mix = 8\n",
    "n_spots = 20000\n",
    "\n",
    "sample_id_n = 151673\n",
    "\n",
    "batch_size = 512\n",
    "num_workers = 8\n",
    "initial_train_epochs = 100\n",
    "\n",
    "early_stop_crit = 100\n",
    "min_epochs = initial_train_epochs\n",
    "\n",
    "\n",
    "early_stop_crit_adv = 10\n",
    "min_epochs_adv = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPEncoder(nn.Module):\n",
    "    def __init__(self, inp_dim, emb_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(inp_dim, 1024), \n",
    "            nn.BatchNorm1d(1024, eps=0.001, momentum=0.99), \n",
    "            nn.ELU(),\n",
    "\n",
    "            nn.Linear(1024, emb_dim), \n",
    "            nn.BatchNorm1d(emb_dim, eps=0.001, momentum=0.99), \n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, emb_dim, ncls_source):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(emb_dim, ncls_source), \n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 32), \n",
    "            nn.BatchNorm1d(32, eps=0.001, momentum=0.99), \n",
    "            nn.ELU(), \n",
    "            \n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_requires_grad(model, requires_grad=True):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADDAST(nn.Module):\n",
    "    def __init__(self, inp_dim, emb_dim, ncls_source):\n",
    "        super().__init__()\n",
    "\n",
    "        self.source_encoder = MLPEncoder(inp_dim, emb_dim)\n",
    "        self.target_encoder = MLPEncoder(inp_dim, emb_dim)\n",
    "        self.clf = Classifier(emb_dim, ncls_source)\n",
    "        self.dis = Discriminator(emb_dim)\n",
    "\n",
    "        self.is_encoder_source = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.is_encoder_source:\n",
    "            x = self.source_encoder(x)\n",
    "        else:\n",
    "            x = self.target_encoder(x)\n",
    "\n",
    "        x = self.clf(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def pretraining(self):\n",
    "        self.is_encoder_source = True\n",
    "        set_requires_grad(self.source_encoder, True)\n",
    "        set_requires_grad(self.clf, True)\n",
    "\n",
    "    def advtraining(self, train_dis=True):\n",
    "        set_requires_grad(self.source_encoder, False)\n",
    "\n",
    "        if train_dis:\n",
    "            self.train_discriminator()\n",
    "        else:\n",
    "            self.train_target_encoder()\n",
    "\n",
    "    def target_inference(self):\n",
    "        self.is_encoder_source = False\n",
    "\n",
    "    def train_discriminator(self):\n",
    "        set_requires_grad(self.target_encoder, False)\n",
    "        set_requires_grad(self.dis, True)\n",
    "\n",
    "    def train_target_encoder(self):\n",
    "        set_requires_grad(self.target_encoder, True)\n",
    "        set_requires_grad(self.dis, False)\n",
    "\n",
    "    def set_encoder(self, encoder=\"source\"):\n",
    "        if encoder == \"source\":\n",
    "            self.is_encoder_source = True\n",
    "        elif encoder == 'target':\n",
    "            self.is_encoder_source = False\n",
    "        else:\n",
    "            current_encoder_str = '\\'source\\'' if self.is_encoder_source else '\\'target\\''\n",
    "            warnings.warn(\n",
    "                f\"encoder parameter should be 'source' or 'target', got {encoder}; encoder is currently {current_encoder_str}\",\n",
    "                RuntimeWarning\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpotDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset for cell spots. Returns a spot with GEx data, and optionally cell type distribution\"\"\"\n",
    "    def __init__(self, X, Y=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (array_like of rank 2): An (n_spots, n_genes) array of normalized log gene expression values.\n",
    "            Y (array_like of rank 2, optional): An (n_spots, n_cell_types) array of cell type distribution.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.X = torch.as_tensor(X).float()\n",
    "        assert self.X.dim() == 2, f\"X should be rank 2, got {self.X.dim()}\"\n",
    "\n",
    "        if Y is None:\n",
    "            self.Y = Y\n",
    "        else:\n",
    "            self.Y = torch.as_tensor(Y).float()\n",
    "            assert self.Y.dim() == 2, f\"Y should be rank 2, got {self.Y.dim()}\"\n",
    "            assert len(self.X) == len(self.Y), \"X and Y unequal lengths\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.Y is None:\n",
    "            return self.X[idx]\n",
    "        else:\n",
    "            return self.X[idx], self.Y[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data\n",
    "## Data load\n",
    "### Load SpatialLIBD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "anndata     0.8.0\n",
      "scanpy      1.9.1\n",
      "-----\n",
      "PIL                         9.2.0\n",
      "asttokens                   NA\n",
      "backcall                    0.2.0\n",
      "beta_ufunc                  NA\n",
      "binom_ufunc                 NA\n",
      "bottleneck                  1.3.5\n",
      "cffi                        1.15.1\n",
      "colorama                    0.4.5\n",
      "cycler                      0.10.0\n",
      "cython_runtime              NA\n",
      "dateutil                    2.8.2\n",
      "debugpy                     1.5.1\n",
      "decorator                   5.1.1\n",
      "defusedxml                  0.7.1\n",
      "entrypoints                 0.4\n",
      "executing                   0.8.3\n",
      "h5py                        3.6.0\n",
      "igraph                      0.9.10\n",
      "ipykernel                   6.15.2\n",
      "ipython_genutils            0.2.0\n",
      "ipywidgets                  7.6.5\n",
      "jedi                        0.18.1\n",
      "joblib                      1.1.0\n",
      "jupyter_server              1.18.1\n",
      "kiwisolver                  1.4.2\n",
      "leidenalg                   0.8.10\n",
      "llvmlite                    0.38.0\n",
      "matplotlib                  3.5.1\n",
      "matplotlib_inline           0.1.6\n",
      "mkl                         2.4.0\n",
      "mpl_toolkits                NA\n",
      "natsort                     7.1.1\n",
      "nbinom_ufunc                NA\n",
      "numba                       0.55.1\n",
      "numexpr                     2.8.3\n",
      "numpy                       1.21.5\n",
      "packaging                   21.3\n",
      "pandas                      1.4.3\n",
      "parso                       0.8.3\n",
      "pexpect                     4.8.0\n",
      "pickleshare                 0.7.5\n",
      "pkg_resources               NA\n",
      "prompt_toolkit              3.0.20\n",
      "psutil                      5.9.0\n",
      "ptyprocess                  0.7.0\n",
      "pure_eval                   0.2.2\n",
      "pydev_ipython               NA\n",
      "pydevconsole                NA\n",
      "pydevd                      2.6.0\n",
      "pydevd_concurrency_analyser NA\n",
      "pydevd_file_utils           NA\n",
      "pydevd_plugins              NA\n",
      "pydevd_tracing              NA\n",
      "pygments                    2.11.2\n",
      "pyparsing                   3.0.9\n",
      "pytz                        2022.1\n",
      "scipy                       1.7.3\n",
      "seaborn                     0.11.2\n",
      "session_info                1.0.0\n",
      "setuptools                  63.4.1\n",
      "six                         1.16.0\n",
      "sklearn                     1.1.1\n",
      "sphinxcontrib               NA\n",
      "stack_data                  0.2.0\n",
      "statsmodels                 0.13.2\n",
      "texttable                   1.6.4\n",
      "threadpoolctl               2.2.0\n",
      "torch                       1.12.1\n",
      "tornado                     6.2\n",
      "tqdm                        4.64.0\n",
      "traitlets                   5.1.1\n",
      "typing_extensions           NA\n",
      "wcwidth                     0.2.5\n",
      "zmq                         23.2.0\n",
      "-----\n",
      "IPython             8.4.0\n",
      "jupyter_client      7.3.5\n",
      "jupyter_core        4.10.0\n",
      "jupyterlab          3.4.5\n",
      "notebook            6.4.12\n",
      "-----\n",
      "Python 3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0]\n",
      "Linux-5.10.102.1-microsoft-standard-WSL2-x86_64-with-glibc2.31\n",
      "-----\n",
      "Session information updated at 2022-09-11 01:10\n"
     ]
    }
   ],
   "source": [
    "spatialLIBD_dir = './data/spatialLIBD'\n",
    "\n",
    "sc.logging.print_versions()\n",
    "sc.set_figure_params(facecolor=\"white\", figsize=(8, 8))\n",
    "sc.settings.verbosity = 3\n",
    "\n",
    "adata_dir = os.path.join(spatialLIBD_dir, 'adata')\n",
    "\n",
    "adata_spatialLIBD_d = {}\n",
    "\n",
    "for name in glob.glob(os.path.join(adata_dir, 'adata_spatialLIBD-*.h5ad')):\n",
    "    sample_id = int(name.partition(\"-\")[2].rpartition(\".\")[0])\n",
    "    # print(sample_id)\n",
    "    adata_spatialLIBD_d[sample_id] = sc.read_h5ad(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Single Cell Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_sc_dlpfc = sc.read_h5ad('./data/sc_dlpfc/adata_sc_dlpfc.h5ad')\n",
    "adata_sc_dlpfc.var_names_make_unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Marker Genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "# adata_sc_dlpfc.var['mt'] = adata_sc_dlpfc.var_names.str.startswith('Mt-')  # annotate the group of mitochondrial genes as 'mt'\n",
    "# sc.pp.calculate_qc_metrics(adata_sc_dlpfc, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True)\n",
    "\n",
    "# sc.pp.normalize_total(adata_sc_dlpfc)\n",
    "\n",
    "# #PCA and clustering : Known markers with 'cell_subclass'\n",
    "# sc.tl.pca(adata_sc_dlpfc, svd_solver='arpack')\n",
    "# sc.pp.neighbors(adata_sc_dlpfc, n_neighbors=10, n_pcs=40)\n",
    "# sc.tl.umap(adata_sc_dlpfc)\n",
    "# sc.tl.leiden(adata_sc_dlpfc, resolution = 0.5)\n",
    "# sc.pl.umap(adata_sc_dlpfc, color=['leiden','cell_subclass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranking genes\n",
      "    finished: added to `.uns['rank_genes_groups']`\n",
      "    'names', sorted np.recarray to be indexed by group ids\n",
      "    'scores', sorted np.recarray to be indexed by group ids\n",
      "    'logfoldchanges', sorted np.recarray to be indexed by group ids\n",
      "    'pvals', sorted np.recarray to be indexed by group ids\n",
      "    'pvals_adj', sorted np.recarray to be indexed by group ids (0:02:03)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:394: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'names'] = self.var_names[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:396: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'scores'] = scores[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:399: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals'] = pvals[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:409: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals_adj'] = pvals_adj[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:420: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'logfoldchanges'] = np.log2(\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:394: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'names'] = self.var_names[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:396: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'scores'] = scores[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:399: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals'] = pvals[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:409: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals_adj'] = pvals_adj[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:420: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'logfoldchanges'] = np.log2(\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:394: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'names'] = self.var_names[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:396: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'scores'] = scores[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:399: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals'] = pvals[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:409: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals_adj'] = pvals_adj[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:420: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'logfoldchanges'] = np.log2(\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:394: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'names'] = self.var_names[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:396: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'scores'] = scores[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:399: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals'] = pvals[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:409: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals_adj'] = pvals_adj[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:420: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'logfoldchanges'] = np.log2(\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:394: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'names'] = self.var_names[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:396: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'scores'] = scores[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:399: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals'] = pvals[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:409: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals_adj'] = pvals_adj[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:420: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'logfoldchanges'] = np.log2(\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:394: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'names'] = self.var_names[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:396: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'scores'] = scores[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:399: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals'] = pvals[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:409: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals_adj'] = pvals_adj[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:420: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'logfoldchanges'] = np.log2(\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:394: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'names'] = self.var_names[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:396: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'scores'] = scores[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:399: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals'] = pvals[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:409: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals_adj'] = pvals_adj[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:420: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'logfoldchanges'] = np.log2(\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:394: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'names'] = self.var_names[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:396: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'scores'] = scores[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:399: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals'] = pvals[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:409: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals_adj'] = pvals_adj[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:420: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'logfoldchanges'] = np.log2(\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:394: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'names'] = self.var_names[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:396: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'scores'] = scores[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:399: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals'] = pvals[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:409: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals_adj'] = pvals_adj[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:420: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'logfoldchanges'] = np.log2(\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:394: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'names'] = self.var_names[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:396: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'scores'] = scores[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:399: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals'] = pvals[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:409: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals_adj'] = pvals_adj[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:420: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'logfoldchanges'] = np.log2(\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:394: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'names'] = self.var_names[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:396: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'scores'] = scores[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:399: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals'] = pvals[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:409: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals_adj'] = pvals_adj[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:420: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'logfoldchanges'] = np.log2(\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:394: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'names'] = self.var_names[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:396: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'scores'] = scores[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:399: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals'] = pvals[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:409: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals_adj'] = pvals_adj[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:420: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'logfoldchanges'] = np.log2(\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:394: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'names'] = self.var_names[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:396: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'scores'] = scores[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:399: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals'] = pvals[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:409: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'pvals_adj'] = pvals_adj[global_indices]\n",
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:420: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.stats[group_name, 'logfoldchanges'] = np.log2(\n"
     ]
    }
   ],
   "source": [
    "sc.tl.rank_genes_groups(adata_sc_dlpfc, 'cell_subclass', method='wilcoxon')\n",
    "# sc.pl.rank_genes_groups(adata_sc_dlpfc, n_genes=20, sharey=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Astros_1</th>\n",
       "      <th>Astros_2</th>\n",
       "      <th>Astros_3</th>\n",
       "      <th>Endo</th>\n",
       "      <th>Ex_1_L5_6</th>\n",
       "      <th>Ex_2_L5</th>\n",
       "      <th>Ex_3_L4_5</th>\n",
       "      <th>Ex_4_L_6</th>\n",
       "      <th>Ex_5_L5</th>\n",
       "      <th>Ex_6_L4_6</th>\n",
       "      <th>...</th>\n",
       "      <th>Mix_1</th>\n",
       "      <th>Mix_2</th>\n",
       "      <th>Mix_3</th>\n",
       "      <th>Mix_4</th>\n",
       "      <th>Mix_5</th>\n",
       "      <th>OPCs_1</th>\n",
       "      <th>OPCs_2</th>\n",
       "      <th>Oligos_1</th>\n",
       "      <th>Oligos_2</th>\n",
       "      <th>Oligos_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CLU</td>\n",
       "      <td>SLC1A2</td>\n",
       "      <td>SLC1A2</td>\n",
       "      <td>CLDN5</td>\n",
       "      <td>TSHZ2</td>\n",
       "      <td>MAP1B</td>\n",
       "      <td>CHN1</td>\n",
       "      <td>KCNIP4</td>\n",
       "      <td>SNAP25</td>\n",
       "      <td>TSHZ2</td>\n",
       "      <td>...</td>\n",
       "      <td>FTH1</td>\n",
       "      <td>DHFR</td>\n",
       "      <td>TBC1D3P1-DHX40P1</td>\n",
       "      <td>PTGDS</td>\n",
       "      <td>CNTNAP2</td>\n",
       "      <td>LHFPL3</td>\n",
       "      <td>LHFPL3</td>\n",
       "      <td>PLP1</td>\n",
       "      <td>GPR17</td>\n",
       "      <td>MBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SLC1A3</td>\n",
       "      <td>PTGDS</td>\n",
       "      <td>GPC5</td>\n",
       "      <td>B2M</td>\n",
       "      <td>ASIC2</td>\n",
       "      <td>SNAP25</td>\n",
       "      <td>NRGN</td>\n",
       "      <td>ZNF804B</td>\n",
       "      <td>CHN1</td>\n",
       "      <td>IL1RAPL2</td>\n",
       "      <td>...</td>\n",
       "      <td>CALM1</td>\n",
       "      <td>IDS</td>\n",
       "      <td>RPL32</td>\n",
       "      <td>NRGN</td>\n",
       "      <td>RBFOX1</td>\n",
       "      <td>PCDH15</td>\n",
       "      <td>PTPRZ1</td>\n",
       "      <td>CTNNA3</td>\n",
       "      <td>BCAS1</td>\n",
       "      <td>PLP1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>APOE</td>\n",
       "      <td>SLC1A3</td>\n",
       "      <td>ADGRV1</td>\n",
       "      <td>FLT1</td>\n",
       "      <td>HTR2C</td>\n",
       "      <td>VSNL1</td>\n",
       "      <td>SLC17A7</td>\n",
       "      <td>RGS12</td>\n",
       "      <td>CALM1</td>\n",
       "      <td>DCC</td>\n",
       "      <td>...</td>\n",
       "      <td>FTL</td>\n",
       "      <td>FTH1</td>\n",
       "      <td>ATP1B1</td>\n",
       "      <td>GRIN1</td>\n",
       "      <td>ROBO2</td>\n",
       "      <td>LRRC4C</td>\n",
       "      <td>PCDH15</td>\n",
       "      <td>ST18</td>\n",
       "      <td>TMEM108</td>\n",
       "      <td>PCDH9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GPC5</td>\n",
       "      <td>GPC5</td>\n",
       "      <td>SLC1A3</td>\n",
       "      <td>EPAS1</td>\n",
       "      <td>ZNF385D</td>\n",
       "      <td>CALM1</td>\n",
       "      <td>SNAP25</td>\n",
       "      <td>SORBS2</td>\n",
       "      <td>MAP1B</td>\n",
       "      <td>RORB</td>\n",
       "      <td>...</td>\n",
       "      <td>MAP1A</td>\n",
       "      <td>CALM1</td>\n",
       "      <td>SNAP25</td>\n",
       "      <td>MBP</td>\n",
       "      <td>GRIK1</td>\n",
       "      <td>PTPRZ1</td>\n",
       "      <td>LRRC4C</td>\n",
       "      <td>MBP</td>\n",
       "      <td>MIR219A2</td>\n",
       "      <td>QKI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GLUL</td>\n",
       "      <td>COL5A3</td>\n",
       "      <td>NPAS3</td>\n",
       "      <td>DLC1</td>\n",
       "      <td>HS3ST4</td>\n",
       "      <td>YWHAH</td>\n",
       "      <td>SNCB</td>\n",
       "      <td>ERC2</td>\n",
       "      <td>NRGN</td>\n",
       "      <td>PTPRD</td>\n",
       "      <td>...</td>\n",
       "      <td>CAMK2N1</td>\n",
       "      <td>CAMK2N1</td>\n",
       "      <td>CALM1</td>\n",
       "      <td>MEG3</td>\n",
       "      <td>DLGAP1</td>\n",
       "      <td>DSCAM</td>\n",
       "      <td>DSCAM</td>\n",
       "      <td>QKI</td>\n",
       "      <td>MAML2</td>\n",
       "      <td>IL1RAPL1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Astros_1 Astros_2 Astros_3   Endo Ex_1_L5_6 Ex_2_L5 Ex_3_L4_5 Ex_4_L_6  \\\n",
       "0      CLU   SLC1A2   SLC1A2  CLDN5     TSHZ2   MAP1B      CHN1   KCNIP4   \n",
       "1   SLC1A3    PTGDS     GPC5    B2M     ASIC2  SNAP25      NRGN  ZNF804B   \n",
       "2     APOE   SLC1A3   ADGRV1   FLT1     HTR2C   VSNL1   SLC17A7    RGS12   \n",
       "3     GPC5     GPC5   SLC1A3  EPAS1   ZNF385D   CALM1    SNAP25   SORBS2   \n",
       "4     GLUL   COL5A3    NPAS3   DLC1    HS3ST4   YWHAH      SNCB     ERC2   \n",
       "\n",
       "  Ex_5_L5 Ex_6_L4_6  ...    Mix_1    Mix_2             Mix_3  Mix_4    Mix_5  \\\n",
       "0  SNAP25     TSHZ2  ...     FTH1     DHFR  TBC1D3P1-DHX40P1  PTGDS  CNTNAP2   \n",
       "1    CHN1  IL1RAPL2  ...    CALM1      IDS             RPL32   NRGN   RBFOX1   \n",
       "2   CALM1       DCC  ...      FTL     FTH1            ATP1B1  GRIN1    ROBO2   \n",
       "3   MAP1B      RORB  ...    MAP1A    CALM1            SNAP25    MBP    GRIK1   \n",
       "4    NRGN     PTPRD  ...  CAMK2N1  CAMK2N1             CALM1   MEG3   DLGAP1   \n",
       "\n",
       "   OPCs_1  OPCs_2 Oligos_1  Oligos_2  Oligos_3  \n",
       "0  LHFPL3  LHFPL3     PLP1     GPR17       MBP  \n",
       "1  PCDH15  PTPRZ1   CTNNA3     BCAS1      PLP1  \n",
       "2  LRRC4C  PCDH15     ST18   TMEM108     PCDH9  \n",
       "3  PTPRZ1  LRRC4C      MBP  MIR219A2       QKI  \n",
       "4   DSCAM   DSCAM      QKI     MAML2  IL1RAPL1  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genelists = adata_sc_dlpfc.uns['rank_genes_groups']['names']\n",
    "df_genelists = pd.DataFrame.from_records(genelists)\n",
    "df_genelists.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_genes = []\n",
    "for column in df_genelists.head(num_markers):\n",
    "    res_genes.extend(df_genelists.head(num_markers)[column].tolist())\n",
    "res_genes_ = list(set(res_genes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select same gene features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wma/miniconda3/envs/agreda2/lib/python3.10/site-packages/anndata/_core/anndata.py:1828: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n"
     ]
    }
   ],
   "source": [
    "adata_spatialLIBD = ad.concat(\n",
    "    adata_spatialLIBD_d.values(), \n",
    "    label='sample_id', \n",
    "    keys=adata_spatialLIBD_d.keys()\n",
    ")\n",
    "adata_spatialLIBD.obs_names_make_unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Feature Gene number 362\n"
     ]
    }
   ],
   "source": [
    "adata_spatialLIBD.var_names_make_unique()\n",
    "inter_genes = [val for val in res_genes_ if val in adata_spatialLIBD.var.index]\n",
    "print('Selected Feature Gene number', len(inter_genes))\n",
    "adata_sc_dlpfc = adata_sc_dlpfc[:, inter_genes]\n",
    "\n",
    "adata_spatialLIBD = adata_spatialLIBD[:, inter_genes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_sc = adata_sc_dlpfc.X.todense()\n",
    "mat_sp_train = adata_spatialLIBD[adata_spatialLIBD.obs.sample_id == sample_id_n].X.todense()\n",
    "# mat_sp_train = adata_spatialLIBD.X.todense()\n",
    "mat_sp_test = adata_spatialLIBD[adata_spatialLIBD.obs.sample_id == sample_id_n].X.todense()\n",
    "\n",
    "df_sc = adata_sc_dlpfc.obs\n",
    "\n",
    "lab_sc_sub = df_sc.cell_subclass\n",
    "sc_sub_dict = dict(zip(range(len(set(lab_sc_sub))), df_genelists.columns.tolist()))\n",
    "sc_sub_dict2 = dict((y, x) for x, y in sc_sub_dict.items())\n",
    "lab_sc_num = [sc_sub_dict2[ii] for ii in lab_sc_sub]\n",
    "lab_sc_num = np.asarray(lab_sc_num, dtype='int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Pseudospots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "def random_mix(Xs, ys, nmix=5, n_samples=10000, seed=0):\n",
    "    # Define empty lists\n",
    "    Xs_new, ys_new = [], []\n",
    "    ys_ = OneHotEncoder().fit_transform(ys.reshape(-1, 1)).toarray()\n",
    "\n",
    "    rstate = np.random.RandomState(seed)\n",
    "    fraction_all = rstate.rand(n_samples, nmix)\n",
    "    randindex_all = rstate.randint(len(Xs), size=(n_samples, nmix))\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # fraction: random fraction across the \"nmix\" number of sampled cells\n",
    "        fraction = fraction_all[i]\n",
    "        fraction = fraction / np.sum(fraction)\n",
    "        fraction = np.reshape(fraction, (nmix, 1))\n",
    "\n",
    "        # Random selection of the single cell data by the index\n",
    "        randindex = randindex_all[i]\n",
    "        ymix = ys_[randindex]\n",
    "        # Calculate the fraction of cell types in the cell mixture\n",
    "        yy = np.sum(ymix * fraction, axis=0)\n",
    "        # Calculate weighted gene expression of the cell mixture\n",
    "        XX = np.asarray(Xs[randindex]) * fraction\n",
    "        XX_ = np.sum(XX, axis=0)\n",
    "\n",
    "        # Add cell type fraction & composite gene expression in the list\n",
    "        ys_new.append(yy)\n",
    "        Xs_new.append(XX_)\n",
    "\n",
    "    Xs_new = np.asarray(Xs_new)\n",
    "    ys_new = np.asarray(ys_new)\n",
    "\n",
    "    return Xs_new, ys_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_sc_train, mat_sc_test, lab_sc_num_train, lab_sc_num_test = train_test_split(\n",
    "    mat_sc, lab_sc_num, test_size=0.2, random_state=225, stratify=lab_sc_num\n",
    ")\n",
    "\n",
    "sc_mix_train, lab_mix_train = random_mix(mat_sc_train, lab_sc_num_train, nmix=n_mix, n_samples=n_spots)\n",
    "sc_mix_test, lab_mix_test = random_mix(mat_sc_test, lab_sc_num_test, nmix=n_mix, n_samples=n_spots // 4)\n",
    "\n",
    "\n",
    "def log_minmaxscale(arr):\n",
    "    arrd = len(arr)\n",
    "    arr = np.log1p(arr)\n",
    "    return (arr - np.reshape(np.min(arr, axis=1),\n",
    "                             (arrd, 1))) / np.reshape((np.max(arr, axis=1) - np.min(arr, axis=1)), (arrd, 1))\n",
    "\n",
    "\n",
    "sc_mix_train_s = log_minmaxscale(sc_mix_train)\n",
    "sc_mix_test_s = log_minmaxscale(sc_mix_test)\n",
    "\n",
    "mat_sp_train_s = log_minmaxscale(mat_sp_train)\n",
    "mat_sp_test_s = log_minmaxscale(mat_sp_test)\n",
    "mat_sc_s = log_minmaxscale(mat_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train_set = SpotDataset(sc_mix_train_s, lab_mix_train)\n",
    "source_test_set = SpotDataset(sc_mix_test_s, lab_mix_test)\n",
    "\n",
    "target_train_set = SpotDataset(mat_sp_train_s)\n",
    "target_test_set = SpotDataset(mat_sp_test_s)\n",
    "\n",
    "dataloader_source_train = torch.utils.data.DataLoader(\n",
    "    source_train_set, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")\n",
    "dataloader_source_test = torch.utils.data.DataLoader(\n",
    "    source_test_set, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "dataloader_target_train = torch.utils.data.DataLoader(\n",
    "    target_train_set, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")\n",
    "dataloader_target_test = torch.utils.data.DataLoader(\n",
    "    target_test_set, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training: Adversarial domain adaptation for cell fraction estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADDAST(\n",
       "  (source_encoder): MLPEncoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): Linear(in_features=362, out_features=1024, bias=True)\n",
       "      (1): BatchNorm1d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (2): ELU(alpha=1.0)\n",
       "      (3): Linear(in_features=1024, out_features=64, bias=True)\n",
       "      (4): BatchNorm1d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (5): ELU(alpha=1.0)\n",
       "    )\n",
       "  )\n",
       "  (target_encoder): MLPEncoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): Linear(in_features=362, out_features=1024, bias=True)\n",
       "      (1): BatchNorm1d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (2): ELU(alpha=1.0)\n",
       "      (3): Linear(in_features=1024, out_features=64, bias=True)\n",
       "      (4): BatchNorm1d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (5): ELU(alpha=1.0)\n",
       "    )\n",
       "  )\n",
       "  (clf): Classifier(\n",
       "    (head): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=33, bias=True)\n",
       "      (1): LogSoftmax(dim=1)\n",
       "    )\n",
       "  )\n",
       "  (dis): Discriminator(\n",
       "    (head): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (1): BatchNorm1d(32, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (2): ELU(alpha=1.0)\n",
       "      (3): Dropout(p=0.5, inplace=False)\n",
       "      (4): Linear(in_features=32, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ADDAST(\n",
    "    sc_mix_train_s.shape[1], \n",
    "    emb_dim=64, \n",
    "    ncls_source=lab_mix_train.shape[1]\n",
    ")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_folder = os.path.join(model_folder, \"pretrain\")\n",
    "\n",
    "if not os.path.isdir(pretrain_folder):\n",
    "    os.makedirs(pretrain_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_optimizer = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=0.001, \n",
    "    betas=(0.9, 0.999), \n",
    "    eps=1e-07\n",
    ")\n",
    "\n",
    "criterion = nn.KLDivLoss(reduction=\"batchmean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pretraining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(dataloader, model):\n",
    "    loss_running = []\n",
    "    mean_weights = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, (x, y_true) in enumerate(dataloader):\n",
    "\n",
    "            x = x.to(torch.float32).to(device)\n",
    "            y_true = y_true.to(torch.float32).to(device)\n",
    "\n",
    "            y_pred = model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y_true)\n",
    "\n",
    "            loss_running.append(loss.item())\n",
    "\n",
    "            # we will weight average by batch size later\n",
    "            mean_weights.append(len(x))\n",
    "\n",
    "    # print(len(loss_running), len(mean_weights))\n",
    "    return np.average(loss_running, weights=mean_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start pretrain...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c9577953cf4e658efe8a7af6ab71e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f616570fb2b54c09ac8f1a867975a08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train loss: 1.197426 validation loss: 1.012398\n",
      "epoch: 1 train loss: 0.855513 validation loss: 0.734401\n",
      "epoch: 2 train loss: 0.697261 validation loss: 0.668774\n",
      "epoch: 3 train loss: 0.625851 validation loss: 0.607161\n",
      "epoch: 4 train loss: 0.593486 validation loss: 0.580399\n",
      "epoch: 5 train loss: 0.571307 validation loss: 0.565508\n",
      "epoch: 6 train loss: 0.553025 validation loss: 0.55745\n",
      "epoch: 7 train loss: 0.535326 validation loss: 0.541819\n",
      "epoch: 8 train loss: 0.521437 validation loss: 0.517787\n",
      "epoch: 9 train loss: 0.509444 validation loss: 0.526597\n",
      "epoch: 10 train loss: 0.496111 validation loss: 0.52184\n",
      "epoch: 11 train loss: 0.484132 validation loss: 0.496984\n",
      "epoch: 12 train loss: 0.470173 validation loss: 0.471665\n",
      "epoch: 13 train loss: 0.457035 validation loss: 0.494114\n",
      "epoch: 14 train loss: 0.44294 validation loss: 0.490062\n",
      "epoch: 15 train loss: 0.432719 validation loss: 0.450392\n",
      "epoch: 16 train loss: 0.418739 validation loss: 0.464916\n",
      "epoch: 17 train loss: 0.404132 validation loss: 0.448417\n",
      "epoch: 18 train loss: 0.396551 validation loss: 0.528252\n",
      "epoch: 19 train loss: 0.386292 validation loss: 0.465638\n",
      "epoch: 20 train loss: 0.369223 validation loss: 0.429007\n",
      "epoch: 21 train loss: 0.360242 validation loss: 0.436637\n",
      "epoch: 22 train loss: 0.348738 validation loss: 0.707346\n",
      "epoch: 23 train loss: 0.343899 validation loss: 0.425509\n",
      "epoch: 24 train loss: 0.323618 validation loss: 0.414281\n",
      "epoch: 25 train loss: 0.31486 validation loss: 0.431641\n",
      "epoch: 26 train loss: 0.304762 validation loss: 0.364944\n",
      "epoch: 27 train loss: 0.304721 validation loss: 0.396864\n",
      "epoch: 28 train loss: 0.293995 validation loss: 0.373238\n",
      "epoch: 29 train loss: 0.284141 validation loss: 0.367491\n",
      "epoch: 30 train loss: 0.279748 validation loss: 0.343366\n",
      "epoch: 31 train loss: 0.266765 validation loss: 0.44747\n",
      "epoch: 32 train loss: 0.268512 validation loss: 0.40476\n",
      "epoch: 33 train loss: 0.256452 validation loss: 0.356067\n",
      "epoch: 34 train loss: 0.251928 validation loss: 0.599101\n",
      "epoch: 35 train loss: 0.245536 validation loss: 0.451071\n",
      "epoch: 36 train loss: 0.238764 validation loss: 0.393871\n",
      "epoch: 37 train loss: 0.238392 validation loss: 0.280203\n",
      "epoch: 38 train loss: 0.227101 validation loss: 0.319569\n",
      "epoch: 39 train loss: 0.227679 validation loss: 0.319608\n",
      "epoch: 40 train loss: 0.224989 validation loss: 0.351287\n",
      "epoch: 41 train loss: 0.214639 validation loss: 0.270305\n",
      "epoch: 42 train loss: 0.211577 validation loss: 0.629464\n",
      "epoch: 43 train loss: 0.224359 validation loss: 0.307913\n",
      "epoch: 44 train loss: 0.211503 validation loss: 0.387732\n",
      "epoch: 45 train loss: 0.211074 validation loss: 0.273577\n",
      "epoch: 46 train loss: 0.203113 validation loss: 0.306686\n",
      "epoch: 47 train loss: 0.195153 validation loss: 0.276845\n",
      "epoch: 48 train loss: 0.192828 validation loss: 0.268961\n",
      "epoch: 49 train loss: 0.187712 validation loss: 0.319151\n",
      "epoch: 50 train loss: 0.184059 validation loss: 0.340423\n",
      "epoch: 51 train loss: 0.194826 validation loss: 0.250375\n",
      "epoch: 52 train loss: 0.178339 validation loss: 0.31056\n",
      "epoch: 53 train loss: 0.169919 validation loss: 0.409232\n",
      "epoch: 54 train loss: 0.171502 validation loss: 0.284826\n",
      "epoch: 55 train loss: 0.170443 validation loss: 0.298401\n",
      "epoch: 56 train loss: 0.178424 validation loss: 0.282448\n",
      "epoch: 57 train loss: 0.169703 validation loss: 0.288164\n",
      "epoch: 58 train loss: 0.167302 validation loss: 0.339036\n",
      "epoch: 59 train loss: 0.161001 validation loss: 0.247944\n",
      "epoch: 60 train loss: 0.163764 validation loss: 0.266742\n",
      "epoch: 61 train loss: 0.168419 validation loss: 0.383322\n",
      "epoch: 62 train loss: 0.162975 validation loss: 0.337149\n",
      "epoch: 63 train loss: 0.163081 validation loss: 0.287363\n",
      "epoch: 64 train loss: 0.158523 validation loss: 0.277756\n",
      "epoch: 65 train loss: 0.152538 validation loss: 0.226146\n",
      "epoch: 66 train loss: 0.147441 validation loss: 0.231046\n",
      "epoch: 67 train loss: 0.147091 validation loss: 0.256153\n",
      "epoch: 68 train loss: 0.148018 validation loss: 0.305073\n",
      "epoch: 69 train loss: 0.152743 validation loss: 0.303011\n",
      "epoch: 70 train loss: 0.148844 validation loss: 0.419935\n",
      "epoch: 71 train loss: 0.149051 validation loss: 0.211222\n",
      "epoch: 72 train loss: 0.149098 validation loss: 0.228942\n",
      "epoch: 73 train loss: 0.145347 validation loss: 0.247319\n",
      "epoch: 74 train loss: 0.146559 validation loss: 0.296326\n",
      "epoch: 75 train loss: 0.145799 validation loss: 0.267899\n",
      "epoch: 76 train loss: 0.144812 validation loss: 0.254834\n",
      "epoch: 77 train loss: 0.137632 validation loss: 0.223709\n",
      "epoch: 78 train loss: 0.130063 validation loss: 0.250859\n",
      "epoch: 79 train loss: 0.130746 validation loss: 0.234839\n",
      "epoch: 80 train loss: 0.130369 validation loss: 0.217252\n",
      "epoch: 81 train loss: 0.133428 validation loss: 0.361043\n",
      "epoch: 82 train loss: 0.137253 validation loss: 0.238856\n",
      "epoch: 83 train loss: 0.133402 validation loss: 0.243512\n",
      "epoch: 84 train loss: 0.129867 validation loss: 0.261617\n",
      "epoch: 85 train loss: 0.134035 validation loss: 0.222155\n",
      "epoch: 86 train loss: 0.127371 validation loss: 0.277756\n",
      "epoch: 87 train loss: 0.122608 validation loss: 0.214288\n",
      "epoch: 88 train loss: 0.120395 validation loss: 0.216847\n",
      "epoch: 89 train loss: 0.117886 validation loss: 0.245699\n",
      "epoch: 90 train loss: 0.123386 validation loss: 0.231844\n",
      "epoch: 91 train loss: 0.126199 validation loss: 0.300056\n",
      "epoch: 92 train loss: 0.126396 validation loss: 0.247196\n",
      "epoch: 93 train loss: 0.126005 validation loss: 0.287481\n",
      "epoch: 94 train loss: 0.119533 validation loss: 0.276664\n",
      "epoch: 95 train loss: 0.114916 validation loss: 0.314809\n",
      "epoch: 96 train loss: 0.119572 validation loss: 0.202559\n",
      "epoch: 97 train loss: 0.115056 validation loss: 0.214741\n",
      "epoch: 98 train loss: 0.116758 validation loss: 0.318867\n",
      "epoch: 99 train loss: 0.118563 validation loss: 0.214582\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "# Initialize lists to store loss and accuracy values\n",
    "loss_history = []\n",
    "loss_history_val = []\n",
    "\n",
    "loss_history_running = []\n",
    "\n",
    "# Early Stopping\n",
    "best_loss_val = np.inf\n",
    "early_stop_count = 0\n",
    "\n",
    "# Train\n",
    "print(\"Start pretrain...\")\n",
    "outer = tqdm(total=initial_train_epochs, desc='Epochs', position=0)\n",
    "inner = tqdm(total=len(dataloader_source_train), desc=f'Batch', position=1)\n",
    "for epoch in range(initial_train_epochs):\n",
    "\n",
    "    # check to see if validation loss has plateau'd\n",
    "    if early_stop_count >= early_stop_crit and epoch > min_epochs:\n",
    "        print(f'Validation loss plateaud; failed to improve after {early_stop_count} at {epoch}th epoch')\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': model,\n",
    "            'optimizer': pre_optimizer,\n",
    "            # 'scheduler': scheduler,\n",
    "            # 'scaler': scaler\n",
    "        }\n",
    "        torch.save(\n",
    "            checkpoint,\n",
    "            os.path.join(pretrain_folder, f'checkpt_earlystop_{epoch}.pth')\n",
    "        )\n",
    "        break\n",
    "\n",
    "    early_stop_count += 1\n",
    "\n",
    "    #Train mode\n",
    "    model.train()\n",
    "    loss_running = []\n",
    "    mean_weights = []\n",
    "\n",
    "    inner.refresh()  #force print final state\n",
    "    inner.reset()  #reuse bar\n",
    "    for _, (x, y_true) in enumerate(dataloader_source_train):\n",
    "        # lr_history_running.append(scheduler.get_last_lr())\n",
    "\n",
    "        pre_optimizer.zero_grad()\n",
    "        x = x.to(torch.float32).to(device)\n",
    "        y_true = y_true.to(torch.float32).to(device)\n",
    "\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = criterion(y_pred, y_true)\n",
    "\n",
    "        loss_running.append(loss.item())\n",
    "\n",
    "        # we will weight average by batch size later\n",
    "        mean_weights.append(len(x))\n",
    "\n",
    "        # scaler.scale(loss).backward()\n",
    "        # scaler.step(optimizer)\n",
    "        # scaler.update()\n",
    "\n",
    "        loss.backward()\n",
    "        pre_optimizer.step()\n",
    "        # Change the learning rate\n",
    "        # scheduler.step()\n",
    "\n",
    "        inner.update(1)\n",
    "\n",
    "    loss_history.append(np.average(loss_running, weights=mean_weights))\n",
    "\n",
    "    loss_history_running.append(loss_running)\n",
    "\n",
    "    # Evaluate mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        curr_loss_val = compute_acc(dataloader_source_train, model)\n",
    "        loss_history_val.append(curr_loss_val)\n",
    "\n",
    "    # Save the best weights\n",
    "    if curr_loss_val < best_loss_val:\n",
    "        best_loss_val = curr_loss_val\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': model,\n",
    "            'optimizer': pre_optimizer,\n",
    "            # 'scheduler': scheduler,\n",
    "            # 'scaler': scaler\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(pretrain_folder, f'best_model.pth'))\n",
    "        early_stop_count = 0\n",
    "\n",
    "    # Save checkpoint every 10\n",
    "    if epoch % 10 == 0 or epoch >= initial_train_epochs - 1:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': model,\n",
    "            'optimizer': pre_optimizer,\n",
    "            # 'scheduler': scheduler,\n",
    "            # 'scaler': scaler\n",
    "        }\n",
    "        torch.save(\n",
    "            checkpoint, os.path.join(pretrain_folder, f'checkpt{epoch}.pth')\n",
    "        )\n",
    "\n",
    "    # Print the results\n",
    "    outer.update(1)\n",
    "    print(\n",
    "        \"epoch:\", epoch, \\\n",
    "        \"train loss:\", round(loss_history[-1], 6), \\\n",
    "        \"validation loss:\", round(loss_history_val[-1], 6), \\\n",
    "        # \"next_lr:\", scheduler.get_last_lr(),\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    )\n",
    "\n",
    "# Save final model\n",
    "checkpoint = {\n",
    "    'epoch': epoch,\n",
    "    'model': model,\n",
    "    'optimizer': pre_optimizer,\n",
    "    # 'scheduler': scheduler,\n",
    "    # 'scaler': scaler\n",
    "}\n",
    "torch.save(checkpoint, os.path.join(pretrain_folder, f'final_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_iter(iter):\n",
    "    while True:\n",
    "        yield from iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 3000\n",
    "epochs = 1000\n",
    "alpha_lr = 5\n",
    "\n",
    "advtrain_folder = os.path.join(model_folder, \"advtrain\")\n",
    "\n",
    "if not os.path.isdir(advtrain_folder):\n",
    "    os.makedirs(advtrain_folder)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "target_optimizer = torch.optim.Adam(\n",
    "    model.target_encoder.parameters(),\n",
    "    lr=0.001,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-07\n",
    ")\n",
    "\n",
    "dis_optimizer = torch.optim.Adam(\n",
    "    model.dis.parameters(),\n",
    "    lr=alpha_lr * 0.001,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-07\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.autonotebook import tqdm\n",
    "\n",
    "# # Initialize lists to store loss and accuracy values\n",
    "# loss_history_running = []\n",
    "\n",
    "# batch_cycler_s = cycle_iter(dataloader_source_train)\n",
    "# batch_cycler_t = cycle_iter(dataloader_target_train)\n",
    "\n",
    "# # Train\n",
    "# print(\"Start adversarial training...\")\n",
    "# outer = tqdm(total=n_iter, desc='Iterations', position=0)\n",
    "\n",
    "# #Train mode\n",
    "# model.train()\n",
    "# model.dis.train()\n",
    "# model.target_encoder.train()\n",
    "\n",
    "# model.advtraining()\n",
    "\n",
    "# loss_running = []\n",
    "# accu_running = []\n",
    "# mean_weights = []\n",
    "\n",
    "# for iters in range(n_iter):\n",
    "#     model.train_discriminator()\n",
    "\n",
    "#     x_source, _ = next(batch_cycler_s)\n",
    "#     x_target = next(batch_cycler_t)\n",
    "#     x_source, x_target = x_source.to(device), x_target.to(device)\n",
    "\n",
    "#     dis_optimizer.zero_grad()\n",
    "#     y_dis = torch.cat(\n",
    "#         [\n",
    "#             torch.ones(x_source.shape[0], device=device, dtype=torch.long),\n",
    "#             torch.zeros(x_target.shape[0], device=device, dtype=torch.long)\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     emb_source = model.source_encoder(x_source).view(x_source.shape[0], -1)\n",
    "#     emb_target = model.target_encoder(x_target).view(x_target.shape[0], -1)\n",
    "\n",
    "#     emb_all = torch.cat((emb_source, emb_target))\n",
    "\n",
    "#     y_pred = model.dis(emb_all)\n",
    "\n",
    "#     loss = criterion(y_pred, y_dis)\n",
    "#     loss_running.append(loss.item())\n",
    "#     accu_running.append(\n",
    "#         torch.mean(\n",
    "#             (torch.flatten(torch.argmax(y_pred,\n",
    "#                                         dim=1)) == y_dis).to(torch.float32)\n",
    "#         )\n",
    "#     )\n",
    "#     # we will weight average by batch size later\n",
    "#     mean_weights.append(len(y_dis))\n",
    "\n",
    "#     # scaler.scale(loss).backward()\n",
    "#     # scaler.step(optimizer)\n",
    "#     # scaler.update()\n",
    "\n",
    "#     loss.backward()\n",
    "#     dis_optimizer.step()\n",
    "\n",
    "#     model.train_target_encoder()\n",
    "\n",
    "#     x_target = next(batch_cycler_t)\n",
    "#     x_target = x_target.to(device)\n",
    "\n",
    "#     target_optimizer.zero_grad()\n",
    "#     # flip label\n",
    "#     y_dis = torch.ones(x_target.shape[0], device=device, dtype=torch.long)\n",
    "\n",
    "#     emb_target = model.target_encoder(x_target).view(x_target.shape[0], -1)\n",
    "#     y_pred = model.dis(emb_target)\n",
    "#     loss = criterion(y_pred, y_dis)\n",
    "\n",
    "#     loss.backward()\n",
    "#     target_optimizer.step()\n",
    "\n",
    "#     # Save checkpoint every 100\n",
    "#     if iters % 100 == 99 or iters >= n_iter - 1:\n",
    "#         checkpoint = {\n",
    "#             'epoch': iters,\n",
    "#             'model': model,\n",
    "#             'dis_optimizer': dis_optimizer,\n",
    "#             'target_optimizer': target_optimizer,\n",
    "\n",
    "#             # 'scheduler': scheduler,\n",
    "#             # 'scaler': scaler\n",
    "#         }\n",
    "#         torch.save(\n",
    "#             checkpoint, os.path.join(advtrain_folder, f'checkpt{iters}.pth')\n",
    "#         )\n",
    "\n",
    "\n",
    "#         accu_running[-100:] = [x.cpu() for x in accu_running[-100:]]\n",
    "\n",
    "#         # Print the results\n",
    "#         print(\n",
    "#             \"iter:\", iters, \\\n",
    "#             \"dis loss:\", round(np.average(loss_running[-100:], weights=mean_weights[-100:]), 6), \\\n",
    "#             \"dis accu:\", round(np.average(accu_running[-100:], weights=mean_weights[-100:]), 6), \\\n",
    "#             # \"next_lr:\", scheduler.get_last_lr(),\n",
    "\n",
    "#         )\n",
    "\n",
    "#     outer.update(1)\n",
    "\n",
    "# # Save final model\n",
    "\n",
    "# checkpoint = {\n",
    "#     'epoch': n_iter,\n",
    "#     'model': model,\n",
    "#     'dis_optimizer': dis_optimizer,\n",
    "#     'target_optimizer': target_optimizer,\n",
    "\n",
    "#     # 'scheduler': scheduler,\n",
    "#     # 'scaler': scaler\n",
    "# }\n",
    "\n",
    "# torch.save(checkpoint, os.path.join(advtrain_folder, f'final_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start adversarial training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34d1563a55641f28ba96d98683c2135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228a4e9c9e324600a4677437b76fb9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Discriminator):   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d46e44b4bf46fa93b1652819ebed67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Encoder):   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 dis loss: 0.227603 dis accu: 0.906244\n",
      "epoch: 1 dis loss: 0.077983 dis accu: 0.976253\n",
      "epoch: 2 dis loss: 0.051713 dis accu: 0.985941\n",
      "epoch: 3 dis loss: 0.038761 dis accu: 0.98937\n",
      "epoch: 4 dis loss: 0.036881 dis accu: 0.989868\n",
      "epoch: 5 dis loss: 0.032352 dis accu: 0.990732\n",
      "epoch: 6 dis loss: 0.03141 dis accu: 0.991282\n",
      "epoch: 7 dis loss: 0.031006 dis accu: 0.991151\n",
      "epoch: 8 dis loss: 0.026954 dis accu: 0.992643\n",
      "epoch: 9 dis loss: 0.027392 dis accu: 0.992591\n",
      "epoch: 10 dis loss: 0.052258 dis accu: 0.983113\n",
      "Discriminator loss plateaud; failed to get worse after 10 at 11th epoch\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "# Initialize lists to store loss and accuracy values\n",
    "loss_history = []\n",
    "accu_history = []\n",
    "loss_history_running = []\n",
    "\n",
    "# Early Stopping\n",
    "best_loss_val = 0\n",
    "early_stop_count = 0\n",
    "\n",
    "batch_cycler = zip(cycle_iter(dataloader_source_train), cycle_iter(dataloader_target_train))\n",
    "iters = max(len(dataloader_source_train), len(dataloader_target_train))\n",
    "\n",
    "# Train\n",
    "print(\"Start adversarial training...\")\n",
    "outer = tqdm(total=epochs, desc='Epochs', position=0)\n",
    "inner1 = tqdm(total=iters, desc=f'Batch (Discriminator)', position=1)\n",
    "inner2 = tqdm(total=iters, desc=f'Batch (Encoder)', position=2)\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # check to see if validation loss has plateau'd\n",
    "    if early_stop_count >= early_stop_crit_adv and epoch > min_epochs_adv:\n",
    "        print(f'Discriminator loss plateaud; failed to get worse after {early_stop_count} at {epoch}th epoch')\n",
    "        checkpoint = {\n",
    "            'epoch': epochs,\n",
    "            'model': model,\n",
    "            'dis_optimizer': dis_optimizer,\n",
    "            'target_optimizer': target_optimizer,\n",
    "\n",
    "            # 'scheduler': scheduler,\n",
    "            # 'scaler': scaler\n",
    "        }\n",
    "        torch.save(\n",
    "            checkpoint,\n",
    "            os.path.join(advtrain_folder, f'checkpt_earlystop_{epoch}.pth')\n",
    "        )\n",
    "        break\n",
    "\n",
    "    early_stop_count += 1\n",
    "\n",
    "    #Train mode\n",
    "    model.train()\n",
    "    model.dis.train()\n",
    "    model.target_encoder.train()\n",
    "\n",
    "    loss_running = []\n",
    "    accu_running = []\n",
    "    mean_weights = []\n",
    "\n",
    "    model.train_discriminator()\n",
    "\n",
    "    inner1.refresh()  #force print final state\n",
    "    inner1.reset()  #reuse bar\n",
    "    inner2.refresh()  #force print final state\n",
    "    inner2.reset()  #reuse bar\n",
    "    for _ in range(iters):\n",
    "        # lr_history_running.append(scheduler.get_last_lr())\n",
    "\n",
    "        # for iters in range(n_iter):\n",
    "\n",
    "        (x_source, _), (x_target) = next(batch_cycler)\n",
    "        x_source, x_target = x_source.to(device), x_target.to(device)\n",
    "\n",
    "        dis_optimizer.zero_grad()\n",
    "        y_dis = torch.cat(\n",
    "            [\n",
    "                torch.ones(x_source.shape[0], device=device, dtype=torch.long),\n",
    "                torch.zeros(x_target.shape[0], device=device, dtype=torch.long)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        emb_source = model.source_encoder(x_source).view(x_source.shape[0], -1)\n",
    "        emb_target = model.target_encoder(x_target).view(x_target.shape[0], -1)\n",
    "\n",
    "        emb_all = torch.cat((emb_source, emb_target))\n",
    "\n",
    "        y_pred = model.dis(emb_all)\n",
    "\n",
    "        #     # we will weight average by batch size later\n",
    "        mean_weights.append(len(y_dis))\n",
    "\n",
    "        #     # scaler.scale(loss).backward()\n",
    "        #     # scaler.step(optimizer)\n",
    "        #     # scaler.update()\n",
    "\n",
    "        loss = criterion(y_pred, y_dis)\n",
    "        loss_running.append(loss.item())\n",
    "        accu_running.append(\n",
    "            torch.mean(\n",
    "                (torch.flatten(torch.argmax(y_pred, dim=1)) == y_dis).to(torch.float32)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # scaler.scale(loss).backward()\n",
    "        # scaler.step(optimizer)\n",
    "        # scaler.update()\n",
    "\n",
    "        loss.backward()\n",
    "        dis_optimizer.step()\n",
    "\n",
    "        inner1.update(1)\n",
    "\n",
    "    accu_running = [x.cpu() for x in accu_running]\n",
    "\n",
    "    loss_history.append(np.average(loss_running, weights=mean_weights))\n",
    "    accu_history.append(np.average(accu_running, weights=mean_weights))\n",
    "\n",
    "    loss_history_running.append(loss_running)\n",
    "\n",
    "    model.train_target_encoder()\n",
    "\n",
    "    for _ in range(iters):\n",
    "        _, x_target = next(batch_cycler)\n",
    "        x_target = x_target.to(device)\n",
    "\n",
    "        target_optimizer.zero_grad()\n",
    "\n",
    "        # flip label\n",
    "        y_dis = torch.ones(x_target.shape[0], device=device, dtype=torch.long)\n",
    "\n",
    "        emb_target = model.target_encoder(x_target).view(x_target.shape[0], -1)\n",
    "        y_pred = model.dis(emb_target)\n",
    "        loss = criterion(y_pred, y_dis)\n",
    "\n",
    "        loss.backward()\n",
    "        target_optimizer.step()\n",
    "\n",
    "        inner2.update(1)\n",
    "\n",
    "    # Save the best weights\n",
    "    if loss_history[-1] > best_loss_val:\n",
    "        best_loss_val = loss_history[-1]\n",
    "        checkpoint = {\n",
    "            'epoch': iters,\n",
    "            'model': model,\n",
    "            'dis_optimizer': dis_optimizer,\n",
    "            'target_optimizer': target_optimizer,\n",
    "\n",
    "            # 'scheduler': scheduler,\n",
    "            # 'scaler': scaler\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(advtrain_folder, f'best_model.pth'))\n",
    "\n",
    "        early_stop_count = 0\n",
    "\n",
    "    # Save checkpoint every 10\n",
    "    if epoch % 10 == 0 or epoch >= epochs - 1:\n",
    "        checkpoint = {\n",
    "            'epoch': iters,\n",
    "            'model': model,\n",
    "            'dis_optimizer': dis_optimizer,\n",
    "            'target_optimizer': target_optimizer,\n",
    "\n",
    "            # 'scheduler': scheduler,\n",
    "            # 'scaler': scaler\n",
    "        }\n",
    "        torch.save(\n",
    "            checkpoint, os.path.join(advtrain_folder, f'checkpt{iters}.pth')\n",
    "        )\n",
    "\n",
    "    # Print the results\n",
    "    outer.update(1)\n",
    "    print(\n",
    "        \"epoch:\", epoch, \\\n",
    "        \"dis loss:\", round(loss_history[-1], 6), \\\n",
    "        \"dis accu:\", round(accu_history[-1], 6), \\\n",
    "        # \"next_lr:\", scheduler.get_last_lr(),\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    )\n",
    "\n",
    "# Save final model\n",
    "checkpoint = {\n",
    "    'epoch': iters,\n",
    "    'model': model,\n",
    "    'dis_optimizer': dis_optimizer,\n",
    "    'target_optimizer': target_optimizer,\n",
    "\n",
    "    # 'scheduler': scheduler,\n",
    "    # 'scaler': scaler\n",
    "}\n",
    "torch.save(checkpoint, os.path.join(advtrain_folder, f'final_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8835, -0.1850,  0.0223]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.atleast_2d(torch.randn(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('agreda2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f612f3300285a6c63ee9982812f0c35696961988d9aa47061b62eefd45ca4bb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
