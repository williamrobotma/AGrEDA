{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # ADDA for ST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Creating something like CellDART but it actually follows Adda in PyTorch as a first step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3998065/3260998234.py:8: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "from itertools import count\n",
    "import warnings\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "from src.da_models.adda import ADDAST\n",
    "from src.da_models.datasets import SpotDataset\n",
    "from src.da_models.utils import set_requires_grad\n",
    "from src.da_models.utils import initialize_weights\n",
    "from src.utils.dupstdout import DupStdout\n",
    "from src.utils.data_loading import load_spatial, load_sc\n",
    "\n",
    "\n",
    "# datetime object containing current date and time\n",
    "script_start_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%Hh%Mm%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cpu\":\n",
    "    warnings.warn(\"Using CPU\", stacklevel=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_USING_ALL_ST_SAMPLES = False\n",
    "\n",
    "SAMPLE_ID_N = \"151673\"\n",
    "\n",
    "ST_SPLIT = False\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "NUM_WORKERS = 4\n",
    "INITIAL_TRAIN_EPOCHS = 100\n",
    "\n",
    "\n",
    "MIN_EPOCHS = 0.4 * INITIAL_TRAIN_EPOCHS\n",
    "EARLY_STOP_CRIT = INITIAL_TRAIN_EPOCHS\n",
    "\n",
    "PROCESSED_DATA_DIR = \"data/preprocessed_markers_standard\"\n",
    "\n",
    "MODEL_NAME = \"ADDA\"\n",
    "MODEL_VERSION = \"Standard1\"\n",
    "\n",
    "adda_kwargs = {\n",
    "    \"emb_dim\": 64,\n",
    "    \"bn_momentum\": 0.01,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adversarial Hyperparameters\n",
    "\n",
    "EPOCHS = 200\n",
    "MIN_EPOCHS_ADV = 0.4 * EPOCHS\n",
    "EARLY_STOP_CRIT_ADV = EPOCHS\n",
    "ENC_LR = 0.0002\n",
    "ADAM_BETA_1 = 0.5\n",
    "ALPHA = 2\n",
    "DIS_LOOP_FACTOR = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = os.path.join(\"model\", MODEL_NAME, script_start_time)\n",
    "\n",
    "model_folder = os.path.join(\"model\", MODEL_NAME, MODEL_VERSION)\n",
    "\n",
    "if not os.path.isdir(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "    print(model_folder)\n",
    "\n",
    "# if not os.path.isdir(results_folder):\n",
    "#     os.makedirs(results_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sc.logging.print_versions()\n",
    "# sc.set_figure_params(facecolor=\"white\", figsize=(8, 8))\n",
    "# sc.settings.verbosity = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spatial data\n",
    "mat_sp_d, mat_sp_train_s, st_sample_id_l = load_spatial(\n",
    "    TRAIN_USING_ALL_ST_SAMPLES, PROCESSED_DATA_DIR, ST_SPLIT\n",
    ")\n",
    "\n",
    "# Load sc data\n",
    "sc_mix_d, lab_mix_d, sc_sub_dict, sc_sub_dict2 = load_sc(PROCESSED_DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Training: Adversarial domain adaptation for cell fraction estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### source dataloaders\n",
    "source_train_set = SpotDataset(sc_mix_d[\"train\"], lab_mix_d[\"train\"])\n",
    "source_val_set = SpotDataset(sc_mix_d[\"val\"], lab_mix_d[\"val\"])\n",
    "source_test_set = SpotDataset(sc_mix_d[\"test\"], lab_mix_d[\"test\"])\n",
    "\n",
    "dataloader_source_train = torch.utils.data.DataLoader(\n",
    "    source_train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False,\n",
    ")\n",
    "dataloader_source_val = torch.utils.data.DataLoader(\n",
    "    source_val_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False,\n",
    ")\n",
    "dataloader_source_test = torch.utils.data.DataLoader(\n",
    "    source_test_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "### target dataloaders\n",
    "target_test_set_d = {}\n",
    "for sample_id in st_sample_id_l:\n",
    "    target_test_set_d[sample_id] = SpotDataset(mat_sp_d[\"test\"][sample_id])\n",
    "\n",
    "dataloader_target_test_d = {}\n",
    "for sample_id in st_sample_id_l:\n",
    "    dataloader_target_test_d[sample_id] = torch.utils.data.DataLoader(\n",
    "        target_test_set_d[sample_id],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "if TRAIN_USING_ALL_ST_SAMPLES:\n",
    "    target_train_set = SpotDataset(mat_sp_train_s)\n",
    "    dataloader_target_train = torch.utils.data.DataLoader(\n",
    "        target_train_set,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    target_train_set_dis = SpotDataset(deepcopy(mat_sp_train_s))\n",
    "    dataloader_target_train_dis = torch.utils.data.DataLoader(\n",
    "        target_train_set_dis,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "else:\n",
    "    target_train_set_d = {}\n",
    "    dataloader_target_train_d = {}\n",
    "\n",
    "    target_train_set_dis_d = {}\n",
    "    dataloader_target_train_dis_d = {}\n",
    "    for sample_id in st_sample_id_l:\n",
    "        target_train_set_d[sample_id] = SpotDataset(\n",
    "            deepcopy(mat_sp_d[\"train\"][sample_id])\n",
    "        )\n",
    "        dataloader_target_train_d[sample_id] = torch.utils.data.DataLoader(\n",
    "            target_train_set_d[sample_id],\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "\n",
    "        target_train_set_dis_d[sample_id] = SpotDataset(\n",
    "            deepcopy(mat_sp_d[\"train\"][sample_id])\n",
    "        )\n",
    "        dataloader_target_train_dis_d[sample_id] = torch.utils.data.DataLoader(\n",
    "            target_train_set_dis_d[sample_id],\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=False,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADDAST(\n",
       "  (source_encoder): ADDAMLPEncoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): Linear(in_features=367, out_features=1024, bias=True)\n",
       "      (1): BatchNorm1d(1024, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Dropout(p=0.5, inplace=False)\n",
       "      (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (5): BatchNorm1d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (6): LeakyReLU(negative_slope=0.01)\n",
       "      (7): Dropout(p=0.5, inplace=False)\n",
       "      (8): Linear(in_features=512, out_features=64, bias=True)\n",
       "      (9): ELU(alpha=1.0)\n",
       "    )\n",
       "  )\n",
       "  (clf): AddaPredictor(\n",
       "    (head): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=33, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ADDAST(\n",
    "    inp_dim=sc_mix_d[\"train\"].shape[1],\n",
    "    ncls_source=lab_mix_d[\"train\"].shape[1],\n",
    "    is_adda=True,\n",
    "    **adda_kwargs\n",
    ")\n",
    "model.apply(initialize_weights)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_folder = os.path.join(model_folder, \"pretrain\")\n",
    "\n",
    "if not os.path.isdir(pretrain_folder):\n",
    "    os.makedirs(pretrain_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=0.002, betas=(0.9, 0.999), eps=1e-07\n",
    ")\n",
    "\n",
    "pre_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    pre_optimizer,\n",
    "    max_lr=0.002,\n",
    "    steps_per_epoch=len(dataloader_source_train),\n",
    "    epochs=INITIAL_TRAIN_EPOCHS,\n",
    ")\n",
    "\n",
    "criterion_clf = nn.KLDivLoss(reduction=\"batchmean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(x, y_true, model):\n",
    "    x = x.to(torch.float32).to(device)\n",
    "    y_true = y_true.to(torch.float32).to(device)\n",
    "\n",
    "    y_pred = model(x)\n",
    "\n",
    "    loss = criterion_clf(y_pred, y_true)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_acc(dataloader, model):\n",
    "    loss_running = []\n",
    "    mean_weights = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(dataloader):\n",
    "\n",
    "            loss = model_loss(*batch, model)\n",
    "\n",
    "            loss_running.append(loss.item())\n",
    "\n",
    "            # we will weight average by batch size later\n",
    "            mean_weights.append(len(batch))\n",
    "\n",
    "    return np.average(loss_running, weights=mean_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pretraining()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start pretrain...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bbaf2ee09694637bf0f0b000e49cee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97fdac38b48e44bc9bdbe3e33473f192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train loss: 2.606392 validation loss: 1.814886 <-- new best val loss\n",
      "epoch: 1 train loss: 2.014261 validation loss: 1.447061 <-- new best val loss\n",
      "epoch: 2 train loss: 1.710186 validation loss: 1.283945 <-- new best val loss\n",
      "epoch: 3 train loss: 1.515462 validation loss: 1.162778 <-- new best val loss\n",
      "epoch: 4 train loss: 1.373006 validation loss: 1.07429 <-- new best val loss\n",
      "epoch: 5 train loss: 1.251432 validation loss: 1.015311 <-- new best val loss\n",
      "epoch: 6 train loss: 1.164843 validation loss: 0.961662 <-- new best val loss\n",
      "epoch: 7 train loss: 1.092277 validation loss: 0.915175 <-- new best val loss\n",
      "epoch: 8 train loss: 1.028818 validation loss: 0.877841 <-- new best val loss\n",
      "epoch: 9 train loss: 0.982976 validation loss: 0.844812 <-- new best val loss\n",
      "epoch: 10 train loss: 0.93964 validation loss: 0.812835 <-- new best val loss\n",
      "epoch: 11 train loss: 0.903315 validation loss: 0.78755 <-- new best val loss\n",
      "epoch: 12 train loss: 0.871271 validation loss: 0.762743 <-- new best val loss\n",
      "epoch: 13 train loss: 0.843275 validation loss: 0.740817 <-- new best val loss\n",
      "epoch: 14 train loss: 0.820207 validation loss: 0.726745 <-- new best val loss\n",
      "epoch: 15 train loss: 0.797478 validation loss: 0.715767 <-- new best val loss\n",
      "epoch: 16 train loss: 0.775619 validation loss: 0.698818 <-- new best val loss\n",
      "epoch: 17 train loss: 0.757372 validation loss: 0.68578 <-- new best val loss\n",
      "epoch: 18 train loss: 0.737828 validation loss: 0.675183 <-- new best val loss\n",
      "epoch: 19 train loss: 0.723191 validation loss: 0.663923 <-- new best val loss\n",
      "epoch: 20 train loss: 0.709203 validation loss: 0.657218 <-- new best val loss\n",
      "epoch: 21 train loss: 0.69437 validation loss: 0.645809 <-- new best val loss\n",
      "epoch: 22 train loss: 0.681174 validation loss: 0.638596 <-- new best val loss\n",
      "epoch: 23 train loss: 0.668604 validation loss: 0.631751 <-- new best val loss\n",
      "epoch: 24 train loss: 0.657804 validation loss: 0.624549 <-- new best val loss\n",
      "epoch: 25 train loss: 0.648214 validation loss: 0.617013 <-- new best val loss\n",
      "epoch: 26 train loss: 0.640661 validation loss: 0.613513 <-- new best val loss\n",
      "epoch: 27 train loss: 0.631438 validation loss: 0.60317 <-- new best val loss\n",
      "epoch: 28 train loss: 0.621971 validation loss: 0.597846 <-- new best val loss\n",
      "epoch: 29 train loss: 0.613898 validation loss: 0.59665 <-- new best val loss\n",
      "epoch: 30 train loss: 0.606305 validation loss: 0.592726 <-- new best val loss\n",
      "epoch: 31 train loss: 0.598553 validation loss: 0.58446 <-- new best val loss\n",
      "epoch: 32 train loss: 0.592066 validation loss: 0.582816 <-- new best val loss\n",
      "epoch: 33 train loss: 0.586036 validation loss: 0.582499 <-- new best val loss\n",
      "epoch: 34 train loss: 0.580471 validation loss: 0.57476 <-- new best val loss\n",
      "epoch: 35 train loss: 0.574612 validation loss: 0.57919 \n",
      "epoch: 36 train loss: 0.568544 validation loss: 0.574595 <-- new best val loss\n",
      "epoch: 37 train loss: 0.563612 validation loss: 0.568635 <-- new best val loss\n",
      "epoch: 38 train loss: 0.558683 validation loss: 0.568576 <-- new best val loss\n",
      "epoch: 39 train loss: 0.554472 validation loss: 0.56622 <-- new best val loss\n",
      "epoch: 40 train loss: 0.54825 validation loss: 0.564037 <-- new best val loss\n",
      "epoch: 41 train loss: 0.544831 validation loss: 0.560756 <-- new best val loss\n",
      "epoch: 42 train loss: 0.540755 validation loss: 0.562177 \n",
      "epoch: 43 train loss: 0.534984 validation loss: 0.559962 <-- new best val loss\n",
      "epoch: 44 train loss: 0.532915 validation loss: 0.556612 <-- new best val loss\n",
      "epoch: 45 train loss: 0.528745 validation loss: 0.556434 <-- new best val loss\n",
      "epoch: 46 train loss: 0.524635 validation loss: 0.55775 \n",
      "epoch: 47 train loss: 0.521286 validation loss: 0.553265 <-- new best val loss\n",
      "epoch: 48 train loss: 0.517616 validation loss: 0.552815 <-- new best val loss\n",
      "epoch: 49 train loss: 0.513368 validation loss: 0.55061 <-- new best val loss\n",
      "epoch: 50 train loss: 0.510267 validation loss: 0.550821 \n",
      "epoch: 51 train loss: 0.506925 validation loss: 0.550529 <-- new best val loss\n",
      "epoch: 52 train loss: 0.503991 validation loss: 0.550979 \n",
      "epoch: 53 train loss: 0.500724 validation loss: 0.548633 <-- new best val loss\n",
      "epoch: 54 train loss: 0.496996 validation loss: 0.550889 \n",
      "epoch: 55 train loss: 0.495785 validation loss: 0.552422 \n",
      "epoch: 56 train loss: 0.492336 validation loss: 0.548968 \n",
      "epoch: 57 train loss: 0.489288 validation loss: 0.548163 <-- new best val loss\n",
      "epoch: 58 train loss: 0.486058 validation loss: 0.547719 <-- new best val loss\n",
      "epoch: 59 train loss: 0.481906 validation loss: 0.549467 \n",
      "epoch: 60 train loss: 0.480584 validation loss: 0.547892 \n",
      "epoch: 61 train loss: 0.478296 validation loss: 0.547101 <-- new best val loss\n",
      "epoch: 62 train loss: 0.47408 validation loss: 0.547911 \n",
      "epoch: 63 train loss: 0.473357 validation loss: 0.547876 \n",
      "epoch: 64 train loss: 0.47121 validation loss: 0.546852 <-- new best val loss\n",
      "epoch: 65 train loss: 0.466961 validation loss: 0.546329 <-- new best val loss\n",
      "epoch: 66 train loss: 0.46496 validation loss: 0.547152 \n",
      "epoch: 67 train loss: 0.463224 validation loss: 0.545344 <-- new best val loss\n",
      "epoch: 68 train loss: 0.460268 validation loss: 0.545614 \n",
      "epoch: 69 train loss: 0.458644 validation loss: 0.545713 \n",
      "epoch: 70 train loss: 0.455718 validation loss: 0.545956 \n",
      "epoch: 71 train loss: 0.457149 validation loss: 0.545257 <-- new best val loss\n",
      "epoch: 72 train loss: 0.454284 validation loss: 0.545205 <-- new best val loss\n",
      "epoch: 73 train loss: 0.453232 validation loss: 0.545211 \n",
      "epoch: 74 train loss: 0.449655 validation loss: 0.54481 <-- new best val loss\n",
      "epoch: 75 train loss: 0.449218 validation loss: 0.545682 \n",
      "epoch: 76 train loss: 0.447357 validation loss: 0.545157 \n",
      "epoch: 77 train loss: 0.446258 validation loss: 0.545354 \n",
      "epoch: 78 train loss: 0.444873 validation loss: 0.545756 \n",
      "epoch: 79 train loss: 0.443369 validation loss: 0.545945 \n",
      "epoch: 80 train loss: 0.441547 validation loss: 0.545686 \n",
      "epoch: 81 train loss: 0.443186 validation loss: 0.545649 \n",
      "epoch: 82 train loss: 0.439982 validation loss: 0.544967 \n",
      "epoch: 83 train loss: 0.43807 validation loss: 0.545186 \n",
      "epoch: 84 train loss: 0.439504 validation loss: 0.545211 \n",
      "epoch: 85 train loss: 0.439454 validation loss: 0.545231 \n",
      "epoch: 86 train loss: 0.437609 validation loss: 0.54509 \n",
      "epoch: 87 train loss: 0.436741 validation loss: 0.544926 \n",
      "epoch: 88 train loss: 0.436869 validation loss: 0.544905 \n",
      "epoch: 89 train loss: 0.436184 validation loss: 0.54486 \n",
      "epoch: 90 train loss: 0.437275 validation loss: 0.54483 \n",
      "epoch: 91 train loss: 0.435754 validation loss: 0.545019 \n",
      "epoch: 92 train loss: 0.43484 validation loss: 0.544919 \n",
      "epoch: 93 train loss: 0.435286 validation loss: 0.545053 \n",
      "epoch: 94 train loss: 0.435264 validation loss: 0.545069 \n",
      "epoch: 95 train loss: 0.434369 validation loss: 0.545049 \n",
      "epoch: 96 train loss: 0.434283 validation loss: 0.545055 \n",
      "epoch: 97 train loss: 0.433919 validation loss: 0.545045 \n",
      "epoch: 98 train loss: 0.435422 validation loss: 0.545061 \n",
      "epoch: 99 train loss: 0.435475 validation loss: 0.545068 \n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store loss and accuracy values\n",
    "loss_history = []\n",
    "loss_history_val = []\n",
    "\n",
    "loss_history_running = []\n",
    "\n",
    "# Early Stopping\n",
    "best_loss_val = np.inf\n",
    "early_stop_count = 0\n",
    "\n",
    "with DupStdout().dup_to_file(os.path.join(pretrain_folder, \"log.txt\"), \"w\") as f_log:\n",
    "    # Train\n",
    "    print(\"Start pretrain...\")\n",
    "    outer = tqdm(total=INITIAL_TRAIN_EPOCHS, desc=\"Epochs\", position=0)\n",
    "    inner = tqdm(total=len(dataloader_source_train), desc=f\"Batch\", position=1)\n",
    "\n",
    "    checkpoint = {\n",
    "        \"epoch\": -1,\n",
    "        \"model\": model,\n",
    "        \"optimizer\": pre_optimizer,\n",
    "        \"scheduler\": pre_scheduler,\n",
    "        # 'scaler': scaler\n",
    "    }\n",
    "    for epoch in range(INITIAL_TRAIN_EPOCHS):\n",
    "        checkpoint[\"epoch\"] = epoch\n",
    "\n",
    "        # Train mode\n",
    "        model.train()\n",
    "        loss_running = []\n",
    "        mean_weights = []\n",
    "\n",
    "        inner.refresh()  # force print final state\n",
    "        inner.reset()  # reuse bar\n",
    "        for _, batch in enumerate(dataloader_source_train):\n",
    "            # lr_history_running.append(scheduler.get_last_lr())\n",
    "\n",
    "            pre_optimizer.zero_grad()\n",
    "            loss = model_loss(*batch, model)\n",
    "            loss_running.append(loss.item())\n",
    "            mean_weights.append(\n",
    "                len(batch)\n",
    "            )  # we will weight average by batch size later\n",
    "\n",
    "            # scaler.scale(loss).backward()\n",
    "            # scaler.step(optimizer)\n",
    "            # scaler.update()\n",
    "\n",
    "            loss.backward()\n",
    "            pre_optimizer.step()\n",
    "            pre_scheduler.step()\n",
    "\n",
    "            inner.update(1)\n",
    "\n",
    "        loss_history.append(np.average(loss_running, weights=mean_weights))\n",
    "        loss_history_running.append(loss_running)\n",
    "\n",
    "        # Evaluate mode\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            curr_loss_val = compute_acc(dataloader_source_val, model)\n",
    "            loss_history_val.append(curr_loss_val)\n",
    "\n",
    "        # Print the results\n",
    "        outer.update(1)\n",
    "        print(\n",
    "            \"epoch:\",\n",
    "            epoch,\n",
    "            \"train loss:\",\n",
    "            round(loss_history[-1], 6),\n",
    "            \"validation loss:\",\n",
    "            round(loss_history_val[-1], 6),\n",
    "            # \"next_lr:\", scheduler.get_last_lr(),\n",
    "            end=\" \",\n",
    "        )\n",
    "        # Save the best weights\n",
    "        if curr_loss_val < best_loss_val:\n",
    "            best_loss_val = curr_loss_val\n",
    "            torch.save(checkpoint, os.path.join(pretrain_folder, f\"best_model.pth\"))\n",
    "            early_stop_count = 0\n",
    "\n",
    "            print(\"<-- new best val loss\")\n",
    "        else:\n",
    "            print(\"\")\n",
    "\n",
    "        # Save checkpoint every 10\n",
    "        if epoch % 10 == 0 or epoch >= INITIAL_TRAIN_EPOCHS - 1:\n",
    "            torch.save(checkpoint, os.path.join(pretrain_folder, f\"checkpt{epoch}.pth\"))\n",
    "\n",
    "        # check to see if validation loss has plateau'd\n",
    "        if early_stop_count >= EARLY_STOP_CRIT and epoch >= MIN_EPOCHS - 1:\n",
    "            print(\n",
    "                f\"Validation loss plateaued after {early_stop_count} at epoch {epoch}\"\n",
    "            )\n",
    "            torch.save(\n",
    "                checkpoint, os.path.join(pretrain_folder, f\"earlystop{epoch}.pth\")\n",
    "            )\n",
    "            break\n",
    "\n",
    "        early_stop_count += 1\n",
    "\n",
    "    # Save final model\n",
    "    best_checkpoint = torch.load(os.path.join(pretrain_folder, f\"best_model.pth\"))\n",
    "    torch.save(best_checkpoint, os.path.join(pretrain_folder, f\"final_model.pth\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Adversarial Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "advtrain_folder = os.path.join(model_folder, \"advtrain\")\n",
    "\n",
    "if not os.path.isdir(advtrain_folder):\n",
    "    os.makedirs(advtrain_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_iter(iter):\n",
    "    while True:\n",
    "        yield from iter\n",
    "\n",
    "\n",
    "def iter_skip(iter, n=1):\n",
    "    for i in range(len(iter) * n):\n",
    "        if (i % n) == n - 1:\n",
    "            yield next(iter)\n",
    "        else:\n",
    "            yield None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_dis = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrim_loss_accu(x, domain, model):\n",
    "    x = x.to(device)\n",
    "\n",
    "    if domain == \"source\":\n",
    "        y_dis = torch.zeros(x.shape[0], device=device, dtype=x.dtype).view(-1, 1)\n",
    "        emb = model.source_encoder(x)  # .view(x.shape[0], -1)\n",
    "    elif domain == \"target\":\n",
    "        y_dis = torch.ones(x.shape[0], device=device, dtype=x.dtype).view(-1, 1)\n",
    "        emb = model.target_encoder(x)  # .view(x.shape[0], -1)\n",
    "    else:\n",
    "        raise (\n",
    "            ValueError,\n",
    "            f\"invalid domain {domain} given, must be 'source' or 'target'\",\n",
    "        )\n",
    "\n",
    "    y_pred = model.dis(emb)\n",
    "\n",
    "    loss = criterion_dis(y_pred, y_dis)\n",
    "    accu = torch.mean(\n",
    "        (torch.round(torch.sigmoid(y_pred)).to(torch.long) == y_dis).to(torch.float32)\n",
    "    ).cpu()\n",
    "\n",
    "    return loss, accu\n",
    "\n",
    "\n",
    "# def discrim_loss_accu(x_source, x_target, model):\n",
    "#     # x = x.to(device)\n",
    "\n",
    "#     x_source, x_target = x_source.to(device), x_target.to(device)\n",
    "\n",
    "#     # if domain == 'source':\n",
    "#     #     y_dis = torch.zeros(x.shape[0], device=device, dtype=x.dtype).view(-1, 1)\n",
    "#     #     emb = model.source_encoder(x) #.view(x.shape[0], -1)\n",
    "#     # elif domain == 'target':\n",
    "#     #     y_dis = torch.ones(x.shape[0], device=device, dtype=x.dtype).view(-1, 1)\n",
    "#     #     emb = model.target_encoder(x) #.view(x.shape[0], -1)\n",
    "#     # else:\n",
    "#     #     raise(ValueError, f\"invalid domain {domain} given, must be 'source' or 'target'\")\n",
    "\n",
    "#     y_dis = torch.cat(\n",
    "#         [\n",
    "#             torch.zeros(x_source.shape[0], device=device, dtype=x_source.dtype).view(\n",
    "#                 -1, 1\n",
    "#             ),\n",
    "#             torch.ones(x_target.shape[0], device=device, dtype=x_target.dtype).view(\n",
    "#                 -1, 1\n",
    "#             ),\n",
    "#         ]\n",
    "#     )\n",
    "#     x = torch.cat([x_source, x_target])\n",
    "#     emb = model.source_encoder(x)  # .view(x.shape[0], -1)\n",
    "#     y_pred = model.dis(emb)\n",
    "\n",
    "#     loss = criterion_dis(y_pred, y_dis)\n",
    "#     accu = torch.mean(\n",
    "#         (torch.round(y_pred).to(torch.long) == y_dis).to(torch.float32)\n",
    "#     ).cpu()\n",
    "\n",
    "#     return loss, accu\n",
    "\n",
    "\n",
    "def compute_acc_dis(dataloader_source, dataloader_target, model):\n",
    "    loss_history = []\n",
    "    accu_history = []\n",
    "    # iters = max(len(dataloader_source), len(dataloader_target))\n",
    "    model.eval()\n",
    "    model.dis.eval()\n",
    "    model.target_encoder.eval()\n",
    "    model.source_encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_running = []\n",
    "        accu_running = []\n",
    "        mean_weights = []\n",
    "        # batch_cycler = zip(cycle_iter(dataloader_source), cycle_iter(dataloader_target))\n",
    "        for _, (X, _) in enumerate(dataloader_source):\n",
    "            X = X.to(device)\n",
    "\n",
    "            y_dis = torch.zeros(X.shape[0], device=device, dtype=X.dtype).view(-1, 1)\n",
    "\n",
    "            emb = model.source_encoder(X)  # .view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = model.dis(emb)\n",
    "\n",
    "            loss_running.append(criterion_dis(y_pred, y_dis).item())\n",
    "            accu_running.append(\n",
    "                torch.mean(\n",
    "                    (\n",
    "                        torch.round(torch.sigmoid(y_pred.detach(), dim=1)).to(\n",
    "                            torch.long\n",
    "                        )\n",
    "                        == y_dis.detach()\n",
    "                    ).to(torch.float32)\n",
    "                ).cpu()\n",
    "            )\n",
    "        loss_history.append(np.average(loss_running, weights=mean_weights))\n",
    "        accu_history.append(np.average(accu_running, weights=mean_weights))\n",
    "\n",
    "        loss_running = []\n",
    "        accu_running = []\n",
    "        mean_weights = []\n",
    "        for _, (X, _) in enumerate(dataloader_target):\n",
    "            X = X.to(device)\n",
    "\n",
    "            y_dis = torch.ones(X.shape[0], device=device, dtype=X.dtype).view(-1, 1)\n",
    "\n",
    "            emb = model.source_encoder(X)  # .view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = model.dis(emb)\n",
    "\n",
    "            loss_running.append(criterion_dis(y_pred, y_dis).item())\n",
    "            accu_running.append(\n",
    "                torch.mean(\n",
    "                    (\n",
    "                        torch.round(torch.sigmoid(y_pred.detach(), dim=1)).to(\n",
    "                            torch.long\n",
    "                        )\n",
    "                        == y_dis.detach()\n",
    "                    ).to(torch.float32)\n",
    "                ).cpu()\n",
    "            )\n",
    "        loss_history.append(np.average(loss_running, weights=mean_weights))\n",
    "        accu_history.append(np.average(accu_running, weights=mean_weights))\n",
    "\n",
    "    return np.average(loss_history), np.average(accu_history)\n",
    "\n",
    "\n",
    "def encoder_loss(x_target, model):\n",
    "    x_target = x_target.to(device)\n",
    "\n",
    "    # flip label\n",
    "    y_dis = torch.zeros(x_target.shape[0], device=device, dtype=x_target.dtype).view(\n",
    "        -1, 1\n",
    "    )\n",
    "\n",
    "    emb_target = model.target_encoder(x_target)  # .view(x_target.shape[0], -1)\n",
    "    y_pred = model.dis(emb_target)\n",
    "    loss = criterion_dis(y_pred, y_dis)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_adversarial(\n",
    "#     model,\n",
    "#     save_folder,\n",
    "#     dataloader_source_train,\n",
    "#     dataloader_source_val,\n",
    "#     dataloader_target_train,\n",
    "# ):\n",
    "#     model.to(device)\n",
    "#     model.advtraining()\n",
    "\n",
    "#     target_optimizer = torch.optim.Adam(\n",
    "#         model.target_encoder.parameters(), lr=0.0005, betas=(0.9, 0.999), eps=1e-07\n",
    "#     )\n",
    "#     dis_optimizer = torch.optim.Adam(\n",
    "#         model.dis.parameters(), lr=0.00025, betas=(0.9, 0.999), eps=1e-07\n",
    "#     )\n",
    "\n",
    "#     iters = max(len(dataloader_source_train), len(dataloader_target_train))\n",
    "\n",
    "#     dis_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "#         dis_optimizer, max_lr=0.0005, steps_per_epoch=iters, epochs=EPOCHS\n",
    "#     )\n",
    "#     target_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "#         target_optimizer, max_lr=0.0005, steps_per_epoch=iters, epochs=EPOCHS\n",
    "#     )\n",
    "\n",
    "#     n_samples_source = len(dataloader_source_train.dataset)\n",
    "#     n_samples_target = len(dataloader_target_train.dataset)\n",
    "#     p = n_samples_source / (n_samples_source + n_samples_target)\n",
    "#     rand_loss = -(p * np.log(0.5)) - (1 - p) * np.log(0.5)\n",
    "\n",
    "#     # Initialize lists to store loss and accuracy values\n",
    "#     loss_history = []\n",
    "#     accu_history = []\n",
    "#     loss_history_running = []\n",
    "\n",
    "#     # Early Stopping\n",
    "#     best_loss_val = np.inf\n",
    "#     early_stop_count = 0\n",
    "\n",
    "#     # Train\n",
    "#     print(\"Start adversarial training...\")\n",
    "#     print(\"Discriminator target loss:\", rand_loss)\n",
    "#     outer = tqdm(total=EPOCHS, desc=\"Epochs\", position=0)\n",
    "#     inner1 = tqdm(total=iters, desc=f\"Batch (Discriminator)\", position=1)\n",
    "#     inner2 = tqdm(total=iters, desc=f\"Batch (Encoder)\", position=2)\n",
    "#     checkpoint = {\n",
    "#         \"epoch\": -1,\n",
    "#         \"model\": model,\n",
    "#         \"dis_optimizer\": dis_optimizer,\n",
    "#         \"target_optimizer\": target_optimizer,\n",
    "#         \"dis_scheduler\": dis_scheduler,\n",
    "#         \"target_scheduler\": target_scheduler,\n",
    "#     }\n",
    "#     for epoch in range(EPOCHS):\n",
    "#         checkpoint[\"epoch\"] = epoch\n",
    "\n",
    "#         # Train mode\n",
    "#         model.train()\n",
    "\n",
    "#         loss_running = []\n",
    "#         accu_running = []\n",
    "#         mean_weights = []\n",
    "\n",
    "#         inner1.refresh()  # force print final state\n",
    "#         inner1.reset()  # reuse bar\n",
    "#         inner2.refresh()  # force print final state\n",
    "#         inner2.reset()  # reuse bar\n",
    "\n",
    "#         model.train_discriminator()\n",
    "#         model.target_encoder.eval()\n",
    "#         model.source_encoder.eval()\n",
    "#         model.dis.train()\n",
    "#         batch_cycler = zip(\n",
    "#             cycle_iter(dataloader_source_train), cycle_iter(dataloader_target_train)\n",
    "#         )\n",
    "#         for _ in range(iters):\n",
    "#             # lr_history_running.append(scheduler.get_last_lr())\n",
    "#             dis_optimizer.zero_grad()\n",
    "\n",
    "#             (x_source, _), (x_target, _) = next(batch_cycler)\n",
    "#             loss, accu = discrim_loss_accu(x_source, x_target, model)\n",
    "#             loss_running.append(loss.item())\n",
    "#             accu_running.append(accu)\n",
    "#             mean_weights.append(len(x_source) + len(x_target))\n",
    "\n",
    "#             # scaler.scale(loss).backward()\n",
    "#             # scaler.step(optimizer)\n",
    "#             # scaler.update()\n",
    "\n",
    "#             loss.backward()\n",
    "#             dis_optimizer.step()\n",
    "#             dis_scheduler.step()\n",
    "\n",
    "#             inner1.update(1)\n",
    "\n",
    "#         loss_history.append(np.average(loss_running, weights=mean_weights))\n",
    "#         accu_history.append(np.average(accu_running, weights=mean_weights))\n",
    "#         loss_history_running.append(loss_running)\n",
    "\n",
    "#         model.train_target_encoder()\n",
    "#         model.target_encoder.train()\n",
    "#         model.source_encoder.eval()\n",
    "#         model.dis.eval()\n",
    "#         batch_cycler = zip(\n",
    "#             cycle_iter(dataloader_source_train), cycle_iter(dataloader_target_train)\n",
    "#         )\n",
    "#         for _ in range(iters):\n",
    "#             target_optimizer.zero_grad()\n",
    "\n",
    "#             _, (x_target, _) = next(batch_cycler)\n",
    "#             loss = encoder_loss(x_target, model)\n",
    "\n",
    "#             loss.backward()\n",
    "#             target_optimizer.step()\n",
    "#             target_scheduler.step()\n",
    "\n",
    "#             inner2.update(1)\n",
    "\n",
    "#         diff_from_rand = math.fabs(loss_history[-1] - rand_loss)\n",
    "\n",
    "#         # Print the results\n",
    "#         outer.update(1)\n",
    "#         print(\n",
    "#             \"epoch:\",\n",
    "#             epoch,\n",
    "#             \"dis loss:\",\n",
    "#             round(loss_history[-1], 6),\n",
    "#             \"dis accu:\",\n",
    "#             round(accu_history[-1], 6),\n",
    "#             \"difference from random loss:\",\n",
    "#             round(diff_from_rand, 6),\n",
    "#             # \"next_lr:\", scheduler.get_last_lr(),\n",
    "#             end=\" \",\n",
    "#         )\n",
    "\n",
    "#         # Save the best weights\n",
    "#         if diff_from_rand < best_loss_val:\n",
    "#             best_loss_val = diff_from_rand\n",
    "#             torch.save(checkpoint, os.path.join(save_folder, f\"best_model.pth\"))\n",
    "#             early_stop_count = 0\n",
    "\n",
    "#             print(\"<-- new best difference from random loss\")\n",
    "#         else:\n",
    "#             print(\"\")\n",
    "\n",
    "#         # Save checkpoint every 10\n",
    "#         if epoch % 10 == 0 or epoch >= EPOCHS - 1:\n",
    "#             torch.save(checkpoint, os.path.join(save_folder, f\"checkpt{epoch}.pth\"))\n",
    "\n",
    "#         # check to see if validation loss has plateau'd\n",
    "#         if early_stop_count >= EARLY_STOP_CRIT_ADV and epoch > MIN_EPOCHS_ADV - 1:\n",
    "#             print(\n",
    "#                 f\"Discriminator loss plateaued after {early_stop_count} at epoch {epoch}\"\n",
    "#             )\n",
    "#             torch.save(checkpoint, os.path.join(save_folder, f\"earlystop_{epoch}.pth\"))\n",
    "#             break\n",
    "\n",
    "#         early_stop_count += 1\n",
    "\n",
    "#     # Save final model\n",
    "#     torch.save(checkpoint, os.path.join(save_folder, f\"final_model.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversarial_iters(\n",
    "    model,\n",
    "    save_folder,\n",
    "    dataloader_source_train,\n",
    "    dataloader_source_val,\n",
    "    dataloader_target_train,\n",
    "    dataloader_target_train_dis,\n",
    "):\n",
    "    model.to(device)\n",
    "    model.advtraining()\n",
    "\n",
    "    target_optimizer = torch.optim.Adam(\n",
    "        model.target_encoder.parameters(),\n",
    "        lr=ENC_LR,\n",
    "        betas=(ADAM_BETA_1, 0.999),\n",
    "        eps=1e-07,\n",
    "    )\n",
    "    dis_optimizer = torch.optim.Adam(\n",
    "        model.dis.parameters(), lr=ALPHA * ENC_LR, betas=(ADAM_BETA_1, 0.999), eps=1e-07\n",
    "    )\n",
    "\n",
    "    # iters = -(max_len_dataloader // -(1 + DIS_LOOP_FACTOR))  # ceiling divide\n",
    "\n",
    "    dataloader_lengths = [\n",
    "        len(dataloader_source_train),\n",
    "        len(dataloader_target_train),\n",
    "        len(dataloader_target_train_dis) * DIS_LOOP_FACTOR,\n",
    "    ]\n",
    "    max_len_dataloader = np.amax(dataloader_lengths)\n",
    "    longest = np.argmax(dataloader_lengths)\n",
    "\n",
    "    iters_val = max(len(dataloader_source_val), len(dataloader_target_train))\n",
    "\n",
    "    # dis_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    #     dis_optimizer, max_lr=0.0005, steps_per_epoch=iters, epochs=EPOCHS\n",
    "    # )\n",
    "    # target_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    #     target_optimizer, max_lr=0.0005, steps_per_epoch=iters, epochs=EPOCHS\n",
    "    # )\n",
    "\n",
    "    # Initialize lists to store loss and accuracy values\n",
    "    loss_history = []\n",
    "    accu_history = []\n",
    "    loss_history_val = []\n",
    "    accu_history_val = []\n",
    "    loss_history_running = []\n",
    "\n",
    "    loss_history_gen = []\n",
    "    loss_history_gen_running = []\n",
    "    mean_weights_gen = []\n",
    "\n",
    "    # Early Stopping\n",
    "    best_loss_val = np.inf\n",
    "    early_stop_count = 0\n",
    "    with DupStdout().dup_to_file(os.path.join(save_folder, \"log.txt\"), \"w\") as f_log:\n",
    "        # Train\n",
    "        print(\"Start adversarial training...\")\n",
    "        outer = tqdm(total=EPOCHS, desc=\"Epochs\", position=0)\n",
    "        inner1 = tqdm(total=max_len_dataloader, desc=f\"Batch\", position=1)\n",
    "        checkpoint = {\n",
    "            \"epoch\": -1,\n",
    "            \"model\": model,\n",
    "            \"dis_optimizer\": dis_optimizer,\n",
    "            \"target_optimizer\": target_optimizer,\n",
    "            # \"dis_scheduler\": dis_scheduler,\n",
    "            # \"target_scheduler\": target_scheduler,\n",
    "        }\n",
    "        for epoch in range(EPOCHS):\n",
    "            checkpoint[\"epoch\"] = epoch\n",
    "\n",
    "            # Train mode\n",
    "            model.train()\n",
    "            model.target_encoder.train()\n",
    "            model.source_encoder.eval()\n",
    "            model.dis.train()\n",
    "\n",
    "            loss_running = []\n",
    "            accu_running = []\n",
    "            mean_weights = []\n",
    "\n",
    "            loss_running_gen = []\n",
    "            mean_weights_gen = []\n",
    "\n",
    "            inner1.refresh()  # force print final state\n",
    "            inner1.reset()  # reuse bar\n",
    "\n",
    "            s_train_iter = iter(dataloader_source_train)\n",
    "            t_train_iter = iter(dataloader_target_train)\n",
    "            t_train_dis_iter = iter(dataloader_target_train_dis)\n",
    "            for i in range(max_len_dataloader):\n",
    "                try:\n",
    "                    x_source, _ = next(s_train_iter)\n",
    "                except StopIteration:\n",
    "                    s_train_iter = iter(dataloader_source_train)\n",
    "                    x_source, _ = next(s_train_iter)\n",
    "                try:\n",
    "                    x_target, _ = next(t_train_iter)\n",
    "                except StopIteration:\n",
    "                    t_train_iter = iter(dataloader_target_train)\n",
    "                    x_target, _ = next(t_train_iter)\n",
    "\n",
    "                train_encoder_step = (i % DIS_LOOP_FACTOR) == DIS_LOOP_FACTOR - 1\n",
    "\n",
    "                model.train_discriminator()\n",
    "                # model.target_encoder.train()\n",
    "                # model.source_encoder.train()\n",
    "                # model.dis.train()\n",
    "\n",
    "                set_requires_grad(model.target_encoder, False)\n",
    "                set_requires_grad(model.source_encoder, False)\n",
    "                set_requires_grad(model.dis, True)\n",
    "\n",
    "                # lr_history_running.append(scheduler.get_last_lr())\n",
    "                dis_optimizer.zero_grad()\n",
    "\n",
    "                loss, accu = discrim_loss_accu(x_source, \"source\", model)\n",
    "                loss_running.append(loss.item())\n",
    "                accu_running.append(accu)\n",
    "                mean_weights.append(len(x_source))\n",
    "\n",
    "                # scaler.scale(loss).backward()\n",
    "                # scaler.step(optimizer)\n",
    "                # scaler.update()\n",
    "\n",
    "                loss.backward()\n",
    "                dis_optimizer.step()\n",
    "\n",
    "                dis_optimizer.zero_grad()\n",
    "\n",
    "                loss, accu = discrim_loss_accu(x_target, \"target\", model)\n",
    "                loss_running.append(loss.item())\n",
    "                accu_running.append(accu)\n",
    "                mean_weights.append(len(x_target))\n",
    "\n",
    "                # scaler.scale(loss).backward()\n",
    "                # scaler.step(optimizer)\n",
    "                # scaler.update()\n",
    "\n",
    "                loss.backward()\n",
    "                dis_optimizer.step()\n",
    "                # dis_scheduler.step()\n",
    "\n",
    "                # print(i % DIS_LOOP_FACTOR)\n",
    "                if train_encoder_step:\n",
    "                    try:\n",
    "                        x_target_enc, _ = next(t_train_dis_iter)\n",
    "                    except StopIteration:\n",
    "                        t_train_dis_iter = iter(dataloader_target_train_dis)\n",
    "                        x_target_enc, _ = next(t_train_dis_iter)\n",
    "                    model.train_target_encoder()\n",
    "                    # model.target_encoder.train()\n",
    "                    # model.source_encoder.train()\n",
    "                    # model.dis.train()\n",
    "\n",
    "                    set_requires_grad(model.target_encoder, True)\n",
    "                    set_requires_grad(model.source_encoder, False)\n",
    "                    set_requires_grad(model.dis, False)\n",
    "\n",
    "                    target_optimizer.zero_grad()\n",
    "\n",
    "                    loss = encoder_loss(x_target_enc, model)\n",
    "\n",
    "                    loss_running_gen.append(loss.item())\n",
    "                    mean_weights_gen.append(len(x_target_enc))\n",
    "\n",
    "                    loss.backward()\n",
    "                    target_optimizer.step()\n",
    "                # target_scheduler.step()\n",
    "\n",
    "                inner1.update(1)\n",
    "            loss_history.append(np.average(loss_running, weights=mean_weights))\n",
    "            accu_history.append(np.average(accu_running, weights=mean_weights))\n",
    "            loss_history_running.append(loss_running)\n",
    "            loss_history_gen.append(\n",
    "                np.average(loss_running_gen, weights=mean_weights_gen)\n",
    "            )\n",
    "            loss_history_gen_running.append(loss_running_gen)\n",
    "\n",
    "            model.eval()\n",
    "            model.dis.eval()\n",
    "            model.target_encoder.eval()\n",
    "            model.source_encoder.eval()\n",
    "\n",
    "            set_requires_grad(model, True)\n",
    "            set_requires_grad(model.target_encoder, True)\n",
    "            set_requires_grad(model.source_encoder, True)\n",
    "            set_requires_grad(model.dis, True)\n",
    "\n",
    "            # del batch_cycler\n",
    "            # with torch.no_grad():\n",
    "            #     curr_loss_val, curr_acc_val = compute_acc_dis(\n",
    "            #         dataloader_source_val, dataloader_target_train, model\n",
    "            #     )\n",
    "            #     loss_history_val.append(curr_loss_val)\n",
    "            #     accu_history_val.append(curr_loss_val)\n",
    "\n",
    "            # Print the results\n",
    "            outer.update(1)\n",
    "            print(\n",
    "                \"epoch:\",\n",
    "                epoch,\n",
    "                \"gen train loss:\",\n",
    "                round(loss_history_gen[-1], 6),\n",
    "                \"dis train loss:\",\n",
    "                round(loss_history[-1], 6),\n",
    "                \"dis train accu:\",\n",
    "                round(accu_history[-1], 6),\n",
    "                # \"dis val loss:\",\n",
    "                # round(loss_history_val[-1], 6),\n",
    "                # \"dis val accu:\",\n",
    "                # round(accu_history_val[-1], 6),\n",
    "                # \"next_lr:\", scheduler.get_last_lr(),\n",
    "                end=\" \",\n",
    "            )\n",
    "\n",
    "            # # Save the best weights\n",
    "            # if diff_from_rand < best_loss_val:\n",
    "            #     best_loss_val = diff_from_rand\n",
    "            #     torch.save(checkpoint, os.path.join(save_folder, f\"best_model.pth\"))\n",
    "            #     early_stop_count = 0\n",
    "\n",
    "            #     print(\"<-- new best difference from random loss\")\n",
    "            # else:\n",
    "            #     print(\"\")\n",
    "\n",
    "            print(\"\")\n",
    "\n",
    "            # Save checkpoint every 10\n",
    "            if epoch % 10 == 0 or epoch >= EPOCHS - 1:\n",
    "                torch.save(checkpoint, os.path.join(save_folder, f\"checkpt{epoch}.pth\"))\n",
    "\n",
    "            # check to see if validation loss has plateau'd\n",
    "            if early_stop_count >= EARLY_STOP_CRIT_ADV and epoch > MIN_EPOCHS_ADV - 1:\n",
    "                print(\n",
    "                    f\"Discriminator loss plateaued after {early_stop_count} at epoch {epoch}\"\n",
    "                )\n",
    "                torch.save(\n",
    "                    checkpoint, os.path.join(save_folder, f\"earlystop_{epoch}.pth\")\n",
    "                )\n",
    "                break\n",
    "\n",
    "            early_stop_count += 1\n",
    "\n",
    "    # Save final model\n",
    "    torch.save(checkpoint, os.path.join(save_folder, f\"final_model.pth\"))\n",
    "\n",
    "    return loss_history_running, loss_history_gen_running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# st_sample_id_l = [SAMPLE_ID_N]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial training for ST slide 151509: \n",
      "Start adversarial training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5b2993bc7748a79822e5811132f039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbcb8c5b80d242aabe808cd41aa38d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 gen train loss: 0.805352 dis train loss: 0.840844 dis train accu: 0.495506 \n",
      "epoch: 1 gen train loss: 0.836411 dis train loss: 0.820815 dis train accu: 0.4984 \n",
      "epoch: 2 gen train loss: 0.826295 dis train loss: 0.811582 dis train accu: 0.501355 \n",
      "epoch: 3 gen train loss: 0.820071 dis train loss: 0.809754 dis train accu: 0.499541 \n",
      "epoch: 4 gen train loss: 0.828546 dis train loss: 0.808058 dis train accu: 0.498889 \n",
      "epoch: 5 gen train loss: 0.825082 dis train loss: 0.806874 dis train accu: 0.500132 \n",
      "epoch: 6 gen train loss: 0.832842 dis train loss: 0.801809 dis train accu: 0.497829 \n",
      "epoch: 7 gen train loss: 0.830821 dis train loss: 0.799013 dis train accu: 0.500785 \n",
      "epoch: 8 gen train loss: 0.805602 dis train loss: 0.795474 dis train accu: 0.500744 \n",
      "epoch: 9 gen train loss: 0.823256 dis train loss: 0.78868 dis train accu: 0.504637 \n",
      "epoch: 10 gen train loss: 0.824542 dis train loss: 0.786233 dis train accu: 0.501416 \n",
      "epoch: 11 gen train loss: 0.806731 dis train loss: 0.786843 dis train accu: 0.50213 \n",
      "epoch: 12 gen train loss: 0.825054 dis train loss: 0.780972 dis train accu: 0.506512 \n",
      "epoch: 13 gen train loss: 0.804467 dis train loss: 0.77723 dis train accu: 0.502578 \n",
      "epoch: 14 gen train loss: 0.815915 dis train loss: 0.774553 dis train accu: 0.506981 \n",
      "epoch: 15 gen train loss: 0.804526 dis train loss: 0.772986 dis train accu: 0.505452 \n",
      "epoch: 16 gen train loss: 0.806359 dis train loss: 0.767012 dis train accu: 0.509691 \n",
      "epoch: 17 gen train loss: 0.811835 dis train loss: 0.763598 dis train accu: 0.512524 \n",
      "epoch: 18 gen train loss: 0.803432 dis train loss: 0.762243 dis train accu: 0.509814 \n",
      "epoch: 19 gen train loss: 0.805738 dis train loss: 0.758668 dis train accu: 0.512769 \n",
      "epoch: 20 gen train loss: 0.803458 dis train loss: 0.75643 dis train accu: 0.512218 \n",
      "epoch: 21 gen train loss: 0.826379 dis train loss: 0.754278 dis train accu: 0.514807 \n",
      "epoch: 22 gen train loss: 0.80218 dis train loss: 0.753694 dis train accu: 0.514583 \n",
      "epoch: 23 gen train loss: 0.798172 dis train loss: 0.749264 dis train accu: 0.517884 \n",
      "epoch: 24 gen train loss: 0.806498 dis train loss: 0.746867 dis train accu: 0.517232 \n",
      "epoch: 25 gen train loss: 0.802135 dis train loss: 0.745719 dis train accu: 0.518516 \n",
      "epoch: 26 gen train loss: 0.793823 dis train loss: 0.745664 dis train accu: 0.517212 \n",
      "epoch: 27 gen train loss: 0.797088 dis train loss: 0.745387 dis train accu: 0.515683 \n",
      "epoch: 28 gen train loss: 0.792594 dis train loss: 0.742183 dis train accu: 0.517681 \n",
      "epoch: 29 gen train loss: 0.800798 dis train loss: 0.739942 dis train accu: 0.519311 \n",
      "epoch: 30 gen train loss: 0.78988 dis train loss: 0.739901 dis train accu: 0.516763 \n",
      "epoch: 31 gen train loss: 0.793004 dis train loss: 0.740936 dis train accu: 0.513258 \n",
      "epoch: 32 gen train loss: 0.799675 dis train loss: 0.736421 dis train accu: 0.519189 \n",
      "epoch: 33 gen train loss: 0.784037 dis train loss: 0.736018 dis train accu: 0.518516 \n",
      "epoch: 34 gen train loss: 0.786995 dis train loss: 0.733696 dis train accu: 0.520024 \n",
      "epoch: 35 gen train loss: 0.790269 dis train loss: 0.733926 dis train accu: 0.519372 \n",
      "epoch: 36 gen train loss: 0.780712 dis train loss: 0.731108 dis train accu: 0.521492 \n",
      "epoch: 37 gen train loss: 0.779731 dis train loss: 0.732813 dis train accu: 0.518944 \n",
      "epoch: 38 gen train loss: 0.785527 dis train loss: 0.732585 dis train accu: 0.51764 \n",
      "epoch: 39 gen train loss: 0.789002 dis train loss: 0.733416 dis train accu: 0.516193 \n",
      "epoch: 40 gen train loss: 0.779217 dis train loss: 0.733277 dis train accu: 0.510608 \n",
      "epoch: 41 gen train loss: 0.770234 dis train loss: 0.731195 dis train accu: 0.514929 \n",
      "epoch: 42 gen train loss: 0.771515 dis train loss: 0.727656 dis train accu: 0.518863 \n",
      "epoch: 43 gen train loss: 0.775164 dis train loss: 0.724429 dis train accu: 0.521084 \n",
      "epoch: 44 gen train loss: 0.770918 dis train loss: 0.728435 dis train accu: 0.514746 \n",
      "epoch: 45 gen train loss: 0.773215 dis train loss: 0.726888 dis train accu: 0.514481 \n",
      "epoch: 46 gen train loss: 0.772572 dis train loss: 0.727874 dis train accu: 0.510384 \n",
      "epoch: 47 gen train loss: 0.778526 dis train loss: 0.723675 dis train accu: 0.518333 \n",
      "epoch: 48 gen train loss: 0.763147 dis train loss: 0.72299 dis train accu: 0.518537 \n",
      "epoch: 49 gen train loss: 0.768645 dis train loss: 0.724131 dis train accu: 0.513645 \n",
      "epoch: 50 gen train loss: 0.766767 dis train loss: 0.722854 dis train accu: 0.516356 \n",
      "epoch: 51 gen train loss: 0.762176 dis train loss: 0.723285 dis train accu: 0.51499 \n",
      "epoch: 52 gen train loss: 0.758106 dis train loss: 0.722047 dis train accu: 0.516702 \n",
      "epoch: 53 gen train loss: 0.768234 dis train loss: 0.724561 dis train accu: 0.510058 \n",
      "epoch: 54 gen train loss: 0.765245 dis train loss: 0.725032 dis train accu: 0.508224 \n",
      "epoch: 55 gen train loss: 0.7581 dis train loss: 0.721548 dis train accu: 0.513238 \n",
      "epoch: 56 gen train loss: 0.762922 dis train loss: 0.721409 dis train accu: 0.512463 \n",
      "epoch: 57 gen train loss: 0.763415 dis train loss: 0.722548 dis train accu: 0.506165 \n",
      "epoch: 58 gen train loss: 0.759833 dis train loss: 0.721109 dis train accu: 0.511444 \n",
      "epoch: 59 gen train loss: 0.759129 dis train loss: 0.721353 dis train accu: 0.509956 \n",
      "epoch: 60 gen train loss: 0.759796 dis train loss: 0.720555 dis train accu: 0.511159 \n",
      "epoch: 61 gen train loss: 0.758736 dis train loss: 0.718264 dis train accu: 0.511179 \n",
      "epoch: 62 gen train loss: 0.752897 dis train loss: 0.72076 dis train accu: 0.508081 \n",
      "epoch: 63 gen train loss: 0.749172 dis train loss: 0.718825 dis train accu: 0.511933 \n",
      "epoch: 64 gen train loss: 0.753107 dis train loss: 0.720114 dis train accu: 0.508958 \n",
      "epoch: 65 gen train loss: 0.757845 dis train loss: 0.719245 dis train accu: 0.506349 \n",
      "epoch: 66 gen train loss: 0.749143 dis train loss: 0.718839 dis train accu: 0.508713 \n",
      "epoch: 67 gen train loss: 0.750309 dis train loss: 0.719449 dis train accu: 0.503842 \n",
      "epoch: 68 gen train loss: 0.752051 dis train loss: 0.718329 dis train accu: 0.504433 \n",
      "epoch: 69 gen train loss: 0.744739 dis train loss: 0.716665 dis train accu: 0.507266 \n",
      "epoch: 70 gen train loss: 0.752772 dis train loss: 0.71777 dis train accu: 0.505819 \n",
      "epoch: 71 gen train loss: 0.74678 dis train loss: 0.717596 dis train accu: 0.503618 \n",
      "epoch: 72 gen train loss: 0.743151 dis train loss: 0.715986 dis train accu: 0.508285 \n",
      "epoch: 73 gen train loss: 0.747169 dis train loss: 0.716512 dis train accu: 0.507286 \n",
      "epoch: 74 gen train loss: 0.744218 dis train loss: 0.712823 dis train accu: 0.510486 \n",
      "epoch: 75 gen train loss: 0.750264 dis train loss: 0.71556 dis train accu: 0.50802 \n",
      "epoch: 76 gen train loss: 0.743697 dis train loss: 0.713739 dis train accu: 0.510486 \n",
      "epoch: 77 gen train loss: 0.748954 dis train loss: 0.711654 dis train accu: 0.514522 \n",
      "epoch: 78 gen train loss: 0.74674 dis train loss: 0.714389 dis train accu: 0.507042 \n",
      "epoch: 79 gen train loss: 0.743216 dis train loss: 0.712931 dis train accu: 0.50749 \n",
      "epoch: 80 gen train loss: 0.746387 dis train loss: 0.713687 dis train accu: 0.506695 \n",
      "epoch: 81 gen train loss: 0.750447 dis train loss: 0.715033 dis train accu: 0.501152 \n",
      "epoch: 82 gen train loss: 0.747412 dis train loss: 0.713266 dis train accu: 0.505697 \n",
      "epoch: 83 gen train loss: 0.746106 dis train loss: 0.71194 dis train accu: 0.508448 \n",
      "epoch: 84 gen train loss: 0.746203 dis train loss: 0.712295 dis train accu: 0.508224 \n",
      "epoch: 85 gen train loss: 0.743802 dis train loss: 0.710768 dis train accu: 0.50804 \n",
      "epoch: 86 gen train loss: 0.752129 dis train loss: 0.709242 dis train accu: 0.510343 \n",
      "epoch: 87 gen train loss: 0.741779 dis train loss: 0.709834 dis train accu: 0.510201 \n",
      "epoch: 88 gen train loss: 0.742342 dis train loss: 0.710516 dis train accu: 0.50804 \n",
      "epoch: 89 gen train loss: 0.74337 dis train loss: 0.711154 dis train accu: 0.507979 \n",
      "epoch: 90 gen train loss: 0.742377 dis train loss: 0.711167 dis train accu: 0.504107 \n",
      "epoch: 91 gen train loss: 0.744199 dis train loss: 0.709138 dis train accu: 0.509324 \n",
      "epoch: 92 gen train loss: 0.742165 dis train loss: 0.7074 dis train accu: 0.510099 \n",
      "epoch: 93 gen train loss: 0.741859 dis train loss: 0.709977 dis train accu: 0.506471 \n",
      "epoch: 94 gen train loss: 0.741412 dis train loss: 0.707735 dis train accu: 0.507959 \n",
      "epoch: 95 gen train loss: 0.737377 dis train loss: 0.707151 dis train accu: 0.511424 \n",
      "epoch: 96 gen train loss: 0.73968 dis train loss: 0.707781 dis train accu: 0.510568 \n",
      "epoch: 97 gen train loss: 0.74032 dis train loss: 0.708486 dis train accu: 0.50963 \n",
      "epoch: 98 gen train loss: 0.740813 dis train loss: 0.70688 dis train accu: 0.509304 \n",
      "epoch: 99 gen train loss: 0.740806 dis train loss: 0.706187 dis train accu: 0.512117 \n",
      "epoch: 100 gen train loss: 0.736412 dis train loss: 0.708308 dis train accu: 0.504168 \n",
      "epoch: 101 gen train loss: 0.737669 dis train loss: 0.706316 dis train accu: 0.510812 \n",
      "epoch: 102 gen train loss: 0.737869 dis train loss: 0.708451 dis train accu: 0.50537 \n",
      "epoch: 103 gen train loss: 0.743644 dis train loss: 0.705943 dis train accu: 0.511689 \n",
      "epoch: 104 gen train loss: 0.743149 dis train loss: 0.707426 dis train accu: 0.506756 \n",
      "epoch: 105 gen train loss: 0.74549 dis train loss: 0.704345 dis train accu: 0.514053 \n",
      "epoch: 106 gen train loss: 0.738513 dis train loss: 0.704859 dis train accu: 0.509386 \n",
      "epoch: 107 gen train loss: 0.737478 dis train loss: 0.704498 dis train accu: 0.509365 \n",
      "epoch: 108 gen train loss: 0.735146 dis train loss: 0.704079 dis train accu: 0.512076 \n",
      "epoch: 109 gen train loss: 0.739926 dis train loss: 0.70549 dis train accu: 0.510914 \n",
      "epoch: 110 gen train loss: 0.737912 dis train loss: 0.704526 dis train accu: 0.509915 \n",
      "epoch: 111 gen train loss: 0.738878 dis train loss: 0.704753 dis train accu: 0.510812 \n",
      "epoch: 112 gen train loss: 0.739916 dis train loss: 0.703209 dis train accu: 0.513136 \n",
      "epoch: 113 gen train loss: 0.738828 dis train loss: 0.70368 dis train accu: 0.511444 \n",
      "epoch: 114 gen train loss: 0.736006 dis train loss: 0.702728 dis train accu: 0.512096 \n",
      "epoch: 115 gen train loss: 0.732937 dis train loss: 0.704126 dis train accu: 0.509528 \n",
      "epoch: 116 gen train loss: 0.736446 dis train loss: 0.704438 dis train accu: 0.507021 \n",
      "epoch: 117 gen train loss: 0.732891 dis train loss: 0.704188 dis train accu: 0.507368 \n",
      "epoch: 118 gen train loss: 0.738038 dis train loss: 0.703874 dis train accu: 0.508754 \n",
      "epoch: 119 gen train loss: 0.73898 dis train loss: 0.703449 dis train accu: 0.507429 \n",
      "epoch: 120 gen train loss: 0.738991 dis train loss: 0.704758 dis train accu: 0.508305 \n",
      "epoch: 121 gen train loss: 0.732849 dis train loss: 0.704214 dis train accu: 0.507735 \n",
      "epoch: 122 gen train loss: 0.730334 dis train loss: 0.703313 dis train accu: 0.508591 \n",
      "epoch: 123 gen train loss: 0.736474 dis train loss: 0.705047 dis train accu: 0.504718 \n",
      "epoch: 124 gen train loss: 0.734876 dis train loss: 0.702487 dis train accu: 0.511811 \n",
      "epoch: 125 gen train loss: 0.732423 dis train loss: 0.703732 dis train accu: 0.507612 \n",
      "epoch: 126 gen train loss: 0.73082 dis train loss: 0.703734 dis train accu: 0.504535 \n",
      "epoch: 127 gen train loss: 0.734827 dis train loss: 0.703071 dis train accu: 0.509039 \n",
      "epoch: 128 gen train loss: 0.731248 dis train loss: 0.702774 dis train accu: 0.507857 \n",
      "epoch: 129 gen train loss: 0.733589 dis train loss: 0.705011 dis train accu: 0.502741 \n",
      "epoch: 130 gen train loss: 0.732678 dis train loss: 0.702766 dis train accu: 0.509528 \n",
      "epoch: 131 gen train loss: 0.727899 dis train loss: 0.702889 dis train accu: 0.50747 \n",
      "epoch: 132 gen train loss: 0.733656 dis train loss: 0.70355 dis train accu: 0.506084 \n",
      "epoch: 133 gen train loss: 0.729865 dis train loss: 0.702343 dis train accu: 0.507021 \n",
      "epoch: 134 gen train loss: 0.728826 dis train loss: 0.702635 dis train accu: 0.507714 \n",
      "epoch: 135 gen train loss: 0.733852 dis train loss: 0.703792 dis train accu: 0.503475 \n",
      "epoch: 136 gen train loss: 0.733995 dis train loss: 0.701576 dis train accu: 0.509059 \n",
      "epoch: 137 gen train loss: 0.723532 dis train loss: 0.702078 dis train accu: 0.508387 \n",
      "epoch: 138 gen train loss: 0.73165 dis train loss: 0.70286 dis train accu: 0.504942 \n",
      "epoch: 139 gen train loss: 0.728813 dis train loss: 0.703318 dis train accu: 0.504739 \n",
      "epoch: 140 gen train loss: 0.72903 dis train loss: 0.701067 dis train accu: 0.512035 \n",
      "epoch: 141 gen train loss: 0.730164 dis train loss: 0.702321 dis train accu: 0.506491 \n",
      "epoch: 142 gen train loss: 0.732869 dis train loss: 0.701976 dis train accu: 0.507755 \n",
      "epoch: 143 gen train loss: 0.732591 dis train loss: 0.701326 dis train accu: 0.510323 \n",
      "epoch: 144 gen train loss: 0.730432 dis train loss: 0.700571 dis train accu: 0.509161 \n",
      "epoch: 145 gen train loss: 0.728073 dis train loss: 0.700892 dis train accu: 0.508407 \n",
      "epoch: 146 gen train loss: 0.730935 dis train loss: 0.700531 dis train accu: 0.510038 \n",
      "epoch: 147 gen train loss: 0.728919 dis train loss: 0.701657 dis train accu: 0.508978 \n",
      "epoch: 148 gen train loss: 0.729562 dis train loss: 0.700649 dis train accu: 0.508958 \n",
      "epoch: 149 gen train loss: 0.729585 dis train loss: 0.69958 dis train accu: 0.510608 \n",
      "epoch: 150 gen train loss: 0.730744 dis train loss: 0.700774 dis train accu: 0.50965 \n",
      "epoch: 151 gen train loss: 0.728295 dis train loss: 0.70041 dis train accu: 0.508346 \n",
      "epoch: 152 gen train loss: 0.729593 dis train loss: 0.700994 dis train accu: 0.508428 \n",
      "epoch: 153 gen train loss: 0.732912 dis train loss: 0.699966 dis train accu: 0.51069 \n",
      "epoch: 154 gen train loss: 0.732034 dis train loss: 0.699737 dis train accu: 0.510058 \n",
      "epoch: 155 gen train loss: 0.729907 dis train loss: 0.700735 dis train accu: 0.510914 \n",
      "epoch: 156 gen train loss: 0.729367 dis train loss: 0.701608 dis train accu: 0.506573 \n",
      "epoch: 157 gen train loss: 0.729207 dis train loss: 0.698952 dis train accu: 0.510894 \n",
      "epoch: 158 gen train loss: 0.729909 dis train loss: 0.699625 dis train accu: 0.509875 \n",
      "epoch: 159 gen train loss: 0.725484 dis train loss: 0.700324 dis train accu: 0.507286 \n",
      "epoch: 160 gen train loss: 0.728066 dis train loss: 0.698822 dis train accu: 0.51393 \n",
      "epoch: 161 gen train loss: 0.729433 dis train loss: 0.699853 dis train accu: 0.507796 \n",
      "epoch: 162 gen train loss: 0.730598 dis train loss: 0.700406 dis train accu: 0.509222 \n",
      "epoch: 163 gen train loss: 0.729667 dis train loss: 0.699207 dis train accu: 0.51285 \n",
      "epoch: 164 gen train loss: 0.730954 dis train loss: 0.699054 dis train accu: 0.508203 \n",
      "epoch: 165 gen train loss: 0.727179 dis train loss: 0.698309 dis train accu: 0.512157 \n",
      "epoch: 166 gen train loss: 0.729064 dis train loss: 0.699002 dis train accu: 0.511587 \n",
      "epoch: 167 gen train loss: 0.728521 dis train loss: 0.698508 dis train accu: 0.514318 \n",
      "epoch: 168 gen train loss: 0.726638 dis train loss: 0.69931 dis train accu: 0.511138 \n",
      "epoch: 169 gen train loss: 0.726441 dis train loss: 0.698582 dis train accu: 0.513034 \n",
      "epoch: 170 gen train loss: 0.72773 dis train loss: 0.699293 dis train accu: 0.510629 \n",
      "epoch: 171 gen train loss: 0.728702 dis train loss: 0.699249 dis train accu: 0.510934 \n",
      "epoch: 172 gen train loss: 0.727506 dis train loss: 0.698009 dis train accu: 0.513115 \n",
      "epoch: 173 gen train loss: 0.72816 dis train loss: 0.699051 dis train accu: 0.508591 \n",
      "epoch: 174 gen train loss: 0.72807 dis train loss: 0.69817 dis train accu: 0.514501 \n",
      "epoch: 175 gen train loss: 0.727215 dis train loss: 0.698618 dis train accu: 0.512463 \n",
      "epoch: 176 gen train loss: 0.733232 dis train loss: 0.699793 dis train accu: 0.507042 \n",
      "epoch: 177 gen train loss: 0.726202 dis train loss: 0.698691 dis train accu: 0.511872 \n",
      "epoch: 178 gen train loss: 0.731358 dis train loss: 0.698247 dis train accu: 0.513625 \n",
      "epoch: 179 gen train loss: 0.728266 dis train loss: 0.698278 dis train accu: 0.510466 \n",
      "epoch: 180 gen train loss: 0.727221 dis train loss: 0.698804 dis train accu: 0.51444 \n",
      "epoch: 181 gen train loss: 0.726507 dis train loss: 0.697935 dis train accu: 0.51391 \n",
      "epoch: 182 gen train loss: 0.733207 dis train loss: 0.697755 dis train accu: 0.515459 \n",
      "epoch: 183 gen train loss: 0.726679 dis train loss: 0.698883 dis train accu: 0.510608 \n",
      "epoch: 184 gen train loss: 0.734025 dis train loss: 0.698199 dis train accu: 0.511994 \n",
      "epoch: 185 gen train loss: 0.728749 dis train loss: 0.697866 dis train accu: 0.514501 \n",
      "epoch: 186 gen train loss: 0.72683 dis train loss: 0.698858 dis train accu: 0.511892 \n",
      "epoch: 187 gen train loss: 0.729056 dis train loss: 0.69908 dis train accu: 0.50696 \n",
      "epoch: 188 gen train loss: 0.72793 dis train loss: 0.697978 dis train accu: 0.514623 \n",
      "epoch: 189 gen train loss: 0.727604 dis train loss: 0.699318 dis train accu: 0.508652 \n",
      "epoch: 190 gen train loss: 0.733001 dis train loss: 0.699052 dis train accu: 0.51228 \n",
      "epoch: 191 gen train loss: 0.729176 dis train loss: 0.697644 dis train accu: 0.513747 \n",
      "epoch: 192 gen train loss: 0.729406 dis train loss: 0.697161 dis train accu: 0.51281 \n",
      "epoch: 193 gen train loss: 0.726881 dis train loss: 0.697876 dis train accu: 0.511444 \n",
      "epoch: 194 gen train loss: 0.730702 dis train loss: 0.696733 dis train accu: 0.516682 \n",
      "epoch: 195 gen train loss: 0.726855 dis train loss: 0.69725 dis train accu: 0.512687 \n",
      "epoch: 196 gen train loss: 0.732497 dis train loss: 0.69802 dis train accu: 0.510282 \n",
      "epoch: 197 gen train loss: 0.730664 dis train loss: 0.696581 dis train accu: 0.518047 \n",
      "epoch: 198 gen train loss: 0.731267 dis train loss: 0.697334 dis train accu: 0.515357 \n",
      "epoch: 199 gen train loss: 0.724375 dis train loss: 0.69741 dis train accu: 0.514725 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial training for ST slide 151510: \n",
      "Start adversarial training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a176c92d1444562a95d9b7c40bd325f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555c1ee40dac4ea4b5d9e9d8578a6c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 gen train loss: 0.854339 dis train loss: 0.845997 dis train accu: 0.491137 \n",
      "epoch: 1 gen train loss: 0.839088 dis train loss: 0.828502 dis train accu: 0.496666 \n",
      "epoch: 2 gen train loss: 0.843289 dis train loss: 0.820138 dis train accu: 0.497287 \n",
      "epoch: 3 gen train loss: 0.847788 dis train loss: 0.817597 dis train accu: 0.499337 \n",
      "epoch: 4 gen train loss: 0.84513 dis train loss: 0.811758 dis train accu: 0.499814 \n",
      "epoch: 5 gen train loss: 0.827509 dis train loss: 0.812372 dis train accu: 0.499669 \n",
      "epoch: 6 gen train loss: 0.822275 dis train loss: 0.811318 dis train accu: 0.496252 \n",
      "epoch: 7 gen train loss: 0.81449 dis train loss: 0.802818 dis train accu: 0.502692 \n",
      "epoch: 8 gen train loss: 0.816982 dis train loss: 0.799794 dis train accu: 0.499027 \n",
      "epoch: 9 gen train loss: 0.836028 dis train loss: 0.798094 dis train accu: 0.499959 \n",
      "epoch: 10 gen train loss: 0.8148 dis train loss: 0.79644 dis train accu: 0.498095 \n",
      "epoch: 11 gen train loss: 0.829046 dis train loss: 0.794897 dis train accu: 0.498965 \n",
      "epoch: 12 gen train loss: 0.819507 dis train loss: 0.790861 dis train accu: 0.498675 \n",
      "epoch: 13 gen train loss: 0.815176 dis train loss: 0.783847 dis train accu: 0.504701 \n",
      "epoch: 14 gen train loss: 0.816892 dis train loss: 0.777115 dis train accu: 0.505943 \n",
      "epoch: 15 gen train loss: 0.816151 dis train loss: 0.775249 dis train accu: 0.509754 \n",
      "epoch: 16 gen train loss: 0.811537 dis train loss: 0.777412 dis train accu: 0.507082 \n",
      "epoch: 17 gen train loss: 0.800543 dis train loss: 0.772092 dis train accu: 0.507786 \n",
      "epoch: 18 gen train loss: 0.803258 dis train loss: 0.770011 dis train accu: 0.508697 \n",
      "epoch: 19 gen train loss: 0.809165 dis train loss: 0.76393 dis train accu: 0.512736 \n",
      "epoch: 20 gen train loss: 0.80874 dis train loss: 0.765656 dis train accu: 0.50789 \n",
      "epoch: 21 gen train loss: 0.8054 dis train loss: 0.763647 dis train accu: 0.505798 \n",
      "epoch: 22 gen train loss: 0.799246 dis train loss: 0.756714 dis train accu: 0.51257 \n",
      "epoch: 23 gen train loss: 0.807038 dis train loss: 0.755572 dis train accu: 0.513564 \n",
      "epoch: 24 gen train loss: 0.802505 dis train loss: 0.757008 dis train accu: 0.511037 \n",
      "epoch: 25 gen train loss: 0.798539 dis train loss: 0.752957 dis train accu: 0.515158 \n",
      "epoch: 26 gen train loss: 0.797161 dis train loss: 0.753226 dis train accu: 0.509671 \n",
      "epoch: 27 gen train loss: 0.792761 dis train loss: 0.749955 dis train accu: 0.515593 \n",
      "epoch: 28 gen train loss: 0.793604 dis train loss: 0.747906 dis train accu: 0.515096 \n",
      "epoch: 29 gen train loss: 0.801369 dis train loss: 0.743665 dis train accu: 0.515428 \n",
      "epoch: 30 gen train loss: 0.787807 dis train loss: 0.745211 dis train accu: 0.512156 \n",
      "epoch: 31 gen train loss: 0.791611 dis train loss: 0.741254 dis train accu: 0.515531 \n",
      "epoch: 32 gen train loss: 0.796464 dis train loss: 0.742055 dis train accu: 0.516297 \n",
      "epoch: 33 gen train loss: 0.790374 dis train loss: 0.738567 dis train accu: 0.51783 \n",
      "epoch: 34 gen train loss: 0.786769 dis train loss: 0.739026 dis train accu: 0.518907 \n",
      "epoch: 35 gen train loss: 0.791599 dis train loss: 0.736536 dis train accu: 0.518927 \n",
      "epoch: 36 gen train loss: 0.786216 dis train loss: 0.73604 dis train accu: 0.518865 \n",
      "epoch: 37 gen train loss: 0.776301 dis train loss: 0.734627 dis train accu: 0.519486 \n",
      "epoch: 38 gen train loss: 0.791014 dis train loss: 0.732711 dis train accu: 0.519673 \n",
      "epoch: 39 gen train loss: 0.784214 dis train loss: 0.731862 dis train accu: 0.51901 \n",
      "epoch: 40 gen train loss: 0.789765 dis train loss: 0.732782 dis train accu: 0.515945 \n",
      "epoch: 41 gen train loss: 0.785329 dis train loss: 0.732675 dis train accu: 0.516277 \n",
      "epoch: 42 gen train loss: 0.785048 dis train loss: 0.730604 dis train accu: 0.520936 \n",
      "epoch: 43 gen train loss: 0.772801 dis train loss: 0.728865 dis train accu: 0.52017 \n",
      "epoch: 44 gen train loss: 0.775442 dis train loss: 0.727463 dis train accu: 0.517188 \n",
      "epoch: 45 gen train loss: 0.777598 dis train loss: 0.724955 dis train accu: 0.522738 \n",
      "epoch: 46 gen train loss: 0.780973 dis train loss: 0.7281 dis train accu: 0.517188 \n",
      "epoch: 47 gen train loss: 0.77427 dis train loss: 0.725356 dis train accu: 0.517436 \n",
      "epoch: 48 gen train loss: 0.779043 dis train loss: 0.725475 dis train accu: 0.51551 \n",
      "epoch: 49 gen train loss: 0.7747 dis train loss: 0.725223 dis train accu: 0.516981 \n",
      "epoch: 50 gen train loss: 0.770558 dis train loss: 0.724061 dis train accu: 0.516794 \n",
      "epoch: 51 gen train loss: 0.776121 dis train loss: 0.723987 dis train accu: 0.518016 \n",
      "epoch: 52 gen train loss: 0.771467 dis train loss: 0.721837 dis train accu: 0.518782 \n",
      "epoch: 53 gen train loss: 0.765845 dis train loss: 0.720622 dis train accu: 0.52077 \n",
      "epoch: 54 gen train loss: 0.764458 dis train loss: 0.724199 dis train accu: 0.514247 \n",
      "epoch: 55 gen train loss: 0.77003 dis train loss: 0.718632 dis train accu: 0.520232 \n",
      "epoch: 56 gen train loss: 0.766121 dis train loss: 0.721405 dis train accu: 0.514268 \n",
      "epoch: 57 gen train loss: 0.764244 dis train loss: 0.719834 dis train accu: 0.516587 \n",
      "epoch: 58 gen train loss: 0.768785 dis train loss: 0.721055 dis train accu: 0.515345 \n",
      "epoch: 59 gen train loss: 0.760966 dis train loss: 0.719532 dis train accu: 0.517167 \n",
      "epoch: 60 gen train loss: 0.755965 dis train loss: 0.71962 dis train accu: 0.513937 \n",
      "epoch: 61 gen train loss: 0.763758 dis train loss: 0.720904 dis train accu: 0.512653 \n",
      "epoch: 62 gen train loss: 0.759632 dis train loss: 0.718496 dis train accu: 0.513129 \n",
      "epoch: 63 gen train loss: 0.755019 dis train loss: 0.716623 dis train accu: 0.517333 \n",
      "epoch: 64 gen train loss: 0.765963 dis train loss: 0.721805 dis train accu: 0.508615 \n",
      "epoch: 65 gen train loss: 0.745129 dis train loss: 0.718664 dis train accu: 0.513025 \n",
      "epoch: 66 gen train loss: 0.760965 dis train loss: 0.717483 dis train accu: 0.511866 \n",
      "epoch: 67 gen train loss: 0.757351 dis train loss: 0.717714 dis train accu: 0.512156 \n",
      "epoch: 68 gen train loss: 0.75193 dis train loss: 0.71536 dis train accu: 0.51317 \n",
      "epoch: 69 gen train loss: 0.750501 dis train loss: 0.717926 dis train accu: 0.509981 \n",
      "epoch: 70 gen train loss: 0.755192 dis train loss: 0.715536 dis train accu: 0.514661 \n",
      "epoch: 71 gen train loss: 0.755545 dis train loss: 0.71587 dis train accu: 0.513191 \n",
      "epoch: 72 gen train loss: 0.75083 dis train loss: 0.715626 dis train accu: 0.515324 \n",
      "epoch: 73 gen train loss: 0.749706 dis train loss: 0.714279 dis train accu: 0.513854 \n",
      "epoch: 74 gen train loss: 0.747327 dis train loss: 0.712943 dis train accu: 0.515055 \n",
      "epoch: 75 gen train loss: 0.750277 dis train loss: 0.712998 dis train accu: 0.514889 \n",
      "epoch: 76 gen train loss: 0.754749 dis train loss: 0.712364 dis train accu: 0.514558 \n",
      "epoch: 77 gen train loss: 0.754711 dis train loss: 0.713026 dis train accu: 0.514434 \n",
      "epoch: 78 gen train loss: 0.749914 dis train loss: 0.713128 dis train accu: 0.510685 \n",
      "epoch: 79 gen train loss: 0.745492 dis train loss: 0.710713 dis train accu: 0.515966 \n",
      "epoch: 80 gen train loss: 0.750644 dis train loss: 0.71436 dis train accu: 0.509174 \n",
      "epoch: 81 gen train loss: 0.7502 dis train loss: 0.71122 dis train accu: 0.514889 \n",
      "epoch: 82 gen train loss: 0.748285 dis train loss: 0.712321 dis train accu: 0.510292 \n",
      "epoch: 83 gen train loss: 0.747676 dis train loss: 0.710766 dis train accu: 0.513212 \n",
      "epoch: 84 gen train loss: 0.742387 dis train loss: 0.712631 dis train accu: 0.509298 \n",
      "epoch: 85 gen train loss: 0.74379 dis train loss: 0.711357 dis train accu: 0.509402 \n",
      "epoch: 86 gen train loss: 0.742539 dis train loss: 0.712163 dis train accu: 0.510043 \n",
      "epoch: 87 gen train loss: 0.743893 dis train loss: 0.710532 dis train accu: 0.512694 \n",
      "epoch: 88 gen train loss: 0.743696 dis train loss: 0.709899 dis train accu: 0.511203 \n",
      "epoch: 89 gen train loss: 0.738275 dis train loss: 0.711548 dis train accu: 0.50586 \n"
     ]
    }
   ],
   "source": [
    "if TRAIN_USING_ALL_ST_SAMPLES:\n",
    "    print(f\"Adversarial training for all ST slides\")\n",
    "    save_folder = advtrain_folder\n",
    "\n",
    "    best_checkpoint = torch.load(os.path.join(pretrain_folder, f\"final_model.pth\"))\n",
    "    model = ADDAST(\n",
    "        sc_mix_d[\"train\"].shape[1],\n",
    "        ncls_source=lab_mix_d[\"train\"].shape[1],\n",
    "        is_adda=True,\n",
    "        **adda_kwargs,\n",
    "    )\n",
    "\n",
    "    model.source_encoder.load_state_dict(\n",
    "        best_checkpoint[\"model\"].source_encoder.state_dict()\n",
    "    )\n",
    "    model.clf.load_state_dict(best_checkpoint[\"model\"].clf.state_dict())\n",
    "\n",
    "    model.init_adv()\n",
    "    model.dis.apply(initialize_weights)\n",
    "    model.to(device)\n",
    "\n",
    "    model.advtraining()\n",
    "\n",
    "    train_adversarial_iters(\n",
    "        model,\n",
    "        save_folder,\n",
    "        dataloader_source_train,\n",
    "        dataloader_source_val,\n",
    "        dataloader_target_train,\n",
    "        dataloader_target_train_dis,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    for sample_id in st_sample_id_l:\n",
    "        print(f\"Adversarial training for ST slide {sample_id}: \")\n",
    "\n",
    "        save_folder = os.path.join(advtrain_folder, sample_id)\n",
    "        if not os.path.isdir(save_folder):\n",
    "            os.makedirs(save_folder)\n",
    "\n",
    "        best_checkpoint = torch.load(os.path.join(pretrain_folder, f\"final_model.pth\"))\n",
    "\n",
    "        model = ADDAST(\n",
    "            sc_mix_d[\"train\"].shape[1],\n",
    "            ncls_source=lab_mix_d[\"train\"].shape[1],\n",
    "            is_adda=True,\n",
    "            **adda_kwargs,\n",
    "        )\n",
    "\n",
    "        model.apply(initialize_weights)\n",
    "\n",
    "        # load state dicts\n",
    "        # this makes it easier, if, say, the discriminator changes\n",
    "        model.source_encoder.load_state_dict(\n",
    "            best_checkpoint[\"model\"].source_encoder.state_dict()\n",
    "        )\n",
    "\n",
    "        model.clf.load_state_dict(best_checkpoint[\"model\"].clf.state_dict())\n",
    "\n",
    "        model.init_adv()\n",
    "        model.dis.apply(initialize_weights)\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        model.advtraining()\n",
    "\n",
    "        loss_history_running, loss_history_gen_running = train_adversarial_iters(\n",
    "            model,\n",
    "            save_folder,\n",
    "            dataloader_source_train,\n",
    "            dataloader_source_val,\n",
    "            dataloader_target_train_d[sample_id],\n",
    "            dataloader_target_train_dis_d[sample_id],\n",
    "        )\n",
    "        loss_history_gen_running = np.ravel(loss_history_gen_running)\n",
    "        x = np.arange(\n",
    "            0,\n",
    "            len(loss_history_gen_running) * (DIS_LOOP_FACTOR * 2),\n",
    "            DIS_LOOP_FACTOR * 2,\n",
    "        )\n",
    "\n",
    "        plt.plot(np.ravel(loss_history_running), label=\"dis\")\n",
    "        plt.plot(x, loss_history_gen_running, label=\"gen\")\n",
    "        plt.legend()\n",
    "        plt.title(sample_id)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn import model_selection\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# for sample_id in st_sample_id_l:\n",
    "#     best_checkpoint = torch.load(\n",
    "#         os.path.join(advtrain_folder, sample_id, f\"final_model.pth\")\n",
    "#     )\n",
    "#     model = best_checkpoint[\"model\"]\n",
    "#     model.to(device)\n",
    "\n",
    "#     model.eval()\n",
    "#     model.target_inference()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         source_emb = model.source_encoder(torch.Tensor(sc_mix_train_s).to(device))\n",
    "#         target_emb = model.target_encoder(\n",
    "#             torch.Tensor(mat_sp_test_s_d[sample_id]).to(device)\n",
    "#         )\n",
    "\n",
    "#         y_dis = torch.cat(\n",
    "#             [\n",
    "#                 torch.zeros(source_emb.shape[0], device=device, dtype=torch.long),\n",
    "#                 torch.ones(target_emb.shape[0], device=device, dtype=torch.long),\n",
    "#             ]\n",
    "#         )\n",
    "\n",
    "#         emb = torch.cat([source_emb, target_emb])\n",
    "\n",
    "#         emb = emb.detach().cpu().numpy()\n",
    "#         y_dis = y_dis.detach().cpu().numpy()\n",
    "\n",
    "#     (emb_train, emb_test, y_dis_train, y_dis_test,) = model_selection.train_test_split(\n",
    "#         emb,\n",
    "#         y_dis,\n",
    "#         test_size=0.2,\n",
    "#         random_state=225,\n",
    "#         stratify=y_dis,\n",
    "#     )\n",
    "\n",
    "#     pca = PCA(n_components=50)\n",
    "#     pca.fit(emb_train)\n",
    "\n",
    "#     emb_train_50 = pca.transform(emb_train)\n",
    "#     emb_test_50 = pca.transform(emb_test)\n",
    "\n",
    "#     clf = RandomForestClassifier(random_state=145, n_jobs=-1)\n",
    "#     clf.fit(emb_train_50, y_dis_train)\n",
    "#     accu_train = clf.score(emb_train_50, y_dis_train)\n",
    "#     accu_test = clf.score(emb_test_50, y_dis_test)\n",
    "#     class_proportions = np.mean(y_dis)\n",
    "\n",
    "#     print(\n",
    "#         \"Training accuracy: {}, Test accuracy: {}, Class proportions: {}\".format(\n",
    "#             accu_train, accu_test, class_proportions\n",
    "#         )\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 4. Predict cell fraction of spots and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_sp_d, pred_sp_noda_d = {}, {}\n",
    "# if TRAIN_USING_ALL_ST_SAMPLES:\n",
    "#     best_checkpoint = torch.load(os.path.join(advtrain_folder, f\"final_model.pth\"))\n",
    "#     model = best_checkpoint[\"model\"]\n",
    "#     model.to(device)\n",
    "\n",
    "#     model.eval()\n",
    "#     model.target_inference()\n",
    "#     with torch.no_grad():\n",
    "#         for sample_id in st_sample_id_l:\n",
    "#             pred_sp_d[sample_id] = (\n",
    "#                 torch.exp(\n",
    "#                     model(torch.Tensor(mat_sp_test_s_d[sample_id]).to(device))\n",
    "#                 )\n",
    "#                 .detach()\n",
    "#                 .cpu()\n",
    "#                 .numpy()\n",
    "#             )\n",
    "\n",
    "# else:\n",
    "#     for sample_id in st_sample_id_l:\n",
    "#         best_checkpoint = torch.load(\n",
    "#             os.path.join(advtrain_folder, sample_id, f\"final_model.pth\")\n",
    "#         )\n",
    "#         model = best_checkpoint[\"model\"]\n",
    "#         model.to(device)\n",
    "\n",
    "#         model.eval()\n",
    "#         model.target_inference()\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             pred_sp_d[sample_id] = (\n",
    "#                 torch.exp(\n",
    "#                     model(torch.Tensor(mat_sp_test_s_d[sample_id]).to(device))\n",
    "#                 )\n",
    "#                 .detach()\n",
    "#                 .cpu()\n",
    "#                 .numpy()\n",
    "#             )\n",
    "\n",
    "\n",
    "# best_checkpoint = torch.load(os.path.join(pretrain_folder, f\"best_model.pth\"))\n",
    "# model = best_checkpoint[\"model\"]\n",
    "# model.to(device)\n",
    "\n",
    "# model.eval()\n",
    "# model.set_encoder(\"source\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for sample_id in st_sample_id_l:\n",
    "#         pred_sp_noda_d[sample_id] = (\n",
    "#             torch.exp(model(torch.Tensor(mat_sp_test_s_d[sample_id]).to(device)))\n",
    "#             .detach()\n",
    "#             .cpu()\n",
    "#             .numpy()\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata_spatialLIBD = sc.read_h5ad(\n",
    "#     os.path.join(PROCESSED_DATA_DIR, \"adata_spatialLIBD.h5ad\")\n",
    "# )\n",
    "\n",
    "# adata_spatialLIBD_d = {}\n",
    "# for sample_id in st_sample_id_l:\n",
    "#     adata_spatialLIBD_d[sample_id] = adata_spatialLIBD[\n",
    "#         adata_spatialLIBD.obs.sample_id == sample_id\n",
    "#     ]\n",
    "#     adata_spatialLIBD_d[sample_id].obsm[\"spatial\"] = (\n",
    "#         adata_spatialLIBD_d[sample_id].obs[[\"X\", \"Y\"]].values\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_name_exN_l = []\n",
    "# for k, v in sc_sub_dict.items():\n",
    "#     if \"Ex\" in v:\n",
    "#         num_name_exN_l.append((k, v, int(v.split(\"_\")[1])))\n",
    "# num_name_exN_l.sort(key=lambda a: a[2])\n",
    "# num_name_exN_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex_to_L_d = {\n",
    "#     1: {5, 6},\n",
    "#     2: {5},\n",
    "#     3: {4, 5},\n",
    "#     4: {6},\n",
    "#     5: {5},\n",
    "#     6: {4, 5, 6},\n",
    "#     7: {4, 5, 6},\n",
    "#     8: {5, 6},\n",
    "#     9: {5, 6},\n",
    "#     10: {2, 3, 4},\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numlist = [t[0] for t in num_name_exN_l]\n",
    "# Ex_l = [t[2] for t in num_name_exN_l]\n",
    "# num_to_ex_d = dict(zip(numlist, Ex_l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_cellfraction(visnum, adata, pred_sp, ax=None):\n",
    "#     \"\"\"Plot predicted cell fraction for a given visnum\"\"\"\n",
    "#     adata.obs[\"Pred_label\"] = pred_sp[:, visnum]\n",
    "#     # vmin = 0\n",
    "#     # vmax = np.amax(pred_sp)\n",
    "\n",
    "#     sc.pl.spatial(\n",
    "#         adata,\n",
    "#         img_key=\"hires\",\n",
    "#         color=\"Pred_label\",\n",
    "#         palette=\"Set1\",\n",
    "#         size=1.5,\n",
    "#         legend_loc=None,\n",
    "#         title=f\"{sc_sub_dict[visnum]}\",\n",
    "#         spot_size=100,\n",
    "#         show=False,\n",
    "#         # vmin=vmin,\n",
    "#         # vmax=vmax,\n",
    "#         ax=ax,\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_roc(visnum, adata, pred_sp, name, ax=None):\n",
    "#     \"\"\"Plot ROC for a given visnum\"\"\"\n",
    "\n",
    "#     def layer_to_layer_number(x):\n",
    "#         for char in x:\n",
    "#             if char.isdigit():\n",
    "#                 if int(char) in Ex_to_L_d[num_to_ex_d[visnum]]:\n",
    "#                     return 1\n",
    "#         return 0\n",
    "\n",
    "#     y_pred = pred_sp[:, visnum]\n",
    "#     y_true = adata.obs[\"spatialLIBD\"].map(layer_to_layer_number).fillna(0)\n",
    "#     # print(y_true)\n",
    "#     # print(y_true.isna().sum())\n",
    "#     RocCurveDisplay.from_predictions(y_true=y_true, y_pred=y_pred, name=name, ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 5), constrained_layout=True)\n",
    "\n",
    "# sc.pl.spatial(\n",
    "#     adata_spatialLIBD_d[SAMPLE_ID_N],\n",
    "#     img_key=None,\n",
    "#     color=\"spatialLIBD\",\n",
    "#     palette=\"Accent_r\",\n",
    "#     size=1.5,\n",
    "#     title=SAMPLE_ID_N,\n",
    "#     # legend_loc = 4,\n",
    "#     spot_size=100,\n",
    "#     show=False,\n",
    "#     ax=ax,\n",
    "# )\n",
    "\n",
    "# ax.axis(\"equal\")\n",
    "# ax.set_xlabel(\"\")\n",
    "# ax.set_ylabel(\"\")\n",
    "\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(2, 5, figsize=(20, 8), constrained_layout=True)\n",
    "\n",
    "# for i, num in enumerate(numlist):\n",
    "#     plot_cellfraction(\n",
    "#         num, adata_spatialLIBD_d[SAMPLE_ID_N], pred_sp_d[SAMPLE_ID_N], ax.flat[i]\n",
    "#     )\n",
    "#     ax.flat[i].axis(\"equal\")\n",
    "#     ax.flat[i].set_xlabel(\"\")\n",
    "#     ax.flat[i].set_ylabel(\"\")\n",
    "\n",
    "# fig.show()\n",
    "\n",
    "# fig, ax = plt.subplots(\n",
    "#     2, 5, figsize=(20, 8), constrained_layout=True, sharex=True, sharey=True\n",
    "# )\n",
    "\n",
    "# for i, num in enumerate(numlist):\n",
    "#     plot_roc(\n",
    "#         num,\n",
    "#         adata_spatialLIBD_d[SAMPLE_ID_N],\n",
    "#         pred_sp_d[SAMPLE_ID_N],\n",
    "#         \"ADDA\",\n",
    "#         ax.flat[i],\n",
    "#     )\n",
    "#     plot_roc(\n",
    "#         num,\n",
    "#         adata_spatialLIBD_d[SAMPLE_ID_N],\n",
    "#         pred_sp_noda_d[SAMPLE_ID_N],\n",
    "#         \"NN_wo_da\",\n",
    "#         ax.flat[i],\n",
    "#     )\n",
    "#     ax.flat[i].plot([0, 1], [0, 1], transform=ax.flat[i].transAxes, ls=\"--\", color=\"k\")\n",
    "#     ax.flat[i].set_aspect(\"equal\")\n",
    "#     ax.flat[i].set_xlim([0, 1])\n",
    "#     ax.flat[i].set_ylim([0, 1])\n",
    "\n",
    "#     ax.flat[i].set_title(f\"{sc_sub_dict[num]}\")\n",
    "\n",
    "#     if i >= len(numlist) - 5:\n",
    "#         ax.flat[i].set_xlabel(\"FPR\")\n",
    "#     else:\n",
    "#         ax.flat[i].set_xlabel(\"\")\n",
    "#     if i % 5 == 0:\n",
    "#         ax.flat[i].set_ylabel(\"TPR\")\n",
    "#     else:\n",
    "#         ax.flat[i].set_ylabel(\"\")\n",
    "\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if TRAIN_USING_ALL_ST_SAMPLES:\n",
    "#     best_checkpoint = torch.load(os.path.join(advtrain_folder, f\"final_model.pth\"))\n",
    "# else:\n",
    "#     best_checkpoint = torch.load(\n",
    "#         os.path.join(advtrain_folder, SAMPLE_ID_N, f\"final_model.pth\")\n",
    "#     )\n",
    "\n",
    "# model = best_checkpoint[\"model\"]\n",
    "# model.to(device)\n",
    "\n",
    "# model.eval()\n",
    "# model.set_encoder(\"source\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     pred_mix = (\n",
    "#         torch.exp(model(torch.Tensor(sc_mix_test_s).to(device)))\n",
    "#         .detach()\n",
    "#         .cpu()\n",
    "#         .numpy()\n",
    "#     )\n",
    "\n",
    "# cell_type_nums = sc_sub_dict.keys()\n",
    "# nrows = ceil(len(cell_type_nums) / 5)\n",
    "\n",
    "# line_kws = {\"color\": \"tab:orange\"}\n",
    "# scatter_kws = {\"s\": 5}\n",
    "\n",
    "# props = dict(facecolor=\"w\", alpha=0.5)\n",
    "\n",
    "# fig, ax = plt.subplots(\n",
    "#     nrows,\n",
    "#     5,\n",
    "#     figsize=(25, 5 * nrows),\n",
    "#     constrained_layout=True,\n",
    "#     sharex=False,\n",
    "#     sharey=True,\n",
    "# )\n",
    "# for i, visnum in enumerate(cell_type_nums):\n",
    "#     sns.regplot(\n",
    "#         x=pred_mix[:, visnum],\n",
    "#         y=lab_mix_test[:, visnum],\n",
    "#         line_kws=line_kws,\n",
    "#         scatter_kws=scatter_kws,\n",
    "#         ax=ax.flat[i],\n",
    "#     ).set_title(sc_sub_dict[visnum])\n",
    "\n",
    "#     ax.flat[i].set_aspect(\"equal\")\n",
    "#     ax.flat[i].set_xlabel(\"Predicted Proportion\")\n",
    "\n",
    "#     if i % 5 == 0:\n",
    "#         ax.flat[i].set_ylabel(\"True Proportion\")\n",
    "#     else:\n",
    "#         ax.flat[i].set_ylabel(\"\")\n",
    "#     ax.flat[i].set_xlim([0, 1])\n",
    "#     ax.flat[i].set_ylim([0, 1])\n",
    "\n",
    "#     textstr = (\n",
    "#         f\"MSE: {mean_squared_error(pred_mix[:,visnum], lab_mix_test[:,visnum]):.5f}\"\n",
    "#     )\n",
    "\n",
    "#     # place a text box in upper left in axes coords\n",
    "#     ax.flat[i].text(\n",
    "#         0.95,\n",
    "#         0.05,\n",
    "#         textstr,\n",
    "#         transform=ax.flat[i].transAxes,\n",
    "#         verticalalignment=\"bottom\",\n",
    "#         horizontalalignment=\"right\",\n",
    "#         bbox=props,\n",
    "#     )\n",
    "\n",
    "# for i in range(len(cell_type_nums), nrows * 5):\n",
    "#     ax.flat[i].axis(\"off\")\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "26fed1cebb8fb31fead70ad12704b0087e346cab75d7d35ea772059a15f23623"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
