{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # ADDA for ST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Creating something like CellDART but it actually follows Adda in PyTorch as a first step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2318257/2415756365.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "from itertools import count\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import h5py\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "from src.da_models.adda import ADDAST\n",
    "from src.da_models.datasets import SpotDataset\n",
    "from src.da_models.utils import set_requires_grad\n",
    "from src.da_models.utils import initialize_weights\n",
    "\n",
    "\n",
    "# datetime object containing current date and time\n",
    "script_start_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%Hh%Mm%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_USING_ALL_ST_SAMPLES = False\n",
    "\n",
    "SAMPLE_ID_N = \"151673\"\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "NUM_WORKERS = 4\n",
    "INITIAL_TRAIN_EPOCHS = 200\n",
    "\n",
    "\n",
    "MIN_EPOCHS = 0.4 * INITIAL_TRAIN_EPOCHS\n",
    "EARLY_STOP_CRIT = INITIAL_TRAIN_EPOCHS\n",
    "\n",
    "PROCESSED_DATA_DIR = \"data/preprocessed\"\n",
    "\n",
    "MODEL_NAME = \"ADDA\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adversarial Hyperparameters\n",
    "\n",
    "EPOCHS = 100\n",
    "MIN_EPOCHS_ADV = 0.4 * EPOCHS\n",
    "EARLY_STOP_CRIT_ADV = EPOCHS\n",
    "ENC_LR = 0.0002\n",
    "ADAM_BETA_1 = 0.5\n",
    "ALPHA = 2\n",
    "DIS_LOOP_FACTOR = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = os.path.join(\"model\", MODEL_NAME, script_start_time)\n",
    "\n",
    "model_folder = os.path.join(\"model\", MODEL_NAME, \"V2\")\n",
    "\n",
    "if not os.path.isdir(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "    print(model_folder)\n",
    "\n",
    "# if not os.path.isdir(results_folder):\n",
    "#     os.makedirs(results_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sc.logging.print_versions()\n",
    "# sc.set_figure_params(facecolor=\"white\", figsize=(8, 8))\n",
    "# sc.settings.verbosity = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spatial data\n",
    "mat_sp_test_s_d = {}\n",
    "with h5py.File(os.path.join(PROCESSED_DATA_DIR, \"mat_sp_test_s_d.hdf5\"), \"r\") as f:\n",
    "    for sample_id in f:\n",
    "        mat_sp_test_s_d[sample_id] = f[sample_id][()]\n",
    "\n",
    "if TRAIN_USING_ALL_ST_SAMPLES:\n",
    "    with h5py.File(os.path.join(PROCESSED_DATA_DIR, \"mat_sp_train_s.hdf5\"), \"r\") as f:\n",
    "        mat_sp_train_s = f[\"all\"][()]\n",
    "else:\n",
    "    mat_sp_train_s_d = mat_sp_test_s_d\n",
    "\n",
    "# Load sc data\n",
    "with h5py.File(os.path.join(PROCESSED_DATA_DIR, \"sc.hdf5\"), \"r\") as f:\n",
    "    sc_mix_train_s = f[\"X/train\"][()]\n",
    "    sc_mix_val_s = f[\"X/val\"][()]\n",
    "    sc_mix_test_s = f[\"X/test\"][()]\n",
    "\n",
    "    lab_mix_train = f[\"y/train\"][()]\n",
    "    lab_mix_val = f[\"y/val\"][()]\n",
    "    lab_mix_test = f[\"y/test\"][()]\n",
    "\n",
    "# Load helper dicts / lists\n",
    "with open(os.path.join(PROCESSED_DATA_DIR, \"sc_sub_dict.pkl\"), \"rb\") as f:\n",
    "    sc_sub_dict = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(PROCESSED_DATA_DIR, \"sc_sub_dict2.pkl\"), \"rb\") as f:\n",
    "    sc_sub_dict2 = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(PROCESSED_DATA_DIR, \"st_sample_id_l.pkl\"), \"rb\") as f:\n",
    "    st_sample_id_l = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Training: Adversarial domain adaptation for cell fraction estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### source dataloaders\n",
    "source_train_set = SpotDataset(sc_mix_train_s, lab_mix_train)\n",
    "source_val_set = SpotDataset(sc_mix_val_s, lab_mix_val)\n",
    "source_test_set = SpotDataset(sc_mix_test_s, lab_mix_test)\n",
    "\n",
    "dataloader_source_train = torch.utils.data.DataLoader(\n",
    "    source_train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "dataloader_source_val = torch.utils.data.DataLoader(\n",
    "    source_val_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "dataloader_source_test = torch.utils.data.DataLoader(\n",
    "    source_test_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "### target dataloaders\n",
    "target_test_set_d = {}\n",
    "for sample_id in st_sample_id_l:\n",
    "    target_test_set_d[sample_id] = SpotDataset(mat_sp_test_s_d[sample_id])\n",
    "\n",
    "dataloader_target_test_d = {}\n",
    "for sample_id in st_sample_id_l:\n",
    "    dataloader_target_test_d[sample_id] = torch.utils.data.DataLoader(\n",
    "        target_test_set_d[sample_id],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "if TRAIN_USING_ALL_ST_SAMPLES:\n",
    "    target_train_set = SpotDataset(mat_sp_train_s)\n",
    "    dataloader_target_train = torch.utils.data.DataLoader(\n",
    "        target_train_set,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    target_train_set_dis = SpotDataset(deepcopy(mat_sp_train_s))\n",
    "    dataloader_target_train_dis = torch.utils.data.DataLoader(\n",
    "        target_train_set_dis,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "else:\n",
    "    target_train_set_d = {}\n",
    "    dataloader_target_train_d = {}\n",
    "\n",
    "    target_train_set_dis_d = {}\n",
    "    dataloader_target_train_dis_d = {}\n",
    "    for sample_id in st_sample_id_l:\n",
    "        target_train_set_d[sample_id] = SpotDataset(deepcopy(mat_sp_test_s_d[sample_id]))\n",
    "        dataloader_target_train_d[sample_id] = torch.utils.data.DataLoader(\n",
    "            target_train_set_d[sample_id],\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "        target_train_set_dis_d[sample_id] = SpotDataset(deepcopy(mat_sp_test_s_d[sample_id]))\n",
    "        dataloader_target_train_dis_d[sample_id] = torch.utils.data.DataLoader(\n",
    "            target_train_set_dis_d[sample_id],\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=True,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADDAST(\n",
       "  (source_encoder): ADDAMLPEncoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): BatchNorm1d(367, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=367, out_features=1024, bias=True)\n",
       "      (3): BatchNorm1d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (4): LeakyReLU(negative_slope=0.01)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=1024, out_features=64, bias=True)\n",
       "      (7): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (target_encoder): ADDAMLPEncoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): BatchNorm1d(367, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=367, out_features=1024, bias=True)\n",
       "      (3): BatchNorm1d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (4): LeakyReLU(negative_slope=0.01)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=1024, out_features=64, bias=True)\n",
       "      (7): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dis): AddaDiscriminator(\n",
       "    (head): Sequential(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (2): BatchNorm1d(32, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Dropout(p=0.5, inplace=False)\n",
       "      (5): Linear(in_features=32, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (clf): AddaPredictor(\n",
       "    (head): Sequential(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): Linear(in_features=64, out_features=33, bias=True)\n",
       "      (2): LogSoftmax(dim=1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ADDAST(sc_mix_train_s.shape[1], emb_dim=64, ncls_source=lab_mix_train.shape[1], is_adda=True)\n",
    "model.apply(initialize_weights)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_folder = os.path.join(model_folder, \"pretrain\")\n",
    "\n",
    "if not os.path.isdir(pretrain_folder):\n",
    "    os.makedirs(pretrain_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=0.002, betas=(0.9, 0.999), eps=1e-07\n",
    ")\n",
    "\n",
    "pre_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    pre_optimizer,\n",
    "    max_lr=0.002,\n",
    "    steps_per_epoch=len(dataloader_source_train),\n",
    "    epochs=INITIAL_TRAIN_EPOCHS,\n",
    ")\n",
    "\n",
    "criterion_clf = nn.KLDivLoss(reduction=\"batchmean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(x, y_true, model):\n",
    "    x = x.to(torch.float32).to(device)\n",
    "    y_true = y_true.to(torch.float32).to(device)\n",
    "\n",
    "    y_pred = model(x)\n",
    "\n",
    "    loss = criterion_clf(y_pred, y_true)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_acc(dataloader, model):\n",
    "    loss_running = []\n",
    "    mean_weights = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(dataloader):\n",
    "\n",
    "            loss = model_loss(*batch, model)\n",
    "\n",
    "            loss_running.append(loss.item())\n",
    "\n",
    "            # we will weight average by batch size later\n",
    "            mean_weights.append(len(batch))\n",
    "\n",
    "    return np.average(loss_running, weights=mean_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pretraining()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Initialize lists to store loss and accuracy values\\nloss_history = []\\nloss_history_val = []\\n\\nloss_history_running = []\\n\\n# Early Stopping\\nbest_loss_val = np.inf\\nearly_stop_count = 0\\n\\n\\n# Train\\nprint(\"Start pretrain...\")\\nouter = tqdm(total=INITIAL_TRAIN_EPOCHS, desc=\"Epochs\", position=0)\\ninner = tqdm(total=len(dataloader_source_train), desc=f\"Batch\", position=1)\\n\\ncheckpoint = {\\n    \"epoch\": -1,\\n    \"model\": model,\\n    \"optimizer\": pre_optimizer,\\n    \"scheduler\": pre_scheduler,\\n    # \\'scaler\\': scaler\\n}\\nfor epoch in range(INITIAL_TRAIN_EPOCHS):\\n    checkpoint[\"epoch\"] = epoch\\n\\n    # Train mode\\n    model.train()\\n    loss_running = []\\n    mean_weights = []\\n\\n    inner.refresh()  # force print final state\\n    inner.reset()  # reuse bar\\n    for _, batch in enumerate(dataloader_source_train):\\n        # lr_history_running.append(scheduler.get_last_lr())\\n\\n        pre_optimizer.zero_grad()\\n        loss = model_loss(*batch, model)\\n        loss_running.append(loss.item())\\n        mean_weights.append(len(batch))  # we will weight average by batch size later\\n\\n        # scaler.scale(loss).backward()\\n        # scaler.step(optimizer)\\n        # scaler.update()\\n\\n        loss.backward()\\n        pre_optimizer.step()\\n        pre_scheduler.step()\\n\\n        inner.update(1)\\n\\n    loss_history.append(np.average(loss_running, weights=mean_weights))\\n    loss_history_running.append(loss_running)\\n\\n    # Evaluate mode\\n    model.eval()\\n    with torch.no_grad():\\n        curr_loss_val = compute_acc(dataloader_source_val, model)\\n        loss_history_val.append(curr_loss_val)\\n\\n    # Print the results\\n    outer.update(1)\\n    print(\\n        \"epoch:\",\\n        epoch,\\n        \"train loss:\",\\n        round(loss_history[-1], 6),\\n        \"validation loss:\",\\n        round(loss_history_val[-1], 6),\\n        # \"next_lr:\", scheduler.get_last_lr(),\\n        end=\" \",\\n    )\\n    # Save the best weights\\n    if curr_loss_val < best_loss_val:\\n        best_loss_val = curr_loss_val\\n        torch.save(checkpoint, os.path.join(pretrain_folder, f\"best_model.pth\"))\\n        early_stop_count = 0\\n\\n        print(\"<-- new best val loss\")\\n    else:\\n        print(\"\")\\n\\n    # Save checkpoint every 10\\n    if epoch % 10 == 0 or epoch >= INITIAL_TRAIN_EPOCHS - 1:\\n        torch.save(checkpoint, os.path.join(pretrain_folder, f\"checkpt{epoch}.pth\"))\\n\\n    # check to see if validation loss has plateau\\'d\\n    if early_stop_count >= EARLY_STOP_CRIT and epoch >= MIN_EPOCHS - 1:\\n        print(f\"Validation loss plateaued after {early_stop_count} at epoch {epoch}\")\\n        torch.save(checkpoint, os.path.join(pretrain_folder, f\"earlystop{epoch}.pth\"))\\n        break\\n\\n    early_stop_count += 1\\n\\n\\n# Save final model\\nbest_checkpoint = torch.load(os.path.join(pretrain_folder, f\"best_model.pth\"))\\ntorch.save(best_checkpoint, os.path.join(pretrain_folder, f\"final_model.pth\"))\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Initialize lists to store loss and accuracy values\n",
    "loss_history = []\n",
    "loss_history_val = []\n",
    "\n",
    "loss_history_running = []\n",
    "\n",
    "# Early Stopping\n",
    "best_loss_val = np.inf\n",
    "early_stop_count = 0\n",
    "\n",
    "\n",
    "# Train\n",
    "print(\"Start pretrain...\")\n",
    "outer = tqdm(total=INITIAL_TRAIN_EPOCHS, desc=\"Epochs\", position=0)\n",
    "inner = tqdm(total=len(dataloader_source_train), desc=f\"Batch\", position=1)\n",
    "\n",
    "checkpoint = {\n",
    "    \"epoch\": -1,\n",
    "    \"model\": model,\n",
    "    \"optimizer\": pre_optimizer,\n",
    "    \"scheduler\": pre_scheduler,\n",
    "    # 'scaler': scaler\n",
    "}\n",
    "for epoch in range(INITIAL_TRAIN_EPOCHS):\n",
    "    checkpoint[\"epoch\"] = epoch\n",
    "\n",
    "    # Train mode\n",
    "    model.train()\n",
    "    loss_running = []\n",
    "    mean_weights = []\n",
    "\n",
    "    inner.refresh()  # force print final state\n",
    "    inner.reset()  # reuse bar\n",
    "    for _, batch in enumerate(dataloader_source_train):\n",
    "        # lr_history_running.append(scheduler.get_last_lr())\n",
    "\n",
    "        pre_optimizer.zero_grad()\n",
    "        loss = model_loss(*batch, model)\n",
    "        loss_running.append(loss.item())\n",
    "        mean_weights.append(len(batch))  # we will weight average by batch size later\n",
    "\n",
    "        # scaler.scale(loss).backward()\n",
    "        # scaler.step(optimizer)\n",
    "        # scaler.update()\n",
    "\n",
    "        loss.backward()\n",
    "        pre_optimizer.step()\n",
    "        pre_scheduler.step()\n",
    "\n",
    "        inner.update(1)\n",
    "\n",
    "    loss_history.append(np.average(loss_running, weights=mean_weights))\n",
    "    loss_history_running.append(loss_running)\n",
    "\n",
    "    # Evaluate mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        curr_loss_val = compute_acc(dataloader_source_val, model)\n",
    "        loss_history_val.append(curr_loss_val)\n",
    "\n",
    "    # Print the results\n",
    "    outer.update(1)\n",
    "    print(\n",
    "        \"epoch:\",\n",
    "        epoch,\n",
    "        \"train loss:\",\n",
    "        round(loss_history[-1], 6),\n",
    "        \"validation loss:\",\n",
    "        round(loss_history_val[-1], 6),\n",
    "        # \"next_lr:\", scheduler.get_last_lr(),\n",
    "        end=\" \",\n",
    "    )\n",
    "    # Save the best weights\n",
    "    if curr_loss_val < best_loss_val:\n",
    "        best_loss_val = curr_loss_val\n",
    "        torch.save(checkpoint, os.path.join(pretrain_folder, f\"best_model.pth\"))\n",
    "        early_stop_count = 0\n",
    "\n",
    "        print(\"<-- new best val loss\")\n",
    "    else:\n",
    "        print(\"\")\n",
    "\n",
    "    # Save checkpoint every 10\n",
    "    if epoch % 10 == 0 or epoch >= INITIAL_TRAIN_EPOCHS - 1:\n",
    "        torch.save(checkpoint, os.path.join(pretrain_folder, f\"checkpt{epoch}.pth\"))\n",
    "\n",
    "    # check to see if validation loss has plateau'd\n",
    "    if early_stop_count >= EARLY_STOP_CRIT and epoch >= MIN_EPOCHS - 1:\n",
    "        print(f\"Validation loss plateaued after {early_stop_count} at epoch {epoch}\")\n",
    "        torch.save(checkpoint, os.path.join(pretrain_folder, f\"earlystop{epoch}.pth\"))\n",
    "        break\n",
    "\n",
    "    early_stop_count += 1\n",
    "\n",
    "\n",
    "# Save final model\n",
    "best_checkpoint = torch.load(os.path.join(pretrain_folder, f\"best_model.pth\"))\n",
    "torch.save(best_checkpoint, os.path.join(pretrain_folder, f\"final_model.pth\"))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Adversarial Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "advtrain_folder = os.path.join(model_folder, \"advtrain\")\n",
    "\n",
    "if not os.path.isdir(advtrain_folder):\n",
    "    os.makedirs(advtrain_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_iter(iter):\n",
    "    while True:\n",
    "        yield from iter\n",
    "\n",
    "\n",
    "def iter_skip(iter, n=1):\n",
    "    for i in range(len(iter)*n):\n",
    "        if (i % n) == n - 1:\n",
    "            yield next(iter)\n",
    "        else:\n",
    "            yield None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_dis = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrim_loss_accu(x, domain, model):\n",
    "    x = x.to(device)\n",
    "\n",
    "    if domain == 'source':\n",
    "        y_dis = torch.zeros(x.shape[0], device=device, dtype=x.dtype).view(-1, 1)\n",
    "        emb = model.source_encoder(x) #.view(x.shape[0], -1)\n",
    "    elif domain == 'target':\n",
    "        y_dis = torch.ones(x.shape[0], device=device, dtype=x.dtype).view(-1, 1)\n",
    "        emb = model.target_encoder(x) #.view(x.shape[0], -1)\n",
    "    else:\n",
    "        raise(ValueError, f\"invalid domain {domain} given, must be 'source' or 'target'\")\n",
    "    \n",
    "    y_pred = model.dis(emb)\n",
    "    \n",
    "    loss = criterion_dis(y_pred, y_dis)\n",
    "    accu = torch.mean(\n",
    "        (torch.round(y_pred).to(torch.long) == y_dis).to(torch.float32)\n",
    "    ).cpu()\n",
    "\n",
    "    return loss, accu\n",
    "\n",
    "\n",
    "def compute_acc_dis(dataloader_source, dataloader_target, model):\n",
    "    loss_history = []\n",
    "    accu_history = []\n",
    "    # iters = max(len(dataloader_source), len(dataloader_target))\n",
    "    model.eval()\n",
    "    model.dis.eval()\n",
    "    model.target_encoder.eval()\n",
    "    model.source_encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_running = []\n",
    "        accu_running = []\n",
    "        mean_weights = []\n",
    "        # batch_cycler = zip(cycle_iter(dataloader_source), cycle_iter(dataloader_target))\n",
    "        for _, (X, _) in enumerate(dataloader_source):\n",
    "            X = X.to(device)\n",
    "\n",
    "            y_dis = torch.zeros(X.shape[0], device=device, dtype=X.dtype).view(-1, 1)\n",
    "\n",
    "            emb = model.source_encoder(X) #.view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = model.dis(emb)\n",
    "\n",
    "            loss_running.append(criterion_dis(y_pred, y_dis).item())\n",
    "            accu_running.append(\n",
    "                torch.mean(\n",
    "                    (torch.flatten(torch.argmax(y_pred, dim=1)) == y_dis).to(\n",
    "                        torch.float32\n",
    "                    )\n",
    "                ).cpu()\n",
    "            )\n",
    "        loss_history.append(np.average(loss_running, weights=mean_weights))\n",
    "        accu_history.append(np.average(accu_running, weights=mean_weights))\n",
    "\n",
    "        loss_running = []\n",
    "        accu_running = []\n",
    "        mean_weights = []\n",
    "        for _, (X, _) in enumerate(dataloader_target):\n",
    "            X = X.to(device)\n",
    "\n",
    "            y_dis = torch.ones(X.shape[0], device=device, dtype=X.dtype).view(-1, 1)\n",
    "\n",
    "            emb = model.source_encoder(X) #.view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = model.dis(emb)\n",
    "\n",
    "            loss_running.append(criterion_dis(y_pred, y_dis).item())\n",
    "            accu_running.append(\n",
    "                torch.mean(\n",
    "                    (torch.flatten(torch.argmax(y_pred, dim=1)) == y_dis).to(\n",
    "                        torch.float32\n",
    "                    )\n",
    "                ).cpu()\n",
    "            )\n",
    "        loss_history.append(np.average(loss_running, weights=mean_weights))\n",
    "        accu_history.append(np.average(accu_running, weights=mean_weights))\n",
    "\n",
    "    return np.average(loss_history), np.average(accu_history)\n",
    "\n",
    "\n",
    "def encoder_loss(x_target, model):\n",
    "    x_target = x_target.to(device)\n",
    "\n",
    "    # flip label\n",
    "    y_dis = torch.zeros(x_target.shape[0], device=device, dtype=x_target.dtype).view(-1, 1)\n",
    "\n",
    "    emb_target = model.target_encoder(x_target) #.view(x_target.shape[0], -1)\n",
    "    y_pred = model.dis(emb_target)\n",
    "    loss = criterion_dis(y_pred, y_dis)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_adversarial(\n",
    "#     model,\n",
    "#     save_folder,\n",
    "#     dataloader_source_train,\n",
    "#     dataloader_source_val,\n",
    "#     dataloader_target_train,\n",
    "# ):\n",
    "#     model.to(device)\n",
    "#     model.advtraining()\n",
    "\n",
    "#     target_optimizer = torch.optim.Adam(\n",
    "#         model.target_encoder.parameters(), lr=0.0005, betas=(0.9, 0.999), eps=1e-07\n",
    "#     )\n",
    "#     dis_optimizer = torch.optim.Adam(\n",
    "#         model.dis.parameters(), lr=0.00025, betas=(0.9, 0.999), eps=1e-07\n",
    "#     )\n",
    "\n",
    "#     iters = max(len(dataloader_source_train), len(dataloader_target_train))\n",
    "\n",
    "#     dis_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "#         dis_optimizer, max_lr=0.0005, steps_per_epoch=iters, epochs=EPOCHS\n",
    "#     )\n",
    "#     target_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "#         target_optimizer, max_lr=0.0005, steps_per_epoch=iters, epochs=EPOCHS\n",
    "#     )\n",
    "\n",
    "#     n_samples_source = len(dataloader_source_train.dataset)\n",
    "#     n_samples_target = len(dataloader_target_train.dataset)\n",
    "#     p = n_samples_source / (n_samples_source + n_samples_target)\n",
    "#     rand_loss = -(p * np.log(0.5)) - (1 - p) * np.log(0.5)\n",
    "\n",
    "#     # Initialize lists to store loss and accuracy values\n",
    "#     loss_history = []\n",
    "#     accu_history = []\n",
    "#     loss_history_running = []\n",
    "\n",
    "#     # Early Stopping\n",
    "#     best_loss_val = np.inf\n",
    "#     early_stop_count = 0\n",
    "\n",
    "#     # Train\n",
    "#     print(\"Start adversarial training...\")\n",
    "#     print(\"Discriminator target loss:\", rand_loss)\n",
    "#     outer = tqdm(total=EPOCHS, desc=\"Epochs\", position=0)\n",
    "#     inner1 = tqdm(total=iters, desc=f\"Batch (Discriminator)\", position=1)\n",
    "#     inner2 = tqdm(total=iters, desc=f\"Batch (Encoder)\", position=2)\n",
    "#     checkpoint = {\n",
    "#         \"epoch\": -1,\n",
    "#         \"model\": model,\n",
    "#         \"dis_optimizer\": dis_optimizer,\n",
    "#         \"target_optimizer\": target_optimizer,\n",
    "#         \"dis_scheduler\": dis_scheduler,\n",
    "#         \"target_scheduler\": target_scheduler,\n",
    "#     }\n",
    "#     for epoch in range(EPOCHS):\n",
    "#         checkpoint[\"epoch\"] = epoch\n",
    "\n",
    "#         # Train mode\n",
    "#         model.train()\n",
    "\n",
    "#         loss_running = []\n",
    "#         accu_running = []\n",
    "#         mean_weights = []\n",
    "\n",
    "#         inner1.refresh()  # force print final state\n",
    "#         inner1.reset()  # reuse bar\n",
    "#         inner2.refresh()  # force print final state\n",
    "#         inner2.reset()  # reuse bar\n",
    "\n",
    "#         model.train_discriminator()\n",
    "#         model.target_encoder.eval()\n",
    "#         model.source_encoder.eval()\n",
    "#         model.dis.train()\n",
    "#         batch_cycler = zip(\n",
    "#             cycle_iter(dataloader_source_train), cycle_iter(dataloader_target_train)\n",
    "#         )\n",
    "#         for _ in range(iters):\n",
    "#             # lr_history_running.append(scheduler.get_last_lr())\n",
    "#             dis_optimizer.zero_grad()\n",
    "\n",
    "#             (x_source, _), (x_target, _) = next(batch_cycler)\n",
    "#             loss, accu = discrim_loss_accu(x_source, x_target, model)\n",
    "#             loss_running.append(loss.item())\n",
    "#             accu_running.append(accu)\n",
    "#             mean_weights.append(len(x_source) + len(x_target))\n",
    "\n",
    "#             # scaler.scale(loss).backward()\n",
    "#             # scaler.step(optimizer)\n",
    "#             # scaler.update()\n",
    "\n",
    "#             loss.backward()\n",
    "#             dis_optimizer.step()\n",
    "#             dis_scheduler.step()\n",
    "\n",
    "#             inner1.update(1)\n",
    "\n",
    "#         loss_history.append(np.average(loss_running, weights=mean_weights))\n",
    "#         accu_history.append(np.average(accu_running, weights=mean_weights))\n",
    "#         loss_history_running.append(loss_running)\n",
    "\n",
    "#         model.train_target_encoder()\n",
    "#         model.target_encoder.train()\n",
    "#         model.source_encoder.eval()\n",
    "#         model.dis.eval()\n",
    "#         batch_cycler = zip(\n",
    "#             cycle_iter(dataloader_source_train), cycle_iter(dataloader_target_train)\n",
    "#         )\n",
    "#         for _ in range(iters):\n",
    "#             target_optimizer.zero_grad()\n",
    "\n",
    "#             _, (x_target, _) = next(batch_cycler)\n",
    "#             loss = encoder_loss(x_target, model)\n",
    "\n",
    "#             loss.backward()\n",
    "#             target_optimizer.step()\n",
    "#             target_scheduler.step()\n",
    "\n",
    "#             inner2.update(1)\n",
    "\n",
    "#         diff_from_rand = math.fabs(loss_history[-1] - rand_loss)\n",
    "\n",
    "#         # Print the results\n",
    "#         outer.update(1)\n",
    "#         print(\n",
    "#             \"epoch:\",\n",
    "#             epoch,\n",
    "#             \"dis loss:\",\n",
    "#             round(loss_history[-1], 6),\n",
    "#             \"dis accu:\",\n",
    "#             round(accu_history[-1], 6),\n",
    "#             \"difference from random loss:\",\n",
    "#             round(diff_from_rand, 6),\n",
    "#             # \"next_lr:\", scheduler.get_last_lr(),\n",
    "#             end=\" \",\n",
    "#         )\n",
    "\n",
    "#         # Save the best weights\n",
    "#         if diff_from_rand < best_loss_val:\n",
    "#             best_loss_val = diff_from_rand\n",
    "#             torch.save(checkpoint, os.path.join(save_folder, f\"best_model.pth\"))\n",
    "#             early_stop_count = 0\n",
    "\n",
    "#             print(\"<-- new best difference from random loss\")\n",
    "#         else:\n",
    "#             print(\"\")\n",
    "\n",
    "#         # Save checkpoint every 10\n",
    "#         if epoch % 10 == 0 or epoch >= EPOCHS - 1:\n",
    "#             torch.save(checkpoint, os.path.join(save_folder, f\"checkpt{epoch}.pth\"))\n",
    "\n",
    "#         # check to see if validation loss has plateau'd\n",
    "#         if early_stop_count >= EARLY_STOP_CRIT_ADV and epoch > MIN_EPOCHS_ADV - 1:\n",
    "#             print(\n",
    "#                 f\"Discriminator loss plateaued after {early_stop_count} at epoch {epoch}\"\n",
    "#             )\n",
    "#             torch.save(checkpoint, os.path.join(save_folder, f\"earlystop_{epoch}.pth\"))\n",
    "#             break\n",
    "\n",
    "#         early_stop_count += 1\n",
    "\n",
    "#     # Save final model\n",
    "#     torch.save(checkpoint, os.path.join(save_folder, f\"final_model.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversarial_iters(\n",
    "    model,\n",
    "    save_folder,\n",
    "    dataloader_source_train,\n",
    "    dataloader_source_val,\n",
    "    dataloader_target_train,\n",
    "    dataloader_target_train_dis,\n",
    "):\n",
    "    model.to(device)\n",
    "    model.advtraining()\n",
    "\n",
    "    target_optimizer = torch.optim.Adam(\n",
    "        model.target_encoder.parameters(), lr=ENC_LR, betas=(ADAM_BETA_1, 0.999), eps=1e-07\n",
    "    )\n",
    "    dis_optimizer = torch.optim.Adam(\n",
    "        model.dis.parameters(), lr=ALPHA * ENC_LR, betas=(ADAM_BETA_1, 0.999), eps=1e-07\n",
    "    )\n",
    "\n",
    "    # iters = -(max_len_dataloader // -(1 + DIS_LOOP_FACTOR))  # ceiling divide\n",
    "\n",
    "    dataloader_lengths = [\n",
    "        len(dataloader_source_train),\n",
    "        len(dataloader_target_train),\n",
    "        len(dataloader_target_train_dis) * DIS_LOOP_FACTOR,\n",
    "    ]\n",
    "    max_len_dataloader = np.amax(dataloader_lengths)\n",
    "    longest = np.argmax(dataloader_lengths)\n",
    "\n",
    "    iters_val = max(len(dataloader_source_val), len(dataloader_target_train))\n",
    "\n",
    "    # dis_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    #     dis_optimizer, max_lr=0.0005, steps_per_epoch=iters, epochs=EPOCHS\n",
    "    # )\n",
    "    # target_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    #     target_optimizer, max_lr=0.0005, steps_per_epoch=iters, epochs=EPOCHS\n",
    "    # )\n",
    "\n",
    "    # Initialize lists to store loss and accuracy values\n",
    "    loss_history = []\n",
    "    accu_history = []\n",
    "    loss_history_val = []\n",
    "    accu_history_val = []\n",
    "    loss_history_running = []\n",
    "\n",
    "    loss_history_gen = []\n",
    "    loss_history_gen_running = []\n",
    "    mean_weights_gen = []\n",
    "\n",
    "    # Early Stopping\n",
    "    best_loss_val = np.inf\n",
    "    early_stop_count = 0\n",
    "    # Train\n",
    "    print(\"Start adversarial training...\")\n",
    "    outer = tqdm(total=EPOCHS, desc=\"Epochs\", position=0)\n",
    "    inner1 = tqdm(total=max_len_dataloader, desc=f\"Batch\", position=1)\n",
    "    checkpoint = {\n",
    "        \"epoch\": -1,\n",
    "        \"model\": model,\n",
    "        \"dis_optimizer\": dis_optimizer,\n",
    "        \"target_optimizer\": target_optimizer,\n",
    "        # \"dis_scheduler\": dis_scheduler,\n",
    "        # \"target_scheduler\": target_scheduler,\n",
    "    }\n",
    "    for epoch in range(EPOCHS):\n",
    "        checkpoint[\"epoch\"] = epoch\n",
    "\n",
    "        # Train mode\n",
    "        model.train()\n",
    "        model.target_encoder.train()\n",
    "        model.source_encoder.train()\n",
    "        model.dis.train()\n",
    "\n",
    "        loss_running = []\n",
    "        accu_running = []\n",
    "        mean_weights = []\n",
    "\n",
    "        loss_running_gen = []\n",
    "        mean_weights_gen = []\n",
    "\n",
    "        inner1.refresh()  # force print final state\n",
    "        inner1.reset()  # reuse bar\n",
    "\n",
    "\n",
    "        # if longest == 0:\n",
    "        #     batch_cycler = zip(\n",
    "        #         dataloader_source_train,\n",
    "        #         cycle_iter(dataloader_target_train),\n",
    "        #         cycle_iter(iter_skip(iter(dataloader_target_train_dis), DIS_LOOP_FACTOR))\n",
    "        #     )\n",
    "        # elif longest == 1:\n",
    "        #     batch_cycler = zip(\n",
    "        #         cycle_iter(dataloader_source_train),\n",
    "        #         dataloader_target_train,\n",
    "        #         cycle_iter(iter_skip(iter(dataloader_target_train_dis), DIS_LOOP_FACTOR))\n",
    "        #     )\n",
    "        # else:\n",
    "        #     batch_cycler = zip(\n",
    "        #         cycle_iter(dataloader_source_train),\n",
    "        #         cycle_iter(dataloader_target_train),\n",
    "        #         iter_skip(iter(dataloader_target_train_dis), DIS_LOOP_FACTOR)\n",
    "        #     )\n",
    "\n",
    "        s_train_iter = iter(dataloader_source_train)\n",
    "        t_train_iter = iter(dataloader_target_train)\n",
    "        t_train_dis_iter = iter(dataloader_target_train_dis)\n",
    "        for i in range(max_len_dataloader):\n",
    "            try:\n",
    "                x_source, _ = next(s_train_iter)\n",
    "            except StopIteration:\n",
    "                s_train_iter = iter(dataloader_source_train)\n",
    "                x_source, _ = next(s_train_iter)\n",
    "            try:\n",
    "                x_target, _ = next(t_train_iter)\n",
    "            except StopIteration:\n",
    "                t_train_iter = iter(dataloader_target_train)\n",
    "                x_target, _ = next(t_train_iter)\n",
    "\n",
    "            train_encoder_step = (i % DIS_LOOP_FACTOR) == DIS_LOOP_FACTOR - 1\n",
    "            \n",
    "            # print(x_target_enc, (i % DIS_LOOP_FACTOR))\n",
    "            model.train_discriminator()\n",
    "            # model.target_encoder.train()\n",
    "            # model.source_encoder.train()\n",
    "            # model.dis.train()\n",
    "\n",
    "            set_requires_grad(model.target_encoder, False)\n",
    "            set_requires_grad(model.source_encoder, False)\n",
    "            set_requires_grad(model.dis, True)\n",
    "\n",
    "            # lr_history_running.append(scheduler.get_last_lr())\n",
    "            dis_optimizer.zero_grad()\n",
    "\n",
    "            loss, accu = discrim_loss_accu(x_source, 'source', model)\n",
    "            loss_running.append(loss.item())\n",
    "            accu_running.append(accu)\n",
    "            mean_weights.append(len(x_source))\n",
    "\n",
    "            \n",
    "            # scaler.scale(loss).backward()\n",
    "            # scaler.step(optimizer)\n",
    "            # scaler.update()\n",
    "\n",
    "            loss.backward()\n",
    "            dis_optimizer.step()\n",
    "\n",
    "            dis_optimizer.zero_grad()\n",
    "\n",
    "            loss, accu = discrim_loss_accu(x_target,'target', model)\n",
    "            loss_running.append(loss.item())\n",
    "            accu_running.append(accu)\n",
    "            mean_weights.append(len(x_target))\n",
    "\n",
    "            \n",
    "            # scaler.scale(loss).backward()\n",
    "            # scaler.step(optimizer)\n",
    "            # scaler.update()\n",
    "\n",
    "            loss.backward()\n",
    "            dis_optimizer.step()\n",
    "            # dis_scheduler.step()\n",
    "                \n",
    "            # print(i % DIS_LOOP_FACTOR)\n",
    "            if train_encoder_step:\n",
    "                try:\n",
    "                    x_target_enc, _ = next(t_train_dis_iter)\n",
    "                except StopIteration:\n",
    "                    t_train_dis_iter = iter(dataloader_target_train_dis)\n",
    "                    x_target_enc, _ = next(t_train_dis_iter)\n",
    "                model.train_target_encoder()\n",
    "                # model.target_encoder.train()\n",
    "                # model.source_encoder.train()\n",
    "                # model.dis.train()\n",
    "\n",
    "                set_requires_grad(model.target_encoder, True)\n",
    "                set_requires_grad(model.source_encoder, False)\n",
    "                set_requires_grad(model.dis, False)\n",
    "\n",
    "                target_optimizer.zero_grad()\n",
    "\n",
    "                loss = encoder_loss(x_target_enc, model)\n",
    "\n",
    "                loss_running_gen.append(loss.item())\n",
    "                mean_weights_gen.append(len(x_target_enc))\n",
    "\n",
    "                loss.backward()\n",
    "                target_optimizer.step()\n",
    "            # target_scheduler.step()\n",
    "\n",
    "            inner1.update(1)\n",
    "        loss_history.append(np.average(loss_running, weights=mean_weights))\n",
    "        accu_history.append(np.average(accu_running, weights=mean_weights))\n",
    "        loss_history_running.append(loss_running)\n",
    "        loss_history_gen.append(np.average(loss_running_gen, weights=mean_weights_gen))\n",
    "        loss_history_gen_running.append(loss_running_gen)\n",
    "        # ensure batch_cycler gets garbage collected, freeing the dataloaders\n",
    "        \n",
    "        model.eval()\n",
    "        model.dis.eval()\n",
    "        model.target_encoder.eval()\n",
    "        model.source_encoder.eval()\n",
    "\n",
    "        set_requires_grad(model, True)\n",
    "        set_requires_grad(model.target_encoder, True)\n",
    "        set_requires_grad(model.source_encoder, True)\n",
    "        set_requires_grad(model.dis, True)\n",
    "\n",
    "        # del batch_cycler\n",
    "        # with torch.no_grad():\n",
    "        #     curr_loss_val, curr_acc_val = compute_acc_dis(\n",
    "        #         dataloader_source_val, dataloader_target_train, model\n",
    "        #     )\n",
    "        #     loss_history_val.append(curr_loss_val)\n",
    "        #     accu_history_val.append(curr_loss_val)\n",
    "\n",
    "        # Print the results\n",
    "        outer.update(1)\n",
    "        print(\n",
    "            \"epoch:\",\n",
    "            epoch,\n",
    "            \"gen train loss:\",\n",
    "            round(loss_history_gen[-1], 6),\n",
    "            \"dis train loss:\",\n",
    "            round(loss_history[-1], 6),\n",
    "            \"dis train accu:\",\n",
    "            round(accu_history[-1], 6),\n",
    "            # \"dis val loss:\",\n",
    "            # round(loss_history_val[-1], 6),\n",
    "            # \"dis val accu:\",\n",
    "            # round(accu_history_val[-1], 6),\n",
    "            # \"next_lr:\", scheduler.get_last_lr(),\n",
    "            end=\" \",\n",
    "        )\n",
    "\n",
    "        # # Save the best weights\n",
    "        # if diff_from_rand < best_loss_val:\n",
    "        #     best_loss_val = diff_from_rand\n",
    "        #     torch.save(checkpoint, os.path.join(save_folder, f\"best_model.pth\"))\n",
    "        #     early_stop_count = 0\n",
    "\n",
    "        #     print(\"<-- new best difference from random loss\")\n",
    "        # else:\n",
    "        #     print(\"\")\n",
    "\n",
    "        print(\"\")\n",
    "\n",
    "        # Save checkpoint every 10\n",
    "        if epoch % 10 == 0 or epoch >= EPOCHS - 1:\n",
    "            torch.save(checkpoint, os.path.join(save_folder, f\"checkpt{epoch}.pth\"))\n",
    "\n",
    "        # check to see if validation loss has plateau'd\n",
    "        if early_stop_count >= EARLY_STOP_CRIT_ADV and epoch > MIN_EPOCHS_ADV - 1:\n",
    "            print(\n",
    "                f\"Discriminator loss plateaued after {early_stop_count} at epoch {epoch}\"\n",
    "            )\n",
    "            torch.save(checkpoint, os.path.join(save_folder, f\"earlystop_{epoch}.pth\"))\n",
    "            break\n",
    "\n",
    "        early_stop_count += 1\n",
    "\n",
    "    # Save final model\n",
    "    torch.save(checkpoint, os.path.join(save_folder, f\"final_model.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# st_sample_id_l = [SAMPLE_ID_N]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial training for ST slide 151509: \n",
      "Start adversarial training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1977e3d48b934bb497e8ae38364193a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b22c39adf44d08be7400ded5176656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 gen train loss: 0.644878 dis train loss: 0.726105 dis train accu: 0.38608 \n",
      "epoch: 1 gen train loss: 0.654195 dis train loss: 0.720103 dis train accu: 0.407582 \n",
      "epoch: 2 gen train loss: 0.657657 dis train loss: 0.71618 dis train accu: 0.423133 \n",
      "epoch: 3 gen train loss: 0.664204 dis train loss: 0.715938 dis train accu: 0.428554 \n",
      "epoch: 4 gen train loss: 0.667946 dis train loss: 0.714565 dis train accu: 0.436238 \n",
      "epoch: 5 gen train loss: 0.67313 dis train loss: 0.713817 dis train accu: 0.441618 \n",
      "epoch: 6 gen train loss: 0.673681 dis train loss: 0.71155 dis train accu: 0.443738 \n",
      "epoch: 7 gen train loss: 0.679589 dis train loss: 0.711386 dis train accu: 0.450056 \n",
      "epoch: 8 gen train loss: 0.676709 dis train loss: 0.711322 dis train accu: 0.454907 \n",
      "epoch: 9 gen train loss: 0.678353 dis train loss: 0.709704 dis train accu: 0.455518 \n",
      "epoch: 10 gen train loss: 0.687062 dis train loss: 0.711099 dis train accu: 0.460471 \n",
      "epoch: 11 gen train loss: 0.688761 dis train loss: 0.708933 dis train accu: 0.463365 \n",
      "epoch: 12 gen train loss: 0.686791 dis train loss: 0.708229 dis train accu: 0.465342 \n",
      "epoch: 13 gen train loss: 0.688276 dis train loss: 0.708639 dis train accu: 0.469174 \n",
      "epoch: 14 gen train loss: 0.689538 dis train loss: 0.708587 dis train accu: 0.47219 \n",
      "epoch: 15 gen train loss: 0.694041 dis train loss: 0.706456 dis train accu: 0.474941 \n",
      "epoch: 16 gen train loss: 0.691421 dis train loss: 0.70566 dis train accu: 0.475981 \n",
      "epoch: 17 gen train loss: 0.6889 dis train loss: 0.706344 dis train accu: 0.476837 \n",
      "epoch: 18 gen train loss: 0.689827 dis train loss: 0.705427 dis train accu: 0.478386 \n",
      "epoch: 19 gen train loss: 0.692507 dis train loss: 0.704961 dis train accu: 0.479242 \n",
      "epoch: 20 gen train loss: 0.693359 dis train loss: 0.705008 dis train accu: 0.479955 \n",
      "epoch: 21 gen train loss: 0.690508 dis train loss: 0.705171 dis train accu: 0.482931 \n",
      "epoch: 22 gen train loss: 0.693264 dis train loss: 0.704898 dis train accu: 0.483685 \n",
      "epoch: 23 gen train loss: 0.688383 dis train loss: 0.704718 dis train accu: 0.485499 \n",
      "epoch: 24 gen train loss: 0.693256 dis train loss: 0.703382 dis train accu: 0.486885 \n",
      "epoch: 25 gen train loss: 0.694795 dis train loss: 0.703854 dis train accu: 0.489249 \n",
      "epoch: 26 gen train loss: 0.691839 dis train loss: 0.703169 dis train accu: 0.489738 \n",
      "epoch: 27 gen train loss: 0.694642 dis train loss: 0.703896 dis train accu: 0.490085 \n",
      "epoch: 28 gen train loss: 0.692297 dis train loss: 0.703057 dis train accu: 0.492041 \n",
      "epoch: 29 gen train loss: 0.691795 dis train loss: 0.702168 dis train accu: 0.494059 \n",
      "epoch: 30 gen train loss: 0.690656 dis train loss: 0.701458 dis train accu: 0.495119 \n",
      "epoch: 31 gen train loss: 0.690748 dis train loss: 0.701452 dis train accu: 0.494854 \n",
      "epoch: 32 gen train loss: 0.689242 dis train loss: 0.701222 dis train accu: 0.494528 \n",
      "epoch: 33 gen train loss: 0.689136 dis train loss: 0.702361 dis train accu: 0.495567 \n",
      "epoch: 34 gen train loss: 0.689807 dis train loss: 0.700386 dis train accu: 0.497564 \n",
      "epoch: 35 gen train loss: 0.689383 dis train loss: 0.700968 dis train accu: 0.498054 \n",
      "epoch: 36 gen train loss: 0.687859 dis train loss: 0.700577 dis train accu: 0.499582 \n",
      "epoch: 37 gen train loss: 0.689337 dis train loss: 0.699943 dis train accu: 0.500601 \n",
      "epoch: 38 gen train loss: 0.691702 dis train loss: 0.700727 dis train accu: 0.501009 \n",
      "epoch: 39 gen train loss: 0.690786 dis train loss: 0.699985 dis train accu: 0.500622 \n",
      "epoch: 40 gen train loss: 0.688555 dis train loss: 0.700083 dis train accu: 0.502374 \n",
      "epoch: 41 gen train loss: 0.690607 dis train loss: 0.698583 dis train accu: 0.501844 \n",
      "epoch: 42 gen train loss: 0.692791 dis train loss: 0.699345 dis train accu: 0.50372 \n",
      "epoch: 43 gen train loss: 0.690381 dis train loss: 0.698617 dis train accu: 0.504351 \n",
      "epoch: 44 gen train loss: 0.689811 dis train loss: 0.698262 dis train accu: 0.50533 \n",
      "epoch: 45 gen train loss: 0.688804 dis train loss: 0.698688 dis train accu: 0.504127 \n",
      "epoch: 46 gen train loss: 0.689792 dis train loss: 0.698826 dis train accu: 0.503801 \n",
      "epoch: 47 gen train loss: 0.690273 dis train loss: 0.697597 dis train accu: 0.505798 \n",
      "epoch: 48 gen train loss: 0.687992 dis train loss: 0.698169 dis train accu: 0.506369 \n",
      "epoch: 49 gen train loss: 0.689315 dis train loss: 0.698763 dis train accu: 0.506104 \n",
      "epoch: 50 gen train loss: 0.688221 dis train loss: 0.697893 dis train accu: 0.506817 \n",
      "epoch: 51 gen train loss: 0.688569 dis train loss: 0.698137 dis train accu: 0.507796 \n",
      "epoch: 52 gen train loss: 0.689474 dis train loss: 0.697771 dis train accu: 0.507612 \n",
      "epoch: 53 gen train loss: 0.68809 dis train loss: 0.697995 dis train accu: 0.507816 \n",
      "epoch: 54 gen train loss: 0.688315 dis train loss: 0.697047 dis train accu: 0.508101 \n",
      "epoch: 55 gen train loss: 0.689902 dis train loss: 0.696607 dis train accu: 0.508285 \n",
      "epoch: 56 gen train loss: 0.688185 dis train loss: 0.696384 dis train accu: 0.508856 \n",
      "epoch: 57 gen train loss: 0.687919 dis train loss: 0.696725 dis train accu: 0.509019 \n",
      "epoch: 58 gen train loss: 0.686501 dis train loss: 0.697532 dis train accu: 0.509304 \n",
      "epoch: 59 gen train loss: 0.687496 dis train loss: 0.697233 dis train accu: 0.507694 \n",
      "epoch: 60 gen train loss: 0.689058 dis train loss: 0.696776 dis train accu: 0.509834 \n",
      "epoch: 61 gen train loss: 0.687913 dis train loss: 0.696397 dis train accu: 0.510364 \n",
      "epoch: 62 gen train loss: 0.6862 dis train loss: 0.696967 dis train accu: 0.50961 \n",
      "epoch: 63 gen train loss: 0.687832 dis train loss: 0.696015 dis train accu: 0.510751 \n",
      "epoch: 64 gen train loss: 0.687266 dis train loss: 0.696279 dis train accu: 0.510833 \n",
      "epoch: 65 gen train loss: 0.687083 dis train loss: 0.696023 dis train accu: 0.51122 \n",
      "epoch: 66 gen train loss: 0.68865 dis train loss: 0.695425 dis train accu: 0.510792 \n",
      "epoch: 67 gen train loss: 0.688794 dis train loss: 0.695885 dis train accu: 0.510343 \n",
      "epoch: 68 gen train loss: 0.687439 dis train loss: 0.696062 dis train accu: 0.51071 \n",
      "epoch: 69 gen train loss: 0.686788 dis train loss: 0.695741 dis train accu: 0.511118 \n",
      "epoch: 70 gen train loss: 0.688268 dis train loss: 0.695699 dis train accu: 0.511138 \n",
      "epoch: 71 gen train loss: 0.689216 dis train loss: 0.695231 dis train accu: 0.511118 \n",
      "epoch: 72 gen train loss: 0.687869 dis train loss: 0.695765 dis train accu: 0.511138 \n",
      "epoch: 73 gen train loss: 0.688068 dis train loss: 0.695406 dis train accu: 0.510934 \n",
      "epoch: 74 gen train loss: 0.68687 dis train loss: 0.695607 dis train accu: 0.511016 \n",
      "epoch: 75 gen train loss: 0.688634 dis train loss: 0.696148 dis train accu: 0.51071 \n",
      "epoch: 76 gen train loss: 0.686116 dis train loss: 0.694934 dis train accu: 0.511403 \n",
      "epoch: 77 gen train loss: 0.686867 dis train loss: 0.694638 dis train accu: 0.51177 \n",
      "epoch: 78 gen train loss: 0.687977 dis train loss: 0.69598 dis train accu: 0.511505 \n",
      "epoch: 79 gen train loss: 0.687244 dis train loss: 0.69488 dis train accu: 0.511933 \n",
      "epoch: 80 gen train loss: 0.688124 dis train loss: 0.695492 dis train accu: 0.511505 \n",
      "epoch: 81 gen train loss: 0.687296 dis train loss: 0.695171 dis train accu: 0.511566 \n",
      "epoch: 82 gen train loss: 0.685095 dis train loss: 0.695082 dis train accu: 0.51175 \n",
      "epoch: 83 gen train loss: 0.686843 dis train loss: 0.694744 dis train accu: 0.511546 \n",
      "epoch: 84 gen train loss: 0.685886 dis train loss: 0.694952 dis train accu: 0.511852 \n",
      "epoch: 85 gen train loss: 0.687697 dis train loss: 0.694668 dis train accu: 0.511913 \n",
      "epoch: 86 gen train loss: 0.687278 dis train loss: 0.694975 dis train accu: 0.511648 \n",
      "epoch: 87 gen train loss: 0.686839 dis train loss: 0.693828 dis train accu: 0.511872 \n",
      "epoch: 88 gen train loss: 0.687192 dis train loss: 0.694887 dis train accu: 0.511668 \n",
      "epoch: 89 gen train loss: 0.688032 dis train loss: 0.694704 dis train accu: 0.511566 \n",
      "epoch: 90 gen train loss: 0.687417 dis train loss: 0.694482 dis train accu: 0.511831 \n",
      "epoch: 91 gen train loss: 0.689093 dis train loss: 0.694425 dis train accu: 0.511913 \n",
      "epoch: 92 gen train loss: 0.684926 dis train loss: 0.694598 dis train accu: 0.511892 \n",
      "epoch: 93 gen train loss: 0.688153 dis train loss: 0.694294 dis train accu: 0.511709 \n",
      "epoch: 94 gen train loss: 0.686947 dis train loss: 0.694095 dis train accu: 0.512137 \n",
      "epoch: 95 gen train loss: 0.685619 dis train loss: 0.694479 dis train accu: 0.512096 \n",
      "epoch: 96 gen train loss: 0.687047 dis train loss: 0.694861 dis train accu: 0.511811 \n",
      "epoch: 97 gen train loss: 0.686691 dis train loss: 0.694392 dis train accu: 0.51179 \n",
      "epoch: 98 gen train loss: 0.688365 dis train loss: 0.694159 dis train accu: 0.511974 \n",
      "epoch: 99 gen train loss: 0.686721 dis train loss: 0.694425 dis train accu: 0.511892 \n",
      "Adversarial training for ST slide 151510: \n",
      "Start adversarial training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585cf658726945d8b7a1f3c516c95e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659ef3ece7574d11bd43d57336cb3d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 gen train loss: 0.647467 dis train loss: 0.724435 dis train accu: 0.38859 \n",
      "epoch: 1 gen train loss: 0.652521 dis train loss: 0.717691 dis train accu: 0.413191 \n",
      "epoch: 2 gen train loss: 0.656106 dis train loss: 0.716252 dis train accu: 0.425513 \n",
      "epoch: 3 gen train loss: 0.664434 dis train loss: 0.712843 dis train accu: 0.435204 \n",
      "epoch: 4 gen train loss: 0.667705 dis train loss: 0.712632 dis train accu: 0.438973 \n",
      "epoch: 5 gen train loss: 0.667322 dis train loss: 0.713092 dis train accu: 0.445993 \n",
      "epoch: 6 gen train loss: 0.673688 dis train loss: 0.712343 dis train accu: 0.452185 \n",
      "epoch: 7 gen train loss: 0.682112 dis train loss: 0.710444 dis train accu: 0.458936 \n",
      "epoch: 8 gen train loss: 0.686831 dis train loss: 0.709582 dis train accu: 0.459847 \n",
      "epoch: 9 gen train loss: 0.684036 dis train loss: 0.709403 dis train accu: 0.46345 \n",
      "epoch: 10 gen train loss: 0.686308 dis train loss: 0.709224 dis train accu: 0.466101 \n",
      "epoch: 11 gen train loss: 0.683104 dis train loss: 0.708287 dis train accu: 0.468979 \n",
      "epoch: 12 gen train loss: 0.686846 dis train loss: 0.708891 dis train accu: 0.471153 \n",
      "epoch: 13 gen train loss: 0.685325 dis train loss: 0.708265 dis train accu: 0.47455 \n",
      "epoch: 14 gen train loss: 0.687449 dis train loss: 0.707571 dis train accu: 0.476144 \n",
      "epoch: 15 gen train loss: 0.690567 dis train loss: 0.707581 dis train accu: 0.476993 \n",
      "epoch: 16 gen train loss: 0.693168 dis train loss: 0.706096 dis train accu: 0.479768 \n",
      "epoch: 17 gen train loss: 0.690189 dis train loss: 0.705993 dis train accu: 0.480306 \n",
      "epoch: 18 gen train loss: 0.688932 dis train loss: 0.706078 dis train accu: 0.484034 \n",
      "epoch: 19 gen train loss: 0.688459 dis train loss: 0.705562 dis train accu: 0.487389 \n",
      "epoch: 20 gen train loss: 0.691841 dis train loss: 0.704141 dis train accu: 0.487492 \n",
      "epoch: 21 gen train loss: 0.691637 dis train loss: 0.704897 dis train accu: 0.489045 \n",
      "epoch: 22 gen train loss: 0.690327 dis train loss: 0.703907 dis train accu: 0.491095 \n",
      "epoch: 23 gen train loss: 0.691461 dis train loss: 0.704391 dis train accu: 0.490454 \n",
      "epoch: 24 gen train loss: 0.694655 dis train loss: 0.704239 dis train accu: 0.493373 \n",
      "epoch: 25 gen train loss: 0.694733 dis train loss: 0.703395 dis train accu: 0.496749 \n",
      "epoch: 26 gen train loss: 0.694438 dis train loss: 0.702484 dis train accu: 0.497619 \n",
      "epoch: 27 gen train loss: 0.695791 dis train loss: 0.70133 dis train accu: 0.49913 \n",
      "epoch: 28 gen train loss: 0.692695 dis train loss: 0.702777 dis train accu: 0.497805 \n",
      "epoch: 29 gen train loss: 0.689892 dis train loss: 0.702672 dis train accu: 0.499731 \n",
      "epoch: 30 gen train loss: 0.68888 dis train loss: 0.701769 dis train accu: 0.499524 \n",
      "epoch: 31 gen train loss: 0.690955 dis train loss: 0.700608 dis train accu: 0.502319 \n",
      "epoch: 32 gen train loss: 0.693715 dis train loss: 0.701774 dis train accu: 0.502899 \n",
      "epoch: 33 gen train loss: 0.688779 dis train loss: 0.700758 dis train accu: 0.504742 \n",
      "epoch: 34 gen train loss: 0.692514 dis train loss: 0.701324 dis train accu: 0.506047 \n",
      "epoch: 35 gen train loss: 0.691669 dis train loss: 0.699899 dis train accu: 0.50613 \n",
      "epoch: 36 gen train loss: 0.690703 dis train loss: 0.700001 dis train accu: 0.506792 \n",
      "epoch: 37 gen train loss: 0.689408 dis train loss: 0.700668 dis train accu: 0.508097 \n",
      "epoch: 38 gen train loss: 0.693177 dis train loss: 0.700128 dis train accu: 0.509112 \n",
      "epoch: 39 gen train loss: 0.690865 dis train loss: 0.69984 dis train accu: 0.509484 \n",
      "epoch: 40 gen train loss: 0.691818 dis train loss: 0.700108 dis train accu: 0.510375 \n",
      "epoch: 41 gen train loss: 0.691433 dis train loss: 0.700029 dis train accu: 0.509588 \n",
      "epoch: 42 gen train loss: 0.690429 dis train loss: 0.700287 dis train accu: 0.509174 \n",
      "epoch: 43 gen train loss: 0.690838 dis train loss: 0.699878 dis train accu: 0.512135 \n",
      "epoch: 44 gen train loss: 0.691148 dis train loss: 0.698596 dis train accu: 0.511949 \n",
      "epoch: 45 gen train loss: 0.690872 dis train loss: 0.698253 dis train accu: 0.511887 \n",
      "epoch: 46 gen train loss: 0.690702 dis train loss: 0.698241 dis train accu: 0.512984 \n",
      "epoch: 47 gen train loss: 0.690015 dis train loss: 0.698488 dis train accu: 0.51344 \n",
      "epoch: 48 gen train loss: 0.690741 dis train loss: 0.698397 dis train accu: 0.514516 \n",
      "epoch: 49 gen train loss: 0.689581 dis train loss: 0.698766 dis train accu: 0.514061 \n",
      "epoch: 50 gen train loss: 0.689112 dis train loss: 0.697793 dis train accu: 0.514765 \n",
      "epoch: 51 gen train loss: 0.689408 dis train loss: 0.696582 dis train accu: 0.515904 \n",
      "epoch: 52 gen train loss: 0.689461 dis train loss: 0.697642 dis train accu: 0.515428 \n",
      "epoch: 53 gen train loss: 0.68979 dis train loss: 0.69686 dis train accu: 0.516484 \n",
      "epoch: 54 gen train loss: 0.689933 dis train loss: 0.697093 dis train accu: 0.516318 \n",
      "epoch: 55 gen train loss: 0.689778 dis train loss: 0.697218 dis train accu: 0.516111 \n",
      "epoch: 56 gen train loss: 0.689874 dis train loss: 0.696699 dis train accu: 0.51725 \n",
      "epoch: 57 gen train loss: 0.688146 dis train loss: 0.696761 dis train accu: 0.517084 \n",
      "epoch: 58 gen train loss: 0.688118 dis train loss: 0.696583 dis train accu: 0.517623 \n",
      "epoch: 59 gen train loss: 0.689198 dis train loss: 0.696789 dis train accu: 0.517209 \n",
      "epoch: 60 gen train loss: 0.689386 dis train loss: 0.696267 dis train accu: 0.517685 \n",
      "epoch: 61 gen train loss: 0.690154 dis train loss: 0.696566 dis train accu: 0.517602 \n",
      "epoch: 62 gen train loss: 0.688787 dis train loss: 0.696157 dis train accu: 0.518078 \n",
      "epoch: 63 gen train loss: 0.689544 dis train loss: 0.69624 dis train accu: 0.518099 \n",
      "epoch: 64 gen train loss: 0.68975 dis train loss: 0.696683 dis train accu: 0.518347 \n",
      "epoch: 65 gen train loss: 0.688413 dis train loss: 0.695495 dis train accu: 0.518327 \n",
      "epoch: 66 gen train loss: 0.686062 dis train loss: 0.695941 dis train accu: 0.518513 \n",
      "epoch: 67 gen train loss: 0.688432 dis train loss: 0.695531 dis train accu: 0.518596 \n",
      "epoch: 68 gen train loss: 0.687989 dis train loss: 0.69566 dis train accu: 0.519611 \n",
      "epoch: 69 gen train loss: 0.690005 dis train loss: 0.6961 dis train accu: 0.519507 \n",
      "epoch: 70 gen train loss: 0.691107 dis train loss: 0.696196 dis train accu: 0.519176 \n",
      "epoch: 71 gen train loss: 0.689972 dis train loss: 0.695515 dis train accu: 0.518389 \n",
      "epoch: 72 gen train loss: 0.686987 dis train loss: 0.694649 dis train accu: 0.518989 \n",
      "epoch: 73 gen train loss: 0.688815 dis train loss: 0.69556 dis train accu: 0.519134 \n",
      "epoch: 74 gen train loss: 0.689554 dis train loss: 0.695122 dis train accu: 0.519176 \n",
      "epoch: 75 gen train loss: 0.689806 dis train loss: 0.695211 dis train accu: 0.519652 \n",
      "epoch: 76 gen train loss: 0.687943 dis train loss: 0.695697 dis train accu: 0.519838 \n",
      "epoch: 77 gen train loss: 0.691768 dis train loss: 0.694569 dis train accu: 0.519362 \n",
      "epoch: 78 gen train loss: 0.689539 dis train loss: 0.694767 dis train accu: 0.519611 \n",
      "epoch: 79 gen train loss: 0.688011 dis train loss: 0.695914 dis train accu: 0.519859 \n",
      "epoch: 80 gen train loss: 0.689278 dis train loss: 0.695194 dis train accu: 0.519569 \n",
      "epoch: 81 gen train loss: 0.68863 dis train loss: 0.69519 dis train accu: 0.519942 \n",
      "epoch: 82 gen train loss: 0.690751 dis train loss: 0.694991 dis train accu: 0.519673 \n",
      "epoch: 83 gen train loss: 0.689276 dis train loss: 0.694933 dis train accu: 0.520087 \n",
      "epoch: 84 gen train loss: 0.689535 dis train loss: 0.694907 dis train accu: 0.519838 \n",
      "epoch: 85 gen train loss: 0.688131 dis train loss: 0.694661 dis train accu: 0.519942 \n",
      "epoch: 86 gen train loss: 0.688349 dis train loss: 0.694575 dis train accu: 0.519631 \n",
      "epoch: 87 gen train loss: 0.687743 dis train loss: 0.694409 dis train accu: 0.519776 \n",
      "epoch: 88 gen train loss: 0.688279 dis train loss: 0.694459 dis train accu: 0.519901 \n",
      "epoch: 89 gen train loss: 0.68822 dis train loss: 0.694496 dis train accu: 0.519983 \n",
      "epoch: 90 gen train loss: 0.688815 dis train loss: 0.694281 dis train accu: 0.520108 \n",
      "epoch: 91 gen train loss: 0.689645 dis train loss: 0.694364 dis train accu: 0.520087 \n",
      "epoch: 92 gen train loss: 0.687861 dis train loss: 0.694329 dis train accu: 0.520128 \n",
      "epoch: 93 gen train loss: 0.688589 dis train loss: 0.694402 dis train accu: 0.519859 \n",
      "epoch: 94 gen train loss: 0.687901 dis train loss: 0.694222 dis train accu: 0.52017 \n",
      "epoch: 95 gen train loss: 0.687488 dis train loss: 0.694032 dis train accu: 0.520128 \n",
      "epoch: 96 gen train loss: 0.687053 dis train loss: 0.694001 dis train accu: 0.520066 \n",
      "epoch: 97 gen train loss: 0.688524 dis train loss: 0.693587 dis train accu: 0.520128 \n",
      "epoch: 98 gen train loss: 0.688949 dis train loss: 0.694116 dis train accu: 0.520356 \n",
      "epoch: 99 gen train loss: 0.689035 dis train loss: 0.693813 dis train accu: 0.520273 \n",
      "Adversarial training for ST slide 151671: \n",
      "Start adversarial training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2a1bf4d36147b995b4b220eb6a0e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de69237190a46e5b3397302f2a78581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 gen train loss: 0.649842 dis train loss: 0.721493 dis train accu: 0.405102 \n",
      "epoch: 1 gen train loss: 0.654955 dis train loss: 0.718384 dis train accu: 0.424502 \n",
      "epoch: 2 gen train loss: 0.659443 dis train loss: 0.715116 dis train accu: 0.440661 \n",
      "epoch: 3 gen train loss: 0.670298 dis train loss: 0.713756 dis train accu: 0.448215 \n",
      "epoch: 4 gen train loss: 0.671628 dis train loss: 0.711753 dis train accu: 0.459974 \n",
      "epoch: 5 gen train loss: 0.667552 dis train loss: 0.711325 dis train accu: 0.463499 \n",
      "epoch: 6 gen train loss: 0.676726 dis train loss: 0.709235 dis train accu: 0.4709 \n",
      "epoch: 7 gen train loss: 0.685112 dis train loss: 0.710186 dis train accu: 0.475126 \n",
      "epoch: 8 gen train loss: 0.680999 dis train loss: 0.70929 dis train accu: 0.482198 \n",
      "epoch: 9 gen train loss: 0.680756 dis train loss: 0.709343 dis train accu: 0.481804 \n",
      "epoch: 10 gen train loss: 0.684632 dis train loss: 0.709927 dis train accu: 0.488789 \n",
      "epoch: 11 gen train loss: 0.684882 dis train loss: 0.706007 dis train accu: 0.489972 \n",
      "epoch: 12 gen train loss: 0.690719 dis train loss: 0.70586 dis train accu: 0.493081 \n",
      "epoch: 13 gen train loss: 0.687073 dis train loss: 0.708949 dis train accu: 0.494592 \n",
      "epoch: 14 gen train loss: 0.686857 dis train loss: 0.705438 dis train accu: 0.498642 \n",
      "epoch: 15 gen train loss: 0.691937 dis train loss: 0.705828 dis train accu: 0.501248 \n",
      "epoch: 16 gen train loss: 0.690443 dis train loss: 0.704578 dis train accu: 0.50162 \n",
      "epoch: 17 gen train loss: 0.693495 dis train loss: 0.704939 dis train accu: 0.505365 \n",
      "epoch: 18 gen train loss: 0.695084 dis train loss: 0.70539 dis train accu: 0.507773 \n",
      "epoch: 19 gen train loss: 0.692641 dis train loss: 0.705156 dis train accu: 0.509525 \n",
      "epoch: 20 gen train loss: 0.690934 dis train loss: 0.705802 dis train accu: 0.510904 \n",
      "epoch: 21 gen train loss: 0.689886 dis train loss: 0.704568 dis train accu: 0.512678 \n",
      "epoch: 22 gen train loss: 0.694337 dis train loss: 0.703835 dis train accu: 0.517276 \n",
      "epoch: 23 gen train loss: 0.687431 dis train loss: 0.704042 dis train accu: 0.519707 \n",
      "epoch: 24 gen train loss: 0.686568 dis train loss: 0.702949 dis train accu: 0.519269 \n",
      "epoch: 25 gen train loss: 0.69126 dis train loss: 0.704058 dis train accu: 0.521305 \n",
      "epoch: 26 gen train loss: 0.689369 dis train loss: 0.703757 dis train accu: 0.52113 \n",
      "epoch: 27 gen train loss: 0.691379 dis train loss: 0.703298 dis train accu: 0.524152 \n",
      "epoch: 28 gen train loss: 0.68869 dis train loss: 0.702113 dis train accu: 0.525969 \n",
      "epoch: 29 gen train loss: 0.690581 dis train loss: 0.700243 dis train accu: 0.528049 \n",
      "epoch: 30 gen train loss: 0.69427 dis train loss: 0.702365 dis train accu: 0.528159 \n",
      "epoch: 31 gen train loss: 0.691679 dis train loss: 0.702111 dis train accu: 0.528487 \n",
      "epoch: 32 gen train loss: 0.693449 dis train loss: 0.702577 dis train accu: 0.529735 \n",
      "epoch: 33 gen train loss: 0.69559 dis train loss: 0.700766 dis train accu: 0.531968 \n",
      "epoch: 34 gen train loss: 0.691743 dis train loss: 0.701175 dis train accu: 0.532253 \n",
      "epoch: 35 gen train loss: 0.69134 dis train loss: 0.700321 dis train accu: 0.53372 \n",
      "epoch: 36 gen train loss: 0.686166 dis train loss: 0.701018 dis train accu: 0.533567 \n",
      "epoch: 37 gen train loss: 0.690956 dis train loss: 0.699923 dis train accu: 0.534968 \n",
      "epoch: 38 gen train loss: 0.688789 dis train loss: 0.700294 dis train accu: 0.536895 \n",
      "epoch: 39 gen train loss: 0.693068 dis train loss: 0.699797 dis train accu: 0.535231 \n",
      "epoch: 40 gen train loss: 0.690869 dis train loss: 0.699782 dis train accu: 0.538975 \n",
      "epoch: 41 gen train loss: 0.694847 dis train loss: 0.699843 dis train accu: 0.539764 \n",
      "epoch: 42 gen train loss: 0.690456 dis train loss: 0.699918 dis train accu: 0.539676 \n",
      "epoch: 43 gen train loss: 0.689853 dis train loss: 0.697203 dis train accu: 0.540004 \n",
      "epoch: 44 gen train loss: 0.689964 dis train loss: 0.699667 dis train accu: 0.541034 \n",
      "epoch: 45 gen train loss: 0.690485 dis train loss: 0.69946 dis train accu: 0.541844 \n",
      "epoch: 46 gen train loss: 0.690608 dis train loss: 0.699049 dis train accu: 0.541296 \n",
      "epoch: 47 gen train loss: 0.687603 dis train loss: 0.698232 dis train accu: 0.544033 \n",
      "epoch: 48 gen train loss: 0.687631 dis train loss: 0.698427 dis train accu: 0.544537 \n",
      "epoch: 49 gen train loss: 0.692172 dis train loss: 0.698625 dis train accu: 0.544274 \n",
      "epoch: 50 gen train loss: 0.690098 dis train loss: 0.697906 dis train accu: 0.544011 \n",
      "epoch: 51 gen train loss: 0.688834 dis train loss: 0.697431 dis train accu: 0.544471 \n",
      "epoch: 52 gen train loss: 0.687219 dis train loss: 0.698068 dis train accu: 0.545588 \n",
      "epoch: 53 gen train loss: 0.689861 dis train loss: 0.697297 dis train accu: 0.545457 \n",
      "epoch: 54 gen train loss: 0.68803 dis train loss: 0.697835 dis train accu: 0.545566 \n",
      "epoch: 55 gen train loss: 0.689849 dis train loss: 0.696119 dis train accu: 0.546617 \n",
      "epoch: 56 gen train loss: 0.691025 dis train loss: 0.697487 dis train accu: 0.546639 \n",
      "epoch: 57 gen train loss: 0.688683 dis train loss: 0.696717 dis train accu: 0.548084 \n",
      "epoch: 58 gen train loss: 0.689875 dis train loss: 0.6977 dis train accu: 0.54631 \n",
      "epoch: 59 gen train loss: 0.689298 dis train loss: 0.697105 dis train accu: 0.54804 \n",
      "epoch: 60 gen train loss: 0.690299 dis train loss: 0.697146 dis train accu: 0.547646 \n",
      "epoch: 61 gen train loss: 0.688587 dis train loss: 0.696793 dis train accu: 0.547843 \n",
      "epoch: 62 gen train loss: 0.688778 dis train loss: 0.696216 dis train accu: 0.548982 \n",
      "epoch: 63 gen train loss: 0.688799 dis train loss: 0.696655 dis train accu: 0.547734 \n",
      "epoch: 64 gen train loss: 0.687367 dis train loss: 0.696663 dis train accu: 0.54804 \n",
      "epoch: 65 gen train loss: 0.692529 dis train loss: 0.696362 dis train accu: 0.548631 \n",
      "epoch: 66 gen train loss: 0.688451 dis train loss: 0.696305 dis train accu: 0.548478 \n",
      "epoch: 67 gen train loss: 0.685357 dis train loss: 0.696268 dis train accu: 0.549135 \n",
      "epoch: 68 gen train loss: 0.688344 dis train loss: 0.696025 dis train accu: 0.548741 \n",
      "epoch: 69 gen train loss: 0.689118 dis train loss: 0.696531 dis train accu: 0.549069 \n",
      "epoch: 70 gen train loss: 0.68603 dis train loss: 0.6956 dis train accu: 0.549179 \n",
      "epoch: 71 gen train loss: 0.687352 dis train loss: 0.695409 dis train accu: 0.549507 \n",
      "epoch: 72 gen train loss: 0.688617 dis train loss: 0.695545 dis train accu: 0.550099 \n",
      "epoch: 73 gen train loss: 0.687047 dis train loss: 0.694927 dis train accu: 0.549179 \n",
      "epoch: 74 gen train loss: 0.687297 dis train loss: 0.69539 dis train accu: 0.549332 \n",
      "epoch: 75 gen train loss: 0.689323 dis train loss: 0.696034 dis train accu: 0.549573 \n",
      "epoch: 76 gen train loss: 0.687805 dis train loss: 0.695946 dis train accu: 0.549726 \n",
      "epoch: 77 gen train loss: 0.688625 dis train loss: 0.695198 dis train accu: 0.549595 \n",
      "epoch: 78 gen train loss: 0.689057 dis train loss: 0.695078 dis train accu: 0.549551 \n",
      "epoch: 79 gen train loss: 0.686195 dis train loss: 0.695252 dis train accu: 0.549617 \n",
      "epoch: 80 gen train loss: 0.688916 dis train loss: 0.694888 dis train accu: 0.550011 \n",
      "epoch: 81 gen train loss: 0.688509 dis train loss: 0.695244 dis train accu: 0.549989 \n",
      "epoch: 82 gen train loss: 0.689104 dis train loss: 0.694387 dis train accu: 0.550296 \n",
      "epoch: 83 gen train loss: 0.688491 dis train loss: 0.6955 dis train accu: 0.549923 \n",
      "epoch: 84 gen train loss: 0.689435 dis train loss: 0.694443 dis train accu: 0.549923 \n",
      "epoch: 85 gen train loss: 0.689533 dis train loss: 0.694962 dis train accu: 0.549858 \n",
      "epoch: 86 gen train loss: 0.687308 dis train loss: 0.694939 dis train accu: 0.550077 \n",
      "epoch: 87 gen train loss: 0.689266 dis train loss: 0.694459 dis train accu: 0.550055 \n",
      "epoch: 88 gen train loss: 0.68549 dis train loss: 0.694464 dis train accu: 0.55012 \n",
      "epoch: 89 gen train loss: 0.687803 dis train loss: 0.694653 dis train accu: 0.550033 \n",
      "epoch: 90 gen train loss: 0.6877 dis train loss: 0.694045 dis train accu: 0.550186 \n",
      "epoch: 91 gen train loss: 0.686734 dis train loss: 0.694443 dis train accu: 0.550055 \n",
      "epoch: 92 gen train loss: 0.686338 dis train loss: 0.694819 dis train accu: 0.549858 \n",
      "epoch: 93 gen train loss: 0.687217 dis train loss: 0.694448 dis train accu: 0.55012 \n",
      "epoch: 94 gen train loss: 0.686945 dis train loss: 0.694808 dis train accu: 0.550033 \n",
      "epoch: 95 gen train loss: 0.687841 dis train loss: 0.694558 dis train accu: 0.549945 \n",
      "epoch: 96 gen train loss: 0.6887 dis train loss: 0.694326 dis train accu: 0.550055 \n",
      "epoch: 97 gen train loss: 0.688948 dis train loss: 0.693994 dis train accu: 0.550142 \n",
      "epoch: 98 gen train loss: 0.688945 dis train loss: 0.694064 dis train accu: 0.550077 \n",
      "epoch: 99 gen train loss: 0.689159 dis train loss: 0.694269 dis train accu: 0.549945 \n",
      "Adversarial training for ST slide 151508: \n",
      "Start adversarial training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff61942b91b4b39a5eacfe35946f498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4178c018caa6417487b326ff226ca5b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 gen train loss: 0.649123 dis train loss: 0.722318 dis train accu: 0.398172 \n",
      "epoch: 1 gen train loss: 0.652893 dis train loss: 0.718067 dis train accu: 0.424256 \n",
      "epoch: 2 gen train loss: 0.661271 dis train loss: 0.713233 dis train accu: 0.432696 \n",
      "epoch: 3 gen train loss: 0.662851 dis train loss: 0.713406 dis train accu: 0.441624 \n",
      "epoch: 4 gen train loss: 0.669672 dis train loss: 0.711986 dis train accu: 0.451807 \n",
      "epoch: 5 gen train loss: 0.669902 dis train loss: 0.711126 dis train accu: 0.455995 \n",
      "epoch: 6 gen train loss: 0.67315 dis train loss: 0.710448 dis train accu: 0.460247 \n",
      "epoch: 7 gen train loss: 0.676827 dis train loss: 0.709294 dis train accu: 0.464881 \n",
      "epoch: 8 gen train loss: 0.681585 dis train loss: 0.708915 dis train accu: 0.470259 \n",
      "epoch: 9 gen train loss: 0.682028 dis train loss: 0.709502 dis train accu: 0.474298 \n",
      "epoch: 10 gen train loss: 0.684287 dis train loss: 0.707879 dis train accu: 0.476807 \n",
      "epoch: 11 gen train loss: 0.688081 dis train loss: 0.70711 dis train accu: 0.478635 \n",
      "epoch: 12 gen train loss: 0.684663 dis train loss: 0.706551 dis train accu: 0.481888 \n",
      "epoch: 13 gen train loss: 0.688324 dis train loss: 0.707112 dis train accu: 0.485629 \n",
      "epoch: 14 gen train loss: 0.690449 dis train loss: 0.706485 dis train accu: 0.486373 \n",
      "epoch: 15 gen train loss: 0.686339 dis train loss: 0.706409 dis train accu: 0.490689 \n",
      "epoch: 16 gen train loss: 0.691608 dis train loss: 0.706439 dis train accu: 0.490476 \n",
      "epoch: 17 gen train loss: 0.692102 dis train loss: 0.706184 dis train accu: 0.493176 \n",
      "epoch: 18 gen train loss: 0.691683 dis train loss: 0.705193 dis train accu: 0.495685 \n",
      "epoch: 19 gen train loss: 0.693882 dis train loss: 0.703442 dis train accu: 0.498852 \n",
      "epoch: 20 gen train loss: 0.689886 dis train loss: 0.705409 dis train accu: 0.500935 \n",
      "epoch: 21 gen train loss: 0.693562 dis train loss: 0.703404 dis train accu: 0.50119 \n",
      "epoch: 22 gen train loss: 0.688681 dis train loss: 0.703579 dis train accu: 0.503359 \n",
      "epoch: 23 gen train loss: 0.687537 dis train loss: 0.703205 dis train accu: 0.502317 \n",
      "epoch: 24 gen train loss: 0.692077 dis train loss: 0.703125 dis train accu: 0.505761 \n",
      "epoch: 25 gen train loss: 0.692702 dis train loss: 0.703764 dis train accu: 0.507993 \n",
      "epoch: 26 gen train loss: 0.687243 dis train loss: 0.703298 dis train accu: 0.508206 \n",
      "epoch: 27 gen train loss: 0.68801 dis train loss: 0.702033 dis train accu: 0.510183 \n",
      "epoch: 28 gen train loss: 0.692483 dis train loss: 0.701349 dis train accu: 0.511522 \n",
      "epoch: 29 gen train loss: 0.690513 dis train loss: 0.702796 dis train accu: 0.511118 \n",
      "epoch: 30 gen train loss: 0.68887 dis train loss: 0.701542 dis train accu: 0.515051 \n",
      "epoch: 31 gen train loss: 0.693009 dis train loss: 0.700012 dis train accu: 0.513946 \n",
      "epoch: 32 gen train loss: 0.687773 dis train loss: 0.700692 dis train accu: 0.516305 \n",
      "epoch: 33 gen train loss: 0.688099 dis train loss: 0.70136 dis train accu: 0.517177 \n",
      "epoch: 34 gen train loss: 0.690642 dis train loss: 0.700534 dis train accu: 0.516794 \n",
      "epoch: 35 gen train loss: 0.689918 dis train loss: 0.700024 dis train accu: 0.518984 \n",
      "epoch: 36 gen train loss: 0.686802 dis train loss: 0.699194 dis train accu: 0.51977 \n",
      "epoch: 37 gen train loss: 0.690912 dis train loss: 0.699087 dis train accu: 0.5223 \n",
      "epoch: 38 gen train loss: 0.687667 dis train loss: 0.698925 dis train accu: 0.521365 \n",
      "epoch: 39 gen train loss: 0.689996 dis train loss: 0.699202 dis train accu: 0.521173 \n",
      "epoch: 40 gen train loss: 0.687937 dis train loss: 0.698848 dis train accu: 0.521344 \n",
      "epoch: 41 gen train loss: 0.690093 dis train loss: 0.698254 dis train accu: 0.522109 \n",
      "epoch: 42 gen train loss: 0.688382 dis train loss: 0.698625 dis train accu: 0.523831 \n",
      "epoch: 43 gen train loss: 0.687468 dis train loss: 0.69883 dis train accu: 0.524171 \n",
      "epoch: 44 gen train loss: 0.689222 dis train loss: 0.699031 dis train accu: 0.524192 \n",
      "epoch: 45 gen train loss: 0.689855 dis train loss: 0.698377 dis train accu: 0.524362 \n",
      "epoch: 46 gen train loss: 0.684625 dis train loss: 0.698416 dis train accu: 0.526042 \n",
      "epoch: 47 gen train loss: 0.687234 dis train loss: 0.697982 dis train accu: 0.526361 \n",
      "epoch: 48 gen train loss: 0.689404 dis train loss: 0.698743 dis train accu: 0.526127 \n",
      "epoch: 49 gen train loss: 0.689798 dis train loss: 0.697446 dis train accu: 0.527742 \n",
      "epoch: 50 gen train loss: 0.690992 dis train loss: 0.698475 dis train accu: 0.528401 \n",
      "epoch: 51 gen train loss: 0.688189 dis train loss: 0.697491 dis train accu: 0.528741 \n",
      "epoch: 52 gen train loss: 0.691113 dis train loss: 0.697174 dis train accu: 0.529443 \n",
      "epoch: 53 gen train loss: 0.688489 dis train loss: 0.697557 dis train accu: 0.52889 \n",
      "epoch: 54 gen train loss: 0.685292 dis train loss: 0.697246 dis train accu: 0.528614 \n",
      "epoch: 55 gen train loss: 0.684078 dis train loss: 0.696945 dis train accu: 0.530187 \n",
      "epoch: 56 gen train loss: 0.686964 dis train loss: 0.696524 dis train accu: 0.531101 \n",
      "epoch: 57 gen train loss: 0.68777 dis train loss: 0.696544 dis train accu: 0.529634 \n",
      "epoch: 58 gen train loss: 0.689861 dis train loss: 0.696231 dis train accu: 0.530293 \n",
      "epoch: 59 gen train loss: 0.686415 dis train loss: 0.697158 dis train accu: 0.53125 \n",
      "epoch: 60 gen train loss: 0.689494 dis train loss: 0.696597 dis train accu: 0.530995 \n",
      "epoch: 61 gen train loss: 0.685298 dis train loss: 0.695394 dis train accu: 0.531505 \n",
      "epoch: 62 gen train loss: 0.688766 dis train loss: 0.695702 dis train accu: 0.531888 \n",
      "epoch: 63 gen train loss: 0.687696 dis train loss: 0.695467 dis train accu: 0.531633 \n",
      "epoch: 64 gen train loss: 0.686414 dis train loss: 0.696563 dis train accu: 0.532228 \n",
      "epoch: 65 gen train loss: 0.689963 dis train loss: 0.695578 dis train accu: 0.532079 \n",
      "epoch: 66 gen train loss: 0.686866 dis train loss: 0.695864 dis train accu: 0.532781 \n",
      "epoch: 67 gen train loss: 0.687366 dis train loss: 0.695061 dis train accu: 0.532462 \n",
      "epoch: 68 gen train loss: 0.690672 dis train loss: 0.696257 dis train accu: 0.532143 \n",
      "epoch: 69 gen train loss: 0.688945 dis train loss: 0.695784 dis train accu: 0.532759 \n",
      "epoch: 70 gen train loss: 0.689131 dis train loss: 0.695507 dis train accu: 0.532972 \n",
      "epoch: 71 gen train loss: 0.689138 dis train loss: 0.694979 dis train accu: 0.532844 \n",
      "epoch: 72 gen train loss: 0.687491 dis train loss: 0.694593 dis train accu: 0.533333 \n",
      "epoch: 73 gen train loss: 0.688383 dis train loss: 0.695649 dis train accu: 0.533206 \n",
      "epoch: 74 gen train loss: 0.68888 dis train loss: 0.694343 dis train accu: 0.533142 \n",
      "epoch: 75 gen train loss: 0.686735 dis train loss: 0.69549 dis train accu: 0.533418 \n",
      "epoch: 76 gen train loss: 0.689109 dis train loss: 0.694718 dis train accu: 0.533546 \n",
      "epoch: 77 gen train loss: 0.686985 dis train loss: 0.695298 dis train accu: 0.533312 \n",
      "epoch: 78 gen train loss: 0.685851 dis train loss: 0.694472 dis train accu: 0.533652 \n",
      "epoch: 79 gen train loss: 0.685116 dis train loss: 0.69507 dis train accu: 0.533418 \n",
      "epoch: 80 gen train loss: 0.688041 dis train loss: 0.694468 dis train accu: 0.533759 \n",
      "epoch: 81 gen train loss: 0.687892 dis train loss: 0.694503 dis train accu: 0.53395 \n",
      "epoch: 82 gen train loss: 0.689081 dis train loss: 0.694908 dis train accu: 0.53361 \n",
      "epoch: 83 gen train loss: 0.685111 dis train loss: 0.693854 dis train accu: 0.533886 \n",
      "epoch: 84 gen train loss: 0.687944 dis train loss: 0.694512 dis train accu: 0.534014 \n",
      "epoch: 85 gen train loss: 0.686999 dis train loss: 0.694629 dis train accu: 0.533418 \n",
      "epoch: 86 gen train loss: 0.686034 dis train loss: 0.694355 dis train accu: 0.533567 \n",
      "epoch: 87 gen train loss: 0.687457 dis train loss: 0.694712 dis train accu: 0.534077 \n",
      "epoch: 88 gen train loss: 0.685498 dis train loss: 0.694227 dis train accu: 0.533801 \n",
      "epoch: 89 gen train loss: 0.686985 dis train loss: 0.694285 dis train accu: 0.533971 \n",
      "epoch: 90 gen train loss: 0.68712 dis train loss: 0.694019 dis train accu: 0.534035 \n",
      "epoch: 91 gen train loss: 0.685635 dis train loss: 0.694213 dis train accu: 0.534035 \n",
      "epoch: 92 gen train loss: 0.686268 dis train loss: 0.694285 dis train accu: 0.534014 \n",
      "epoch: 93 gen train loss: 0.68664 dis train loss: 0.693519 dis train accu: 0.533971 \n",
      "epoch: 94 gen train loss: 0.68787 dis train loss: 0.693474 dis train accu: 0.533971 \n",
      "epoch: 95 gen train loss: 0.68646 dis train loss: 0.694708 dis train accu: 0.533865 \n",
      "epoch: 96 gen train loss: 0.686185 dis train loss: 0.69401 dis train accu: 0.533759 \n",
      "epoch: 97 gen train loss: 0.687258 dis train loss: 0.69376 dis train accu: 0.534035 \n",
      "epoch: 98 gen train loss: 0.685833 dis train loss: 0.693696 dis train accu: 0.534099 \n",
      "epoch: 99 gen train loss: 0.687324 dis train loss: 0.693687 dis train accu: 0.53412 \n",
      "Adversarial training for ST slide 151670: \n",
      "Start adversarial training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78bcc8e034694d34919f7959337cb097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3bc73762f64bbcb55a46f471d8b78a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 gen train loss: 0.646722 dis train loss: 0.722689 dis train accu: 0.396532 \n",
      "epoch: 1 gen train loss: 0.658346 dis train loss: 0.719287 dis train accu: 0.41131 \n",
      "epoch: 2 gen train loss: 0.66193 dis train loss: 0.713429 dis train accu: 0.42622 \n",
      "epoch: 3 gen train loss: 0.66165 dis train loss: 0.713012 dis train accu: 0.438037 \n",
      "epoch: 4 gen train loss: 0.66686 dis train loss: 0.712996 dis train accu: 0.444519 \n",
      "epoch: 5 gen train loss: 0.668344 dis train loss: 0.712764 dis train accu: 0.451214 \n",
      "epoch: 6 gen train loss: 0.668638 dis train loss: 0.710394 dis train accu: 0.457402 \n",
      "epoch: 7 gen train loss: 0.669699 dis train loss: 0.711926 dis train accu: 0.457215 \n",
      "epoch: 8 gen train loss: 0.677337 dis train loss: 0.709809 dis train accu: 0.466284 \n",
      "epoch: 9 gen train loss: 0.681205 dis train loss: 0.710903 dis train accu: 0.465431 \n",
      "epoch: 10 gen train loss: 0.679178 dis train loss: 0.709789 dis train accu: 0.466178 \n",
      "epoch: 11 gen train loss: 0.680067 dis train loss: 0.710627 dis train accu: 0.475247 \n",
      "epoch: 12 gen train loss: 0.681926 dis train loss: 0.708969 dis train accu: 0.475994 \n",
      "epoch: 13 gen train loss: 0.681029 dis train loss: 0.708189 dis train accu: 0.479301 \n",
      "epoch: 14 gen train loss: 0.68283 dis train loss: 0.709139 dis train accu: 0.479008 \n",
      "epoch: 15 gen train loss: 0.686626 dis train loss: 0.707357 dis train accu: 0.481462 \n",
      "epoch: 16 gen train loss: 0.683875 dis train loss: 0.707567 dis train accu: 0.483916 \n",
      "epoch: 17 gen train loss: 0.689207 dis train loss: 0.707535 dis train accu: 0.485836 \n",
      "epoch: 18 gen train loss: 0.690832 dis train loss: 0.705551 dis train accu: 0.48725 \n",
      "epoch: 19 gen train loss: 0.688656 dis train loss: 0.706672 dis train accu: 0.48733 \n",
      "epoch: 20 gen train loss: 0.689391 dis train loss: 0.704379 dis train accu: 0.490878 \n",
      "epoch: 21 gen train loss: 0.686892 dis train loss: 0.704694 dis train accu: 0.495599 \n",
      "epoch: 22 gen train loss: 0.695437 dis train loss: 0.704472 dis train accu: 0.493518 \n",
      "epoch: 23 gen train loss: 0.692041 dis train loss: 0.705007 dis train accu: 0.496879 \n",
      "epoch: 24 gen train loss: 0.691971 dis train loss: 0.70666 dis train accu: 0.495519 \n",
      "epoch: 25 gen train loss: 0.689174 dis train loss: 0.704887 dis train accu: 0.500107 \n",
      "epoch: 26 gen train loss: 0.691044 dis train loss: 0.705347 dis train accu: 0.499787 \n",
      "epoch: 27 gen train loss: 0.69028 dis train loss: 0.703849 dis train accu: 0.502427 \n",
      "epoch: 28 gen train loss: 0.694817 dis train loss: 0.703765 dis train accu: 0.501921 \n",
      "epoch: 29 gen train loss: 0.691838 dis train loss: 0.703298 dis train accu: 0.504401 \n",
      "epoch: 30 gen train loss: 0.687523 dis train loss: 0.701673 dis train accu: 0.505015 \n",
      "epoch: 31 gen train loss: 0.693149 dis train loss: 0.701601 dis train accu: 0.507015 \n",
      "epoch: 32 gen train loss: 0.689371 dis train loss: 0.702558 dis train accu: 0.505175 \n",
      "epoch: 33 gen train loss: 0.686029 dis train loss: 0.702341 dis train accu: 0.506935 \n",
      "epoch: 34 gen train loss: 0.690512 dis train loss: 0.702436 dis train accu: 0.510003 \n",
      "epoch: 35 gen train loss: 0.689666 dis train loss: 0.702379 dis train accu: 0.513737 \n",
      "epoch: 36 gen train loss: 0.691845 dis train loss: 0.70258 dis train accu: 0.509843 \n",
      "epoch: 37 gen train loss: 0.689055 dis train loss: 0.701123 dis train accu: 0.51227 \n",
      "epoch: 38 gen train loss: 0.688614 dis train loss: 0.70172 dis train accu: 0.513444 \n",
      "epoch: 39 gen train loss: 0.688708 dis train loss: 0.701747 dis train accu: 0.513123 \n",
      "epoch: 40 gen train loss: 0.688278 dis train loss: 0.701678 dis train accu: 0.515791 \n",
      "epoch: 41 gen train loss: 0.689074 dis train loss: 0.701497 dis train accu: 0.515898 \n",
      "epoch: 42 gen train loss: 0.690084 dis train loss: 0.701331 dis train accu: 0.516858 \n",
      "epoch: 43 gen train loss: 0.688264 dis train loss: 0.700141 dis train accu: 0.517818 \n",
      "epoch: 44 gen train loss: 0.68956 dis train loss: 0.701077 dis train accu: 0.517685 \n",
      "epoch: 45 gen train loss: 0.686671 dis train loss: 0.700977 dis train accu: 0.519072 \n",
      "epoch: 46 gen train loss: 0.688736 dis train loss: 0.699272 dis train accu: 0.519152 \n",
      "epoch: 47 gen train loss: 0.690787 dis train loss: 0.699955 dis train accu: 0.519338 \n",
      "epoch: 48 gen train loss: 0.689574 dis train loss: 0.700036 dis train accu: 0.520165 \n",
      "epoch: 49 gen train loss: 0.689072 dis train loss: 0.699304 dis train accu: 0.521126 \n",
      "epoch: 50 gen train loss: 0.686045 dis train loss: 0.699437 dis train accu: 0.52358 \n",
      "epoch: 51 gen train loss: 0.689609 dis train loss: 0.699274 dis train accu: 0.520966 \n",
      "epoch: 52 gen train loss: 0.689883 dis train loss: 0.698466 dis train accu: 0.522059 \n",
      "epoch: 53 gen train loss: 0.690417 dis train loss: 0.699548 dis train accu: 0.523686 \n",
      "epoch: 54 gen train loss: 0.690592 dis train loss: 0.698457 dis train accu: 0.522779 \n",
      "epoch: 55 gen train loss: 0.686836 dis train loss: 0.698991 dis train accu: 0.524807 \n",
      "epoch: 56 gen train loss: 0.687755 dis train loss: 0.697632 dis train accu: 0.52446 \n",
      "epoch: 57 gen train loss: 0.687815 dis train loss: 0.698287 dis train accu: 0.52606 \n",
      "epoch: 58 gen train loss: 0.690835 dis train loss: 0.698421 dis train accu: 0.525073 \n",
      "epoch: 59 gen train loss: 0.689048 dis train loss: 0.698488 dis train accu: 0.5243 \n",
      "epoch: 60 gen train loss: 0.688182 dis train loss: 0.698553 dis train accu: 0.526274 \n",
      "epoch: 61 gen train loss: 0.68915 dis train loss: 0.697549 dis train accu: 0.526274 \n",
      "epoch: 62 gen train loss: 0.688 dis train loss: 0.698345 dis train accu: 0.526941 \n",
      "epoch: 63 gen train loss: 0.688123 dis train loss: 0.697752 dis train accu: 0.527181 \n",
      "epoch: 64 gen train loss: 0.688254 dis train loss: 0.698253 dis train accu: 0.526754 \n",
      "epoch: 65 gen train loss: 0.691176 dis train loss: 0.69644 dis train accu: 0.528861 \n",
      "epoch: 66 gen train loss: 0.691191 dis train loss: 0.697304 dis train accu: 0.527714 \n",
      "epoch: 67 gen train loss: 0.688482 dis train loss: 0.697385 dis train accu: 0.528568 \n",
      "epoch: 68 gen train loss: 0.691499 dis train loss: 0.696787 dis train accu: 0.528461 \n",
      "epoch: 69 gen train loss: 0.687573 dis train loss: 0.697221 dis train accu: 0.530275 \n",
      "epoch: 70 gen train loss: 0.6891 dis train loss: 0.696204 dis train accu: 0.529741 \n",
      "epoch: 71 gen train loss: 0.687694 dis train loss: 0.696514 dis train accu: 0.529288 \n",
      "epoch: 72 gen train loss: 0.688475 dis train loss: 0.696431 dis train accu: 0.529234 \n",
      "epoch: 73 gen train loss: 0.688193 dis train loss: 0.696115 dis train accu: 0.530248 \n",
      "epoch: 74 gen train loss: 0.689124 dis train loss: 0.696403 dis train accu: 0.530408 \n",
      "epoch: 75 gen train loss: 0.687623 dis train loss: 0.696496 dis train accu: 0.530461 \n",
      "epoch: 76 gen train loss: 0.687579 dis train loss: 0.696352 dis train accu: 0.530355 \n",
      "epoch: 77 gen train loss: 0.69025 dis train loss: 0.695945 dis train accu: 0.531182 \n",
      "epoch: 78 gen train loss: 0.689803 dis train loss: 0.695911 dis train accu: 0.530782 \n",
      "epoch: 79 gen train loss: 0.68787 dis train loss: 0.696685 dis train accu: 0.530088 \n",
      "epoch: 80 gen train loss: 0.686391 dis train loss: 0.695521 dis train accu: 0.531608 \n",
      "epoch: 81 gen train loss: 0.68651 dis train loss: 0.696286 dis train accu: 0.532089 \n",
      "epoch: 82 gen train loss: 0.686762 dis train loss: 0.695824 dis train accu: 0.531528 \n",
      "epoch: 83 gen train loss: 0.691029 dis train loss: 0.695767 dis train accu: 0.532569 \n",
      "epoch: 84 gen train loss: 0.687285 dis train loss: 0.695328 dis train accu: 0.531555 \n",
      "epoch: 85 gen train loss: 0.685673 dis train loss: 0.695926 dis train accu: 0.531182 \n",
      "epoch: 86 gen train loss: 0.687724 dis train loss: 0.695422 dis train accu: 0.532355 \n",
      "epoch: 87 gen train loss: 0.685585 dis train loss: 0.696171 dis train accu: 0.532222 \n",
      "epoch: 88 gen train loss: 0.687334 dis train loss: 0.695309 dis train accu: 0.532302 \n",
      "epoch: 89 gen train loss: 0.687838 dis train loss: 0.694909 dis train accu: 0.531955 \n",
      "epoch: 90 gen train loss: 0.687336 dis train loss: 0.695 dis train accu: 0.531955 \n",
      "epoch: 91 gen train loss: 0.68764 dis train loss: 0.694812 dis train accu: 0.532995 \n",
      "epoch: 92 gen train loss: 0.686133 dis train loss: 0.694455 dis train accu: 0.532595 \n",
      "epoch: 93 gen train loss: 0.685595 dis train loss: 0.694944 dis train accu: 0.531902 \n",
      "epoch: 94 gen train loss: 0.682958 dis train loss: 0.694948 dis train accu: 0.532942 \n",
      "epoch: 95 gen train loss: 0.68565 dis train loss: 0.694807 dis train accu: 0.532329 \n",
      "epoch: 96 gen train loss: 0.689506 dis train loss: 0.69404 dis train accu: 0.532595 \n",
      "epoch: 97 gen train loss: 0.685978 dis train loss: 0.694733 dis train accu: 0.532435 \n",
      "epoch: 98 gen train loss: 0.685162 dis train loss: 0.694308 dis train accu: 0.532649 \n",
      "epoch: 99 gen train loss: 0.686198 dis train loss: 0.694737 dis train accu: 0.532862 \n",
      "Adversarial training for ST slide 151507: \n",
      "Start adversarial training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e063e578d748789ddecf3571125603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f46bcfd9c594872971a135fb83cfa41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 gen train loss: 0.644113 dis train loss: 0.721091 dis train accu: 0.405795 \n",
      "epoch: 1 gen train loss: 0.658612 dis train loss: 0.714705 dis train accu: 0.425189 \n",
      "epoch: 2 gen train loss: 0.660868 dis train loss: 0.713151 dis train accu: 0.43987 \n",
      "epoch: 3 gen train loss: 0.663533 dis train loss: 0.711854 dis train accu: 0.447481 \n",
      "epoch: 4 gen train loss: 0.66581 dis train loss: 0.711649 dis train accu: 0.456195 \n",
      "epoch: 5 gen train loss: 0.669876 dis train loss: 0.711011 dis train accu: 0.461016 \n",
      "epoch: 6 gen train loss: 0.670035 dis train loss: 0.710135 dis train accu: 0.467978 \n",
      "epoch: 7 gen train loss: 0.684427 dis train loss: 0.709596 dis train accu: 0.473881 \n",
      "epoch: 8 gen train loss: 0.6818 dis train loss: 0.710137 dis train accu: 0.476562 \n",
      "epoch: 9 gen train loss: 0.682576 dis train loss: 0.707761 dis train accu: 0.477038 \n",
      "epoch: 10 gen train loss: 0.685383 dis train loss: 0.7088 dis train accu: 0.48173 \n",
      "epoch: 11 gen train loss: 0.690997 dis train loss: 0.707874 dis train accu: 0.486508 \n",
      "epoch: 12 gen train loss: 0.68558 dis train loss: 0.708314 dis train accu: 0.489341 \n",
      "epoch: 13 gen train loss: 0.687577 dis train loss: 0.706911 dis train accu: 0.488432 \n",
      "epoch: 14 gen train loss: 0.690021 dis train loss: 0.706676 dis train accu: 0.493038 \n",
      "epoch: 15 gen train loss: 0.689946 dis train loss: 0.706636 dis train accu: 0.497427 \n",
      "epoch: 16 gen train loss: 0.694197 dis train loss: 0.706566 dis train accu: 0.496908 \n",
      "epoch: 17 gen train loss: 0.691877 dis train loss: 0.703865 dis train accu: 0.502205 \n",
      "epoch: 18 gen train loss: 0.68801 dis train loss: 0.705261 dis train accu: 0.504562 \n",
      "epoch: 19 gen train loss: 0.692762 dis train loss: 0.704579 dis train accu: 0.504584 \n",
      "epoch: 20 gen train loss: 0.691693 dis train loss: 0.705117 dis train accu: 0.506962 \n",
      "epoch: 21 gen train loss: 0.694428 dis train loss: 0.705002 dis train accu: 0.509622 \n",
      "epoch: 22 gen train loss: 0.688811 dis train loss: 0.704111 dis train accu: 0.511416 \n",
      "epoch: 23 gen train loss: 0.690377 dis train loss: 0.703727 dis train accu: 0.512324 \n",
      "epoch: 24 gen train loss: 0.693312 dis train loss: 0.702829 dis train accu: 0.514205 \n",
      "epoch: 25 gen train loss: 0.689778 dis train loss: 0.702291 dis train accu: 0.516714 \n",
      "epoch: 26 gen train loss: 0.693319 dis train loss: 0.703141 dis train accu: 0.516 \n",
      "epoch: 27 gen train loss: 0.69162 dis train loss: 0.70218 dis train accu: 0.517816 \n",
      "epoch: 28 gen train loss: 0.688919 dis train loss: 0.702506 dis train accu: 0.519373 \n",
      "epoch: 29 gen train loss: 0.691951 dis train loss: 0.701279 dis train accu: 0.521168 \n",
      "epoch: 30 gen train loss: 0.690481 dis train loss: 0.70066 dis train accu: 0.522357 \n",
      "epoch: 31 gen train loss: 0.693206 dis train loss: 0.701347 dis train accu: 0.521838 \n",
      "epoch: 32 gen train loss: 0.688983 dis train loss: 0.700782 dis train accu: 0.524195 \n",
      "epoch: 33 gen train loss: 0.688864 dis train loss: 0.700335 dis train accu: 0.524065 \n",
      "epoch: 34 gen train loss: 0.690855 dis train loss: 0.700168 dis train accu: 0.52787 \n",
      "epoch: 35 gen train loss: 0.688196 dis train loss: 0.700497 dis train accu: 0.526659 \n",
      "epoch: 36 gen train loss: 0.689497 dis train loss: 0.699589 dis train accu: 0.528432 \n",
      "epoch: 37 gen train loss: 0.689521 dis train loss: 0.699129 dis train accu: 0.528562 \n",
      "epoch: 38 gen train loss: 0.694181 dis train loss: 0.700417 dis train accu: 0.531654 \n",
      "epoch: 39 gen train loss: 0.688199 dis train loss: 0.699368 dis train accu: 0.530032 \n",
      "epoch: 40 gen train loss: 0.688002 dis train loss: 0.700424 dis train accu: 0.532195 \n",
      "epoch: 41 gen train loss: 0.687682 dis train loss: 0.698853 dis train accu: 0.531222 \n",
      "epoch: 42 gen train loss: 0.686131 dis train loss: 0.698279 dis train accu: 0.531827 \n",
      "epoch: 43 gen train loss: 0.689719 dis train loss: 0.698227 dis train accu: 0.533708 \n",
      "epoch: 44 gen train loss: 0.687576 dis train loss: 0.697788 dis train accu: 0.533881 \n",
      "epoch: 45 gen train loss: 0.686304 dis train loss: 0.698082 dis train accu: 0.533859 \n",
      "epoch: 46 gen train loss: 0.688245 dis train loss: 0.697898 dis train accu: 0.53453 \n",
      "epoch: 47 gen train loss: 0.689436 dis train loss: 0.697325 dis train accu: 0.536065 \n",
      "epoch: 48 gen train loss: 0.688017 dis train loss: 0.697803 dis train accu: 0.535762 \n",
      "epoch: 49 gen train loss: 0.687909 dis train loss: 0.697155 dis train accu: 0.536541 \n",
      "epoch: 50 gen train loss: 0.689138 dis train loss: 0.698166 dis train accu: 0.53693 \n",
      "epoch: 51 gen train loss: 0.687431 dis train loss: 0.697054 dis train accu: 0.537492 \n",
      "epoch: 52 gen train loss: 0.691178 dis train loss: 0.696466 dis train accu: 0.538076 \n",
      "epoch: 53 gen train loss: 0.686992 dis train loss: 0.696686 dis train accu: 0.538184 \n",
      "epoch: 54 gen train loss: 0.688946 dis train loss: 0.696441 dis train accu: 0.539092 \n",
      "epoch: 55 gen train loss: 0.689555 dis train loss: 0.696516 dis train accu: 0.538378 \n",
      "epoch: 56 gen train loss: 0.689395 dis train loss: 0.696997 dis train accu: 0.538141 \n",
      "epoch: 57 gen train loss: 0.688333 dis train loss: 0.696612 dis train accu: 0.540368 \n",
      "epoch: 58 gen train loss: 0.687214 dis train loss: 0.696265 dis train accu: 0.539351 \n",
      "epoch: 59 gen train loss: 0.689619 dis train loss: 0.696287 dis train accu: 0.539935 \n",
      "epoch: 60 gen train loss: 0.686201 dis train loss: 0.696448 dis train accu: 0.540195 \n",
      "epoch: 61 gen train loss: 0.689318 dis train loss: 0.696369 dis train accu: 0.539438 \n",
      "epoch: 62 gen train loss: 0.688715 dis train loss: 0.695514 dis train accu: 0.541254 \n",
      "epoch: 63 gen train loss: 0.686947 dis train loss: 0.695885 dis train accu: 0.540584 \n",
      "epoch: 64 gen train loss: 0.687545 dis train loss: 0.695452 dis train accu: 0.540476 \n",
      "epoch: 65 gen train loss: 0.689831 dis train loss: 0.695682 dis train accu: 0.541405 \n",
      "epoch: 66 gen train loss: 0.688078 dis train loss: 0.694843 dis train accu: 0.541492 \n",
      "epoch: 67 gen train loss: 0.688138 dis train loss: 0.69553 dis train accu: 0.541232 \n",
      "epoch: 68 gen train loss: 0.690037 dis train loss: 0.695438 dis train accu: 0.54147 \n",
      "epoch: 69 gen train loss: 0.68809 dis train loss: 0.69466 dis train accu: 0.541665 \n",
      "epoch: 70 gen train loss: 0.688887 dis train loss: 0.695706 dis train accu: 0.541816 \n",
      "epoch: 71 gen train loss: 0.685918 dis train loss: 0.69551 dis train accu: 0.541773 \n",
      "epoch: 72 gen train loss: 0.685465 dis train loss: 0.695199 dis train accu: 0.541946 \n",
      "epoch: 73 gen train loss: 0.687706 dis train loss: 0.695642 dis train accu: 0.541514 \n",
      "epoch: 74 gen train loss: 0.687426 dis train loss: 0.695273 dis train accu: 0.541708 \n",
      "epoch: 75 gen train loss: 0.687885 dis train loss: 0.694538 dis train accu: 0.542011 \n",
      "epoch: 76 gen train loss: 0.687757 dis train loss: 0.694525 dis train accu: 0.542119 \n",
      "epoch: 77 gen train loss: 0.688245 dis train loss: 0.694818 dis train accu: 0.542659 \n",
      "epoch: 78 gen train loss: 0.687822 dis train loss: 0.69397 dis train accu: 0.542465 \n",
      "epoch: 79 gen train loss: 0.685853 dis train loss: 0.694862 dis train accu: 0.542681 \n",
      "epoch: 80 gen train loss: 0.688538 dis train loss: 0.695126 dis train accu: 0.541686 \n",
      "epoch: 81 gen train loss: 0.688156 dis train loss: 0.694636 dis train accu: 0.542573 \n",
      "epoch: 82 gen train loss: 0.687589 dis train loss: 0.694435 dis train accu: 0.542724 \n",
      "epoch: 83 gen train loss: 0.686353 dis train loss: 0.693911 dis train accu: 0.542768 \n",
      "epoch: 84 gen train loss: 0.68917 dis train loss: 0.694431 dis train accu: 0.542681 \n",
      "epoch: 85 gen train loss: 0.686626 dis train loss: 0.694582 dis train accu: 0.542919 \n",
      "epoch: 86 gen train loss: 0.689159 dis train loss: 0.694358 dis train accu: 0.543027 \n",
      "epoch: 87 gen train loss: 0.68818 dis train loss: 0.694286 dis train accu: 0.542876 \n",
      "epoch: 88 gen train loss: 0.68713 dis train loss: 0.693972 dis train accu: 0.542919 \n",
      "epoch: 89 gen train loss: 0.688085 dis train loss: 0.694387 dis train accu: 0.543027 \n",
      "epoch: 90 gen train loss: 0.688716 dis train loss: 0.693981 dis train accu: 0.543114 \n",
      "epoch: 91 gen train loss: 0.685406 dis train loss: 0.693899 dis train accu: 0.542854 \n",
      "epoch: 92 gen train loss: 0.688103 dis train loss: 0.694295 dis train accu: 0.542941 \n",
      "epoch: 93 gen train loss: 0.688304 dis train loss: 0.694043 dis train accu: 0.542984 \n",
      "epoch: 94 gen train loss: 0.688599 dis train loss: 0.69351 dis train accu: 0.543135 \n",
      "epoch: 95 gen train loss: 0.689535 dis train loss: 0.693735 dis train accu: 0.542897 \n",
      "epoch: 96 gen train loss: 0.689877 dis train loss: 0.693658 dis train accu: 0.543178 \n",
      "epoch: 97 gen train loss: 0.689425 dis train loss: 0.69391 dis train accu: 0.54307 \n",
      "epoch: 98 gen train loss: 0.68709 dis train loss: 0.69372 dis train accu: 0.543027 \n",
      "epoch: 99 gen train loss: 0.689382 dis train loss: 0.694057 dis train accu: 0.5432 \n",
      "Adversarial training for ST slide 151674: \n",
      "Start adversarial training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7d1fe6d32b480d9f9012d8138a04a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f17c3eabf14955b43be43db5e7c883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 gen train loss: 0.642616 dis train loss: 0.726462 dis train accu: 0.389626 \n",
      "epoch: 1 gen train loss: 0.654271 dis train loss: 0.719632 dis train accu: 0.405682 \n",
      "epoch: 2 gen train loss: 0.657243 dis train loss: 0.71634 dis train accu: 0.419236 \n",
      "epoch: 3 gen train loss: 0.665424 dis train loss: 0.716569 dis train accu: 0.430027 \n",
      "epoch: 4 gen train loss: 0.664431 dis train loss: 0.712989 dis train accu: 0.437378 \n",
      "epoch: 5 gen train loss: 0.672859 dis train loss: 0.711945 dis train accu: 0.442852 \n",
      "epoch: 6 gen train loss: 0.667123 dis train loss: 0.71139 dis train accu: 0.448482 \n",
      "epoch: 7 gen train loss: 0.676614 dis train loss: 0.711462 dis train accu: 0.449107 \n",
      "epoch: 8 gen train loss: 0.678005 dis train loss: 0.711178 dis train accu: 0.456041 \n",
      "epoch: 9 gen train loss: 0.682399 dis train loss: 0.710492 dis train accu: 0.459898 \n",
      "epoch: 10 gen train loss: 0.685382 dis train loss: 0.711147 dis train accu: 0.457552 \n",
      "epoch: 11 gen train loss: 0.681232 dis train loss: 0.70953 dis train accu: 0.462531 \n",
      "epoch: 12 gen train loss: 0.690505 dis train loss: 0.708744 dis train accu: 0.466597 \n",
      "epoch: 13 gen train loss: 0.685909 dis train loss: 0.708731 dis train accu: 0.468031 \n",
      "epoch: 14 gen train loss: 0.686269 dis train loss: 0.709294 dis train accu: 0.469516 \n",
      "epoch: 15 gen train loss: 0.686615 dis train loss: 0.709552 dis train accu: 0.470455 \n",
      "epoch: 16 gen train loss: 0.687961 dis train loss: 0.708573 dis train accu: 0.475277 \n",
      "epoch: 17 gen train loss: 0.686516 dis train loss: 0.705991 dis train accu: 0.476528 \n",
      "epoch: 18 gen train loss: 0.690919 dis train loss: 0.707342 dis train accu: 0.477649 \n",
      "epoch: 19 gen train loss: 0.690233 dis train loss: 0.706309 dis train accu: 0.481376 \n",
      "epoch: 20 gen train loss: 0.692445 dis train loss: 0.705921 dis train accu: 0.480516 \n",
      "epoch: 21 gen train loss: 0.691622 dis train loss: 0.705018 dis train accu: 0.486068 \n",
      "epoch: 22 gen train loss: 0.688181 dis train loss: 0.706962 dis train accu: 0.481637 \n",
      "epoch: 23 gen train loss: 0.692833 dis train loss: 0.704794 dis train accu: 0.485182 \n",
      "epoch: 24 gen train loss: 0.692975 dis train loss: 0.705524 dis train accu: 0.490108 \n",
      "epoch: 25 gen train loss: 0.690173 dis train loss: 0.705159 dis train accu: 0.489118 \n",
      "epoch: 26 gen train loss: 0.69202 dis train loss: 0.705007 dis train accu: 0.487476 \n",
      "epoch: 27 gen train loss: 0.691545 dis train loss: 0.703255 dis train accu: 0.491307 \n",
      "epoch: 28 gen train loss: 0.685555 dis train loss: 0.704127 dis train accu: 0.493653 \n",
      "epoch: 29 gen train loss: 0.691667 dis train loss: 0.704881 dis train accu: 0.495504 \n",
      "epoch: 30 gen train loss: 0.691918 dis train loss: 0.704005 dis train accu: 0.496286 \n",
      "epoch: 31 gen train loss: 0.688853 dis train loss: 0.703666 dis train accu: 0.497276 \n",
      "epoch: 32 gen train loss: 0.692328 dis train loss: 0.702763 dis train accu: 0.496077 \n",
      "epoch: 33 gen train loss: 0.690141 dis train loss: 0.703522 dis train accu: 0.49725 \n",
      "epoch: 34 gen train loss: 0.69279 dis train loss: 0.70264 dis train accu: 0.498449 \n",
      "epoch: 35 gen train loss: 0.695576 dis train loss: 0.702251 dis train accu: 0.499231 \n",
      "epoch: 36 gen train loss: 0.691898 dis train loss: 0.701814 dis train accu: 0.499596 \n",
      "epoch: 37 gen train loss: 0.691324 dis train loss: 0.701687 dis train accu: 0.500456 \n",
      "epoch: 38 gen train loss: 0.692129 dis train loss: 0.700839 dis train accu: 0.501994 \n",
      "epoch: 39 gen train loss: 0.692586 dis train loss: 0.701906 dis train accu: 0.504131 \n",
      "epoch: 40 gen train loss: 0.69524 dis train loss: 0.701014 dis train accu: 0.505669 \n",
      "epoch: 41 gen train loss: 0.689864 dis train loss: 0.700957 dis train accu: 0.504731 \n",
      "epoch: 42 gen train loss: 0.688805 dis train loss: 0.700776 dis train accu: 0.503714 \n",
      "epoch: 43 gen train loss: 0.690236 dis train loss: 0.7004 dis train accu: 0.505278 \n",
      "epoch: 44 gen train loss: 0.69162 dis train loss: 0.699985 dis train accu: 0.504887 \n",
      "epoch: 45 gen train loss: 0.690152 dis train loss: 0.69926 dis train accu: 0.509814 \n",
      "epoch: 46 gen train loss: 0.692612 dis train loss: 0.700411 dis train accu: 0.506816 \n",
      "epoch: 47 gen train loss: 0.68988 dis train loss: 0.699505 dis train accu: 0.508667 \n",
      "epoch: 48 gen train loss: 0.691192 dis train loss: 0.700523 dis train accu: 0.509371 \n",
      "epoch: 49 gen train loss: 0.69539 dis train loss: 0.699145 dis train accu: 0.510205 \n",
      "epoch: 50 gen train loss: 0.691476 dis train loss: 0.699875 dis train accu: 0.508693 \n",
      "epoch: 51 gen train loss: 0.690204 dis train loss: 0.700163 dis train accu: 0.508771 \n",
      "epoch: 52 gen train loss: 0.689631 dis train loss: 0.700216 dis train accu: 0.511951 \n",
      "epoch: 53 gen train loss: 0.690723 dis train loss: 0.698901 dis train accu: 0.513098 \n",
      "epoch: 54 gen train loss: 0.689459 dis train loss: 0.699651 dis train accu: 0.512394 \n",
      "epoch: 55 gen train loss: 0.68842 dis train loss: 0.699013 dis train accu: 0.513098 \n",
      "epoch: 56 gen train loss: 0.689421 dis train loss: 0.698081 dis train accu: 0.513567 \n",
      "epoch: 57 gen train loss: 0.693142 dis train loss: 0.698143 dis train accu: 0.514375 \n",
      "epoch: 58 gen train loss: 0.689692 dis train loss: 0.699065 dis train accu: 0.513046 \n",
      "epoch: 59 gen train loss: 0.688903 dis train loss: 0.699234 dis train accu: 0.514114 \n",
      "epoch: 60 gen train loss: 0.691724 dis train loss: 0.697926 dis train accu: 0.515939 \n",
      "epoch: 61 gen train loss: 0.69052 dis train loss: 0.6985 dis train accu: 0.51461 \n",
      "epoch: 62 gen train loss: 0.6881 dis train loss: 0.698655 dis train accu: 0.516252 \n",
      "epoch: 63 gen train loss: 0.685294 dis train loss: 0.696924 dis train accu: 0.516017 \n",
      "epoch: 64 gen train loss: 0.688438 dis train loss: 0.698011 dis train accu: 0.516799 \n",
      "epoch: 65 gen train loss: 0.690195 dis train loss: 0.696888 dis train accu: 0.517034 \n",
      "epoch: 66 gen train loss: 0.688627 dis train loss: 0.69704 dis train accu: 0.517268 \n",
      "epoch: 67 gen train loss: 0.688791 dis train loss: 0.697499 dis train accu: 0.517138 \n",
      "epoch: 68 gen train loss: 0.686834 dis train loss: 0.697293 dis train accu: 0.518337 \n",
      "epoch: 69 gen train loss: 0.689667 dis train loss: 0.697387 dis train accu: 0.517607 \n",
      "epoch: 70 gen train loss: 0.691773 dis train loss: 0.696662 dis train accu: 0.517581 \n",
      "epoch: 71 gen train loss: 0.688508 dis train loss: 0.696977 dis train accu: 0.518624 \n",
      "epoch: 72 gen train loss: 0.690829 dis train loss: 0.697064 dis train accu: 0.518129 \n",
      "epoch: 73 gen train loss: 0.688893 dis train loss: 0.696397 dis train accu: 0.519171 \n",
      "epoch: 74 gen train loss: 0.688498 dis train loss: 0.696119 dis train accu: 0.518937 \n",
      "epoch: 75 gen train loss: 0.690509 dis train loss: 0.69662 dis train accu: 0.519197 \n",
      "epoch: 76 gen train loss: 0.689735 dis train loss: 0.696989 dis train accu: 0.519275 \n",
      "epoch: 77 gen train loss: 0.687902 dis train loss: 0.696139 dis train accu: 0.519458 \n",
      "epoch: 78 gen train loss: 0.688577 dis train loss: 0.695243 dis train accu: 0.518285 \n",
      "epoch: 79 gen train loss: 0.687742 dis train loss: 0.697017 dis train accu: 0.519562 \n",
      "epoch: 80 gen train loss: 0.688944 dis train loss: 0.695673 dis train accu: 0.518624 \n",
      "epoch: 81 gen train loss: 0.688939 dis train loss: 0.695674 dis train accu: 0.519015 \n",
      "epoch: 82 gen train loss: 0.687115 dis train loss: 0.695648 dis train accu: 0.519432 \n",
      "epoch: 83 gen train loss: 0.687896 dis train loss: 0.695561 dis train accu: 0.520474 \n",
      "epoch: 84 gen train loss: 0.686086 dis train loss: 0.696583 dis train accu: 0.519823 \n",
      "epoch: 85 gen train loss: 0.687768 dis train loss: 0.696129 dis train accu: 0.519666 \n",
      "epoch: 86 gen train loss: 0.688806 dis train loss: 0.696113 dis train accu: 0.520109 \n",
      "epoch: 87 gen train loss: 0.689368 dis train loss: 0.695666 dis train accu: 0.520057 \n",
      "epoch: 88 gen train loss: 0.689309 dis train loss: 0.695735 dis train accu: 0.520083 \n",
      "epoch: 89 gen train loss: 0.687353 dis train loss: 0.695153 dis train accu: 0.520683 \n",
      "epoch: 90 gen train loss: 0.687664 dis train loss: 0.696117 dis train accu: 0.519927 \n",
      "epoch: 91 gen train loss: 0.687932 dis train loss: 0.695166 dis train accu: 0.520422 \n",
      "epoch: 92 gen train loss: 0.687697 dis train loss: 0.695097 dis train accu: 0.520553 \n",
      "epoch: 93 gen train loss: 0.688909 dis train loss: 0.69632 dis train accu: 0.521048 \n",
      "epoch: 94 gen train loss: 0.690456 dis train loss: 0.695444 dis train accu: 0.521022 \n",
      "epoch: 95 gen train loss: 0.689772 dis train loss: 0.695115 dis train accu: 0.520996 \n",
      "epoch: 96 gen train loss: 0.688878 dis train loss: 0.694286 dis train accu: 0.521048 \n",
      "epoch: 97 gen train loss: 0.688021 dis train loss: 0.695051 dis train accu: 0.520944 \n",
      "epoch: 98 gen train loss: 0.687672 dis train loss: 0.695137 dis train accu: 0.521413 \n",
      "epoch: 99 gen train loss: 0.687545 dis train loss: 0.696028 dis train accu: 0.520579 \n",
      "Adversarial training for ST slide 151676: \n",
      "Start adversarial training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c70fe7a6ec498482a1eccc852d6efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb26633beb34a4bb085310b465c9c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 gen train loss: 0.648872 dis train loss: 0.722975 dis train accu: 0.400509 \n",
      "epoch: 1 gen train loss: 0.654178 dis train loss: 0.717687 dis train accu: 0.416863 \n",
      "epoch: 2 gen train loss: 0.658703 dis train loss: 0.714609 dis train accu: 0.431206 \n",
      "epoch: 3 gen train loss: 0.661306 dis train loss: 0.713351 dis train accu: 0.43992 \n",
      "epoch: 4 gen train loss: 0.667514 dis train loss: 0.712349 dis train accu: 0.446971 \n",
      "epoch: 5 gen train loss: 0.670668 dis train loss: 0.71098 dis train accu: 0.450322 \n",
      "epoch: 6 gen train loss: 0.669719 dis train loss: 0.711331 dis train accu: 0.459625 \n",
      "epoch: 7 gen train loss: 0.677503 dis train loss: 0.711177 dis train accu: 0.461126 \n",
      "epoch: 8 gen train loss: 0.672973 dis train loss: 0.709852 dis train accu: 0.466488 \n",
      "epoch: 9 gen train loss: 0.680663 dis train loss: 0.710372 dis train accu: 0.469169 \n",
      "epoch: 10 gen train loss: 0.679697 dis train loss: 0.709834 dis train accu: 0.470617 \n",
      "epoch: 11 gen train loss: 0.683095 dis train loss: 0.71027 dis train accu: 0.475576 \n",
      "epoch: 12 gen train loss: 0.686697 dis train loss: 0.709198 dis train accu: 0.47933 \n",
      "epoch: 13 gen train loss: 0.680704 dis train loss: 0.708481 dis train accu: 0.482038 \n",
      "epoch: 14 gen train loss: 0.692533 dis train loss: 0.707055 dis train accu: 0.480402 \n",
      "epoch: 15 gen train loss: 0.685593 dis train loss: 0.706819 dis train accu: 0.485282 \n",
      "epoch: 16 gen train loss: 0.691546 dis train loss: 0.707668 dis train accu: 0.486568 \n",
      "epoch: 17 gen train loss: 0.692846 dis train loss: 0.707675 dis train accu: 0.48941 \n",
      "epoch: 18 gen train loss: 0.694009 dis train loss: 0.707134 dis train accu: 0.490402 \n",
      "epoch: 19 gen train loss: 0.690927 dis train loss: 0.706957 dis train accu: 0.493458 \n",
      "epoch: 20 gen train loss: 0.691988 dis train loss: 0.706229 dis train accu: 0.494129 \n",
      "epoch: 21 gen train loss: 0.691134 dis train loss: 0.70591 dis train accu: 0.495416 \n",
      "epoch: 22 gen train loss: 0.69577 dis train loss: 0.706398 dis train accu: 0.498284 \n",
      "epoch: 23 gen train loss: 0.691183 dis train loss: 0.705295 dis train accu: 0.498365 \n",
      "epoch: 24 gen train loss: 0.693679 dis train loss: 0.704952 dis train accu: 0.498418 \n",
      "epoch: 25 gen train loss: 0.689794 dis train loss: 0.704325 dis train accu: 0.501689 \n",
      "epoch: 26 gen train loss: 0.689816 dis train loss: 0.704067 dis train accu: 0.503753 \n",
      "epoch: 27 gen train loss: 0.690597 dis train loss: 0.703748 dis train accu: 0.505335 \n",
      "epoch: 28 gen train loss: 0.692803 dis train loss: 0.704064 dis train accu: 0.506542 \n",
      "epoch: 29 gen train loss: 0.689255 dis train loss: 0.70278 dis train accu: 0.507855 \n",
      "epoch: 30 gen train loss: 0.694313 dis train loss: 0.704009 dis train accu: 0.510509 \n",
      "epoch: 31 gen train loss: 0.695423 dis train loss: 0.703921 dis train accu: 0.505657 \n",
      "epoch: 32 gen train loss: 0.688991 dis train loss: 0.701997 dis train accu: 0.510858 \n",
      "epoch: 33 gen train loss: 0.697008 dis train loss: 0.702214 dis train accu: 0.511823 \n",
      "epoch: 34 gen train loss: 0.690273 dis train loss: 0.702079 dis train accu: 0.510429 \n",
      "epoch: 35 gen train loss: 0.687364 dis train loss: 0.701685 dis train accu: 0.51244 \n",
      "epoch: 36 gen train loss: 0.689745 dis train loss: 0.702593 dis train accu: 0.513056 \n",
      "epoch: 37 gen train loss: 0.694442 dis train loss: 0.701606 dis train accu: 0.515282 \n",
      "epoch: 38 gen train loss: 0.689624 dis train loss: 0.700959 dis train accu: 0.517265 \n",
      "epoch: 39 gen train loss: 0.690848 dis train loss: 0.700361 dis train accu: 0.518579 \n",
      "epoch: 40 gen train loss: 0.69109 dis train loss: 0.700364 dis train accu: 0.518123 \n",
      "epoch: 41 gen train loss: 0.68851 dis train loss: 0.700794 dis train accu: 0.517265 \n",
      "epoch: 42 gen train loss: 0.690045 dis train loss: 0.700453 dis train accu: 0.518445 \n",
      "epoch: 43 gen train loss: 0.689585 dis train loss: 0.700909 dis train accu: 0.518499 \n",
      "epoch: 44 gen train loss: 0.689115 dis train loss: 0.701751 dis train accu: 0.52059 \n",
      "epoch: 45 gen train loss: 0.690227 dis train loss: 0.700359 dis train accu: 0.521287 \n",
      "epoch: 46 gen train loss: 0.691567 dis train loss: 0.699614 dis train accu: 0.521555 \n",
      "epoch: 47 gen train loss: 0.686529 dis train loss: 0.69914 dis train accu: 0.523244 \n",
      "epoch: 48 gen train loss: 0.691942 dis train loss: 0.698842 dis train accu: 0.523164 \n",
      "epoch: 49 gen train loss: 0.690982 dis train loss: 0.699905 dis train accu: 0.525174 \n",
      "epoch: 50 gen train loss: 0.686939 dis train loss: 0.699848 dis train accu: 0.524048 \n",
      "epoch: 51 gen train loss: 0.693461 dis train loss: 0.698827 dis train accu: 0.525523 \n",
      "epoch: 52 gen train loss: 0.691082 dis train loss: 0.698703 dis train accu: 0.52496 \n",
      "epoch: 53 gen train loss: 0.689565 dis train loss: 0.698207 dis train accu: 0.524772 \n",
      "epoch: 54 gen train loss: 0.691082 dis train loss: 0.698737 dis train accu: 0.527078 \n",
      "epoch: 55 gen train loss: 0.688753 dis train loss: 0.698571 dis train accu: 0.527399 \n",
      "epoch: 56 gen train loss: 0.686331 dis train loss: 0.697876 dis train accu: 0.526997 \n",
      "epoch: 57 gen train loss: 0.691018 dis train loss: 0.698263 dis train accu: 0.528043 \n",
      "epoch: 58 gen train loss: 0.692696 dis train loss: 0.697168 dis train accu: 0.52815 \n",
      "epoch: 59 gen train loss: 0.686586 dis train loss: 0.699166 dis train accu: 0.528874 \n",
      "epoch: 60 gen train loss: 0.69233 dis train loss: 0.698044 dis train accu: 0.527936 \n",
      "epoch: 61 gen train loss: 0.686869 dis train loss: 0.697794 dis train accu: 0.529115 \n",
      "epoch: 62 gen train loss: 0.688682 dis train loss: 0.697966 dis train accu: 0.52882 \n",
      "epoch: 63 gen train loss: 0.689684 dis train loss: 0.696605 dis train accu: 0.530804 \n",
      "epoch: 64 gen train loss: 0.689617 dis train loss: 0.69678 dis train accu: 0.528981 \n",
      "epoch: 65 gen train loss: 0.688964 dis train loss: 0.697715 dis train accu: 0.530054 \n",
      "epoch: 66 gen train loss: 0.689779 dis train loss: 0.696828 dis train accu: 0.530456 \n",
      "epoch: 67 gen train loss: 0.6886 dis train loss: 0.69759 dis train accu: 0.531448 \n",
      "epoch: 68 gen train loss: 0.689045 dis train loss: 0.697161 dis train accu: 0.531287 \n",
      "epoch: 69 gen train loss: 0.689936 dis train loss: 0.697575 dis train accu: 0.532172 \n",
      "epoch: 70 gen train loss: 0.689912 dis train loss: 0.696863 dis train accu: 0.531877 \n",
      "epoch: 71 gen train loss: 0.691417 dis train loss: 0.69658 dis train accu: 0.532547 \n",
      "epoch: 72 gen train loss: 0.690907 dis train loss: 0.696626 dis train accu: 0.532145 \n",
      "epoch: 73 gen train loss: 0.688472 dis train loss: 0.696808 dis train accu: 0.532654 \n",
      "epoch: 74 gen train loss: 0.690063 dis train loss: 0.696145 dis train accu: 0.533137 \n",
      "epoch: 75 gen train loss: 0.689716 dis train loss: 0.696177 dis train accu: 0.534129 \n",
      "epoch: 76 gen train loss: 0.687606 dis train loss: 0.695872 dis train accu: 0.533914 \n",
      "epoch: 77 gen train loss: 0.687512 dis train loss: 0.696115 dis train accu: 0.53378 \n",
      "epoch: 78 gen train loss: 0.689501 dis train loss: 0.696648 dis train accu: 0.533056 \n",
      "epoch: 79 gen train loss: 0.68686 dis train loss: 0.69496 dis train accu: 0.533861 \n",
      "epoch: 80 gen train loss: 0.689751 dis train loss: 0.695129 dis train accu: 0.535201 \n",
      "epoch: 81 gen train loss: 0.689717 dis train loss: 0.696258 dis train accu: 0.533887 \n",
      "epoch: 82 gen train loss: 0.686894 dis train loss: 0.695422 dis train accu: 0.534129 \n",
      "epoch: 83 gen train loss: 0.689625 dis train loss: 0.69513 dis train accu: 0.534826 \n",
      "epoch: 84 gen train loss: 0.689226 dis train loss: 0.696194 dis train accu: 0.534853 \n",
      "epoch: 85 gen train loss: 0.69031 dis train loss: 0.695689 dis train accu: 0.534584 \n",
      "epoch: 86 gen train loss: 0.688434 dis train loss: 0.695205 dis train accu: 0.534853 \n",
      "epoch: 87 gen train loss: 0.688769 dis train loss: 0.695113 dis train accu: 0.535067 \n",
      "epoch: 88 gen train loss: 0.688893 dis train loss: 0.695786 dis train accu: 0.534397 \n",
      "epoch: 89 gen train loss: 0.689387 dis train loss: 0.695394 dis train accu: 0.534397 \n",
      "epoch: 90 gen train loss: 0.689655 dis train loss: 0.695423 dis train accu: 0.534879 \n",
      "epoch: 91 gen train loss: 0.690204 dis train loss: 0.694884 dis train accu: 0.535389 \n",
      "epoch: 92 gen train loss: 0.689715 dis train loss: 0.695043 dis train accu: 0.53496 \n",
      "epoch: 93 gen train loss: 0.687953 dis train loss: 0.695013 dis train accu: 0.535201 \n",
      "epoch: 94 gen train loss: 0.689243 dis train loss: 0.695171 dis train accu: 0.53563 \n",
      "epoch: 95 gen train loss: 0.687846 dis train loss: 0.694551 dis train accu: 0.53571 \n",
      "epoch: 96 gen train loss: 0.68794 dis train loss: 0.694474 dis train accu: 0.535228 \n",
      "epoch: 97 gen train loss: 0.690622 dis train loss: 0.694768 dis train accu: 0.536059 \n",
      "epoch: 98 gen train loss: 0.690915 dis train loss: 0.695146 dis train accu: 0.535764 \n",
      "epoch: 99 gen train loss: 0.690252 dis train loss: 0.694789 dis train accu: 0.535791 \n",
      "Adversarial training for ST slide 151675: \n",
      "Start adversarial training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877bd80b46c3446bba91ae745a488128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305bab5f879c4f328d945f39f7ff5732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 gen train loss: 0.651117 dis train loss: 0.722422 dis train accu: 0.396654 \n",
      "epoch: 1 gen train loss: 0.655717 dis train loss: 0.718226 dis train accu: 0.413277 \n",
      "epoch: 2 gen train loss: 0.660126 dis train loss: 0.717802 dis train accu: 0.426159 \n",
      "epoch: 3 gen train loss: 0.658532 dis train loss: 0.713304 dis train accu: 0.434089 \n",
      "epoch: 4 gen train loss: 0.662617 dis train loss: 0.715056 dis train accu: 0.439726 \n",
      "epoch: 5 gen train loss: 0.671339 dis train loss: 0.712226 dis train accu: 0.445285 \n",
      "epoch: 6 gen train loss: 0.677042 dis train loss: 0.711429 dis train accu: 0.450448 \n",
      "epoch: 7 gen train loss: 0.678874 dis train loss: 0.711639 dis train accu: 0.453583 \n",
      "epoch: 8 gen train loss: 0.675649 dis train loss: 0.710363 dis train accu: 0.459642 \n",
      "epoch: 9 gen train loss: 0.683378 dis train loss: 0.708387 dis train accu: 0.463857 \n",
      "epoch: 10 gen train loss: 0.681479 dis train loss: 0.710654 dis train accu: 0.464278 \n",
      "epoch: 11 gen train loss: 0.680922 dis train loss: 0.708954 dis train accu: 0.466517 \n",
      "epoch: 12 gen train loss: 0.688725 dis train loss: 0.708189 dis train accu: 0.471602 \n",
      "epoch: 13 gen train loss: 0.687511 dis train loss: 0.709037 dis train accu: 0.472813 \n",
      "epoch: 14 gen train loss: 0.684825 dis train loss: 0.708854 dis train accu: 0.472734 \n",
      "epoch: 15 gen train loss: 0.688186 dis train loss: 0.707055 dis train accu: 0.478741 \n",
      "epoch: 16 gen train loss: 0.692196 dis train loss: 0.708547 dis train accu: 0.478583 \n",
      "epoch: 17 gen train loss: 0.694737 dis train loss: 0.707455 dis train accu: 0.481481 \n",
      "epoch: 18 gen train loss: 0.69069 dis train loss: 0.706557 dis train accu: 0.483509 \n",
      "epoch: 19 gen train loss: 0.69401 dis train loss: 0.706646 dis train accu: 0.4848 \n",
      "epoch: 20 gen train loss: 0.692663 dis train loss: 0.705726 dis train accu: 0.48833 \n",
      "epoch: 21 gen train loss: 0.694253 dis train loss: 0.706824 dis train accu: 0.48667 \n",
      "epoch: 22 gen train loss: 0.690432 dis train loss: 0.705488 dis train accu: 0.488093 \n",
      "epoch: 23 gen train loss: 0.689784 dis train loss: 0.706055 dis train accu: 0.491359 \n",
      "epoch: 24 gen train loss: 0.692066 dis train loss: 0.706088 dis train accu: 0.492387 \n",
      "epoch: 25 gen train loss: 0.692583 dis train loss: 0.705177 dis train accu: 0.49726 \n",
      "epoch: 26 gen train loss: 0.690131 dis train loss: 0.707059 dis train accu: 0.496154 \n",
      "epoch: 27 gen train loss: 0.689826 dis train loss: 0.705017 dis train accu: 0.49697 \n",
      "epoch: 28 gen train loss: 0.695553 dis train loss: 0.703825 dis train accu: 0.497787 \n",
      "epoch: 29 gen train loss: 0.691546 dis train loss: 0.7039 dis train accu: 0.499579 \n",
      "epoch: 30 gen train loss: 0.6948 dis train loss: 0.702514 dis train accu: 0.500395 \n",
      "epoch: 31 gen train loss: 0.694347 dis train loss: 0.703446 dis train accu: 0.500606 \n",
      "epoch: 32 gen train loss: 0.689989 dis train loss: 0.7029 dis train accu: 0.501475 \n",
      "epoch: 33 gen train loss: 0.691895 dis train loss: 0.701846 dis train accu: 0.503846 \n",
      "epoch: 34 gen train loss: 0.693889 dis train loss: 0.703386 dis train accu: 0.504294 \n",
      "epoch: 35 gen train loss: 0.690309 dis train loss: 0.702587 dis train accu: 0.504004 \n",
      "epoch: 36 gen train loss: 0.692138 dis train loss: 0.701346 dis train accu: 0.503952 \n",
      "epoch: 37 gen train loss: 0.689205 dis train loss: 0.701936 dis train accu: 0.506006 \n",
      "epoch: 38 gen train loss: 0.68932 dis train loss: 0.701551 dis train accu: 0.506955 \n",
      "epoch: 39 gen train loss: 0.688503 dis train loss: 0.701743 dis train accu: 0.509194 \n",
      "epoch: 40 gen train loss: 0.690074 dis train loss: 0.700972 dis train accu: 0.509194 \n",
      "epoch: 41 gen train loss: 0.693904 dis train loss: 0.700563 dis train accu: 0.510616 \n",
      "epoch: 42 gen train loss: 0.69068 dis train loss: 0.701208 dis train accu: 0.50872 \n",
      "epoch: 43 gen train loss: 0.696651 dis train loss: 0.701276 dis train accu: 0.510643 \n",
      "epoch: 44 gen train loss: 0.693414 dis train loss: 0.700425 dis train accu: 0.511038 \n",
      "epoch: 45 gen train loss: 0.691519 dis train loss: 0.699828 dis train accu: 0.512935 \n",
      "epoch: 46 gen train loss: 0.689518 dis train loss: 0.700527 dis train accu: 0.514305 \n",
      "epoch: 47 gen train loss: 0.687813 dis train loss: 0.701338 dis train accu: 0.515121 \n",
      "epoch: 48 gen train loss: 0.694393 dis train loss: 0.699887 dis train accu: 0.51362 \n",
      "epoch: 49 gen train loss: 0.69238 dis train loss: 0.69854 dis train accu: 0.515437 \n",
      "epoch: 50 gen train loss: 0.694747 dis train loss: 0.699712 dis train accu: 0.516307 \n",
      "epoch: 51 gen train loss: 0.689263 dis train loss: 0.699665 dis train accu: 0.516228 \n",
      "epoch: 52 gen train loss: 0.691659 dis train loss: 0.6995 dis train accu: 0.515991 \n",
      "epoch: 53 gen train loss: 0.690135 dis train loss: 0.699936 dis train accu: 0.516438 \n",
      "epoch: 54 gen train loss: 0.690174 dis train loss: 0.699574 dis train accu: 0.518704 \n",
      "epoch: 55 gen train loss: 0.692112 dis train loss: 0.698376 dis train accu: 0.518967 \n",
      "epoch: 56 gen train loss: 0.688503 dis train loss: 0.698699 dis train accu: 0.517835 \n",
      "epoch: 57 gen train loss: 0.688183 dis train loss: 0.6984 dis train accu: 0.517334 \n",
      "epoch: 58 gen train loss: 0.690802 dis train loss: 0.698344 dis train accu: 0.518888 \n",
      "epoch: 59 gen train loss: 0.692213 dis train loss: 0.697303 dis train accu: 0.520126 \n",
      "epoch: 60 gen train loss: 0.688386 dis train loss: 0.69778 dis train accu: 0.521075 \n",
      "epoch: 61 gen train loss: 0.690224 dis train loss: 0.697519 dis train accu: 0.521048 \n",
      "epoch: 62 gen train loss: 0.692473 dis train loss: 0.697916 dis train accu: 0.520469 \n",
      "epoch: 63 gen train loss: 0.687519 dis train loss: 0.698522 dis train accu: 0.521075 \n",
      "epoch: 64 gen train loss: 0.693226 dis train loss: 0.697943 dis train accu: 0.521259 \n",
      "epoch: 65 gen train loss: 0.690001 dis train loss: 0.697425 dis train accu: 0.522102 \n",
      "epoch: 66 gen train loss: 0.689949 dis train loss: 0.697386 dis train accu: 0.523367 \n",
      "epoch: 67 gen train loss: 0.688344 dis train loss: 0.696753 dis train accu: 0.522708 \n",
      "epoch: 68 gen train loss: 0.689 dis train loss: 0.696626 dis train accu: 0.523103 \n",
      "epoch: 69 gen train loss: 0.68971 dis train loss: 0.69698 dis train accu: 0.523077 \n",
      "epoch: 70 gen train loss: 0.689344 dis train loss: 0.697174 dis train accu: 0.523051 \n",
      "epoch: 71 gen train loss: 0.687387 dis train loss: 0.696182 dis train accu: 0.523683 \n",
      "epoch: 72 gen train loss: 0.689786 dis train loss: 0.696351 dis train accu: 0.522629 \n",
      "epoch: 73 gen train loss: 0.689479 dis train loss: 0.696304 dis train accu: 0.523815 \n",
      "epoch: 74 gen train loss: 0.690656 dis train loss: 0.696473 dis train accu: 0.523419 \n",
      "epoch: 75 gen train loss: 0.687464 dis train loss: 0.696361 dis train accu: 0.523946 \n",
      "epoch: 76 gen train loss: 0.687431 dis train loss: 0.697012 dis train accu: 0.524104 \n",
      "epoch: 77 gen train loss: 0.690287 dis train loss: 0.696468 dis train accu: 0.524868 \n",
      "epoch: 78 gen train loss: 0.68919 dis train loss: 0.696656 dis train accu: 0.524447 \n",
      "epoch: 79 gen train loss: 0.69144 dis train loss: 0.696052 dis train accu: 0.525132 \n",
      "epoch: 80 gen train loss: 0.687541 dis train loss: 0.696187 dis train accu: 0.525 \n",
      "epoch: 81 gen train loss: 0.68993 dis train loss: 0.696041 dis train accu: 0.525158 \n",
      "epoch: 82 gen train loss: 0.690024 dis train loss: 0.696832 dis train accu: 0.525342 \n",
      "epoch: 83 gen train loss: 0.690891 dis train loss: 0.696481 dis train accu: 0.525026 \n",
      "epoch: 84 gen train loss: 0.687955 dis train loss: 0.695834 dis train accu: 0.525738 \n",
      "epoch: 85 gen train loss: 0.689009 dis train loss: 0.696612 dis train accu: 0.524579 \n",
      "epoch: 86 gen train loss: 0.684052 dis train loss: 0.696083 dis train accu: 0.525026 \n",
      "epoch: 87 gen train loss: 0.689546 dis train loss: 0.69574 dis train accu: 0.525869 \n",
      "epoch: 88 gen train loss: 0.687707 dis train loss: 0.695522 dis train accu: 0.525395 \n",
      "epoch: 89 gen train loss: 0.690108 dis train loss: 0.695767 dis train accu: 0.525896 \n",
      "epoch: 90 gen train loss: 0.687982 dis train loss: 0.695674 dis train accu: 0.525395 \n",
      "epoch: 91 gen train loss: 0.687796 dis train loss: 0.695645 dis train accu: 0.52608 \n",
      "epoch: 92 gen train loss: 0.688615 dis train loss: 0.694834 dis train accu: 0.526528 \n",
      "epoch: 93 gen train loss: 0.686803 dis train loss: 0.695068 dis train accu: 0.526001 \n",
      "epoch: 94 gen train loss: 0.68876 dis train loss: 0.69515 dis train accu: 0.52558 \n",
      "epoch: 95 gen train loss: 0.687556 dis train loss: 0.695248 dis train accu: 0.52579 \n",
      "epoch: 96 gen train loss: 0.687747 dis train loss: 0.694419 dis train accu: 0.526185 \n",
      "epoch: 97 gen train loss: 0.688465 dis train loss: 0.694348 dis train accu: 0.525948 \n",
      "epoch: 98 gen train loss: 0.688864 dis train loss: 0.694786 dis train accu: 0.526185 \n",
      "epoch: 99 gen train loss: 0.688731 dis train loss: 0.694601 dis train accu: 0.526291 \n",
      "Adversarial training for ST slide 151673: \n",
      "Start adversarial training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28059a57b16c401184c9b6f6c4c66e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd0b31183bf4f9bb1c6fe697753bc0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 gen train loss: 0.650477 dis train loss: 0.725372 dis train accu: 0.393062 \n",
      "epoch: 1 gen train loss: 0.653152 dis train loss: 0.71934 dis train accu: 0.411049 \n",
      "epoch: 2 gen train loss: 0.658555 dis train loss: 0.715889 dis train accu: 0.423223 \n",
      "epoch: 3 gen train loss: 0.662367 dis train loss: 0.712399 dis train accu: 0.431104 \n",
      "epoch: 4 gen train loss: 0.667519 dis train loss: 0.714626 dis train accu: 0.435109 \n",
      "epoch: 5 gen train loss: 0.668361 dis train loss: 0.713007 dis train accu: 0.4421 \n",
      "epoch: 6 gen train loss: 0.67696 dis train loss: 0.712196 dis train accu: 0.446289 \n",
      "epoch: 7 gen train loss: 0.674628 dis train loss: 0.711362 dis train accu: 0.450818 \n",
      "epoch: 8 gen train loss: 0.681658 dis train loss: 0.710976 dis train accu: 0.45286 \n",
      "epoch: 9 gen train loss: 0.684908 dis train loss: 0.710787 dis train accu: 0.459955 \n",
      "epoch: 10 gen train loss: 0.679855 dis train loss: 0.709828 dis train accu: 0.463726 \n",
      "epoch: 11 gen train loss: 0.68782 dis train loss: 0.710002 dis train accu: 0.46548 \n",
      "epoch: 12 gen train loss: 0.684096 dis train loss: 0.708039 dis train accu: 0.469957 \n",
      "epoch: 13 gen train loss: 0.688474 dis train loss: 0.71016 dis train accu: 0.472706 \n",
      "epoch: 14 gen train loss: 0.691264 dis train loss: 0.707698 dis train accu: 0.470585 \n",
      "epoch: 15 gen train loss: 0.687282 dis train loss: 0.706641 dis train accu: 0.478178 \n",
      "epoch: 16 gen train loss: 0.686417 dis train loss: 0.707739 dis train accu: 0.475743 \n",
      "epoch: 17 gen train loss: 0.690679 dis train loss: 0.706882 dis train accu: 0.479042 \n",
      "epoch: 18 gen train loss: 0.684315 dis train loss: 0.707541 dis train accu: 0.479408 \n",
      "epoch: 19 gen train loss: 0.68597 dis train loss: 0.706591 dis train accu: 0.480508 \n",
      "epoch: 20 gen train loss: 0.686771 dis train loss: 0.706562 dis train accu: 0.485116 \n",
      "epoch: 21 gen train loss: 0.691148 dis train loss: 0.705567 dis train accu: 0.483597 \n",
      "epoch: 22 gen train loss: 0.688672 dis train loss: 0.705303 dis train accu: 0.485378 \n",
      "epoch: 23 gen train loss: 0.69199 dis train loss: 0.705599 dis train accu: 0.487708 \n",
      "epoch: 24 gen train loss: 0.688543 dis train loss: 0.705358 dis train accu: 0.490378 \n",
      "epoch: 25 gen train loss: 0.689487 dis train loss: 0.706405 dis train accu: 0.488834 \n",
      "epoch: 26 gen train loss: 0.694543 dis train loss: 0.704051 dis train accu: 0.491818 \n",
      "epoch: 27 gen train loss: 0.688822 dis train loss: 0.704719 dis train accu: 0.493887 \n",
      "epoch: 28 gen train loss: 0.69439 dis train loss: 0.704086 dis train accu: 0.494882 \n",
      "epoch: 29 gen train loss: 0.693492 dis train loss: 0.703975 dis train accu: 0.495615 \n",
      "epoch: 30 gen train loss: 0.689482 dis train loss: 0.703302 dis train accu: 0.496138 \n",
      "epoch: 31 gen train loss: 0.689576 dis train loss: 0.703872 dis train accu: 0.497735 \n",
      "epoch: 32 gen train loss: 0.697045 dis train loss: 0.703234 dis train accu: 0.500039 \n",
      "epoch: 33 gen train loss: 0.692801 dis train loss: 0.702249 dis train accu: 0.501296 \n",
      "epoch: 34 gen train loss: 0.688337 dis train loss: 0.701767 dis train accu: 0.502605 \n",
      "epoch: 35 gen train loss: 0.692148 dis train loss: 0.702679 dis train accu: 0.503574 \n",
      "epoch: 36 gen train loss: 0.691294 dis train loss: 0.702545 dis train accu: 0.502369 \n",
      "epoch: 37 gen train loss: 0.692785 dis train loss: 0.702918 dis train accu: 0.504281 \n",
      "epoch: 38 gen train loss: 0.690193 dis train loss: 0.701762 dis train accu: 0.503417 \n",
      "epoch: 39 gen train loss: 0.691974 dis train loss: 0.701903 dis train accu: 0.50538 \n",
      "epoch: 40 gen train loss: 0.695455 dis train loss: 0.701532 dis train accu: 0.505223 \n",
      "epoch: 41 gen train loss: 0.69157 dis train loss: 0.700645 dis train accu: 0.506977 \n",
      "epoch: 42 gen train loss: 0.692536 dis train loss: 0.701721 dis train accu: 0.507527 \n",
      "epoch: 43 gen train loss: 0.690988 dis train loss: 0.700803 dis train accu: 0.508522 \n",
      "epoch: 44 gen train loss: 0.688191 dis train loss: 0.699652 dis train accu: 0.508444 \n",
      "epoch: 45 gen train loss: 0.692621 dis train loss: 0.700374 dis train accu: 0.508182 \n",
      "epoch: 46 gen train loss: 0.69675 dis train loss: 0.700111 dis train accu: 0.510747 \n",
      "epoch: 47 gen train loss: 0.690829 dis train loss: 0.698847 dis train accu: 0.511873 \n",
      "epoch: 48 gen train loss: 0.688707 dis train loss: 0.699683 dis train accu: 0.510957 \n",
      "epoch: 49 gen train loss: 0.688337 dis train loss: 0.698682 dis train accu: 0.513313 \n",
      "epoch: 50 gen train loss: 0.68906 dis train loss: 0.698389 dis train accu: 0.512423 \n",
      "epoch: 51 gen train loss: 0.691774 dis train loss: 0.699778 dis train accu: 0.512057 \n",
      "epoch: 52 gen train loss: 0.691042 dis train loss: 0.698674 dis train accu: 0.514203 \n",
      "epoch: 53 gen train loss: 0.692295 dis train loss: 0.699001 dis train accu: 0.515513 \n",
      "epoch: 54 gen train loss: 0.690106 dis train loss: 0.699296 dis train accu: 0.513523 \n",
      "epoch: 55 gen train loss: 0.691514 dis train loss: 0.698515 dis train accu: 0.515251 \n",
      "epoch: 56 gen train loss: 0.690995 dis train loss: 0.698573 dis train accu: 0.514596 \n",
      "epoch: 57 gen train loss: 0.691396 dis train loss: 0.697902 dis train accu: 0.51546 \n",
      "epoch: 58 gen train loss: 0.693923 dis train loss: 0.698104 dis train accu: 0.51711 \n",
      "epoch: 59 gen train loss: 0.688371 dis train loss: 0.697546 dis train accu: 0.518288 \n",
      "epoch: 60 gen train loss: 0.690075 dis train loss: 0.69707 dis train accu: 0.516822 \n",
      "epoch: 61 gen train loss: 0.690808 dis train loss: 0.697875 dis train accu: 0.516769 \n",
      "epoch: 62 gen train loss: 0.689804 dis train loss: 0.698612 dis train accu: 0.518366 \n",
      "epoch: 63 gen train loss: 0.690523 dis train loss: 0.69656 dis train accu: 0.518235 \n",
      "epoch: 64 gen train loss: 0.690563 dis train loss: 0.697972 dis train accu: 0.518628 \n",
      "epoch: 65 gen train loss: 0.692153 dis train loss: 0.697419 dis train accu: 0.519387 \n",
      "epoch: 66 gen train loss: 0.690794 dis train loss: 0.697461 dis train accu: 0.519806 \n",
      "epoch: 67 gen train loss: 0.688956 dis train loss: 0.697225 dis train accu: 0.51923 \n",
      "epoch: 68 gen train loss: 0.69024 dis train loss: 0.697513 dis train accu: 0.51889 \n",
      "epoch: 69 gen train loss: 0.690726 dis train loss: 0.697044 dis train accu: 0.519885 \n",
      "epoch: 70 gen train loss: 0.688194 dis train loss: 0.696895 dis train accu: 0.520199 \n",
      "epoch: 71 gen train loss: 0.689414 dis train loss: 0.696217 dis train accu: 0.520775 \n",
      "epoch: 72 gen train loss: 0.692162 dis train loss: 0.697107 dis train accu: 0.520618 \n",
      "epoch: 73 gen train loss: 0.688926 dis train loss: 0.696341 dis train accu: 0.519885 \n",
      "epoch: 74 gen train loss: 0.692123 dis train loss: 0.697111 dis train accu: 0.519649 \n",
      "epoch: 75 gen train loss: 0.691538 dis train loss: 0.69641 dis train accu: 0.52067 \n",
      "epoch: 76 gen train loss: 0.691853 dis train loss: 0.695752 dis train accu: 0.521272 \n",
      "epoch: 77 gen train loss: 0.690787 dis train loss: 0.696839 dis train accu: 0.521325 \n",
      "epoch: 78 gen train loss: 0.689062 dis train loss: 0.696292 dis train accu: 0.52122 \n",
      "epoch: 79 gen train loss: 0.689436 dis train loss: 0.695807 dis train accu: 0.520906 \n",
      "epoch: 80 gen train loss: 0.690325 dis train loss: 0.695302 dis train accu: 0.522032 \n",
      "epoch: 81 gen train loss: 0.688461 dis train loss: 0.696107 dis train accu: 0.521744 \n",
      "epoch: 82 gen train loss: 0.687159 dis train loss: 0.696055 dis train accu: 0.521848 \n",
      "epoch: 83 gen train loss: 0.688867 dis train loss: 0.695561 dis train accu: 0.521691 \n",
      "epoch: 84 gen train loss: 0.688469 dis train loss: 0.695845 dis train accu: 0.521953 \n",
      "epoch: 85 gen train loss: 0.688151 dis train loss: 0.696207 dis train accu: 0.522372 \n",
      "epoch: 86 gen train loss: 0.688729 dis train loss: 0.695662 dis train accu: 0.521613 \n",
      "epoch: 87 gen train loss: 0.686919 dis train loss: 0.695593 dis train accu: 0.522189 \n",
      "epoch: 88 gen train loss: 0.687071 dis train loss: 0.695878 dis train accu: 0.522765 \n",
      "epoch: 89 gen train loss: 0.687296 dis train loss: 0.696004 dis train accu: 0.522686 \n",
      "epoch: 90 gen train loss: 0.686648 dis train loss: 0.696112 dis train accu: 0.522529 \n",
      "epoch: 91 gen train loss: 0.689327 dis train loss: 0.69532 dis train accu: 0.522503 \n",
      "epoch: 92 gen train loss: 0.687861 dis train loss: 0.695201 dis train accu: 0.52266 \n",
      "epoch: 93 gen train loss: 0.687761 dis train loss: 0.695109 dis train accu: 0.522948 \n",
      "epoch: 94 gen train loss: 0.687265 dis train loss: 0.694731 dis train accu: 0.523445 \n",
      "epoch: 95 gen train loss: 0.686322 dis train loss: 0.694536 dis train accu: 0.522739 \n",
      "epoch: 96 gen train loss: 0.687747 dis train loss: 0.694782 dis train accu: 0.522424 \n",
      "epoch: 97 gen train loss: 0.686992 dis train loss: 0.694674 dis train accu: 0.522451 \n",
      "epoch: 98 gen train loss: 0.687782 dis train loss: 0.694549 dis train accu: 0.523131 \n",
      "epoch: 99 gen train loss: 0.690012 dis train loss: 0.694584 dis train accu: 0.523341 \n",
      "Adversarial training for ST slide 151672: \n",
      "Start adversarial training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55cff2f0f7d4470ea75d656522954254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e207a88acbd94c48832cad8552e894cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 gen train loss: 0.645571 dis train loss: 0.728751 dis train accu: 0.379139 \n",
      "epoch: 1 gen train loss: 0.650909 dis train loss: 0.722513 dis train accu: 0.393138 \n",
      "epoch: 2 gen train loss: 0.656088 dis train loss: 0.718807 dis train accu: 0.40791 \n",
      "epoch: 3 gen train loss: 0.658337 dis train loss: 0.716847 dis train accu: 0.414173 \n",
      "epoch: 4 gen train loss: 0.665344 dis train loss: 0.717411 dis train accu: 0.420412 \n",
      "epoch: 5 gen train loss: 0.676496 dis train loss: 0.715784 dis train accu: 0.424828 \n",
      "epoch: 6 gen train loss: 0.669737 dis train loss: 0.712857 dis train accu: 0.430393 \n",
      "epoch: 7 gen train loss: 0.680742 dis train loss: 0.71313 dis train accu: 0.436282 \n",
      "epoch: 8 gen train loss: 0.678541 dis train loss: 0.713102 dis train accu: 0.436382 \n",
      "epoch: 9 gen train loss: 0.678044 dis train loss: 0.71278 dis train accu: 0.442171 \n",
      "epoch: 10 gen train loss: 0.681862 dis train loss: 0.71147 dis train accu: 0.443868 \n",
      "epoch: 11 gen train loss: 0.684306 dis train loss: 0.711686 dis train accu: 0.447661 \n",
      "epoch: 12 gen train loss: 0.685799 dis train loss: 0.708909 dis train accu: 0.448509 \n",
      "epoch: 13 gen train loss: 0.685643 dis train loss: 0.709351 dis train accu: 0.450655 \n",
      "epoch: 14 gen train loss: 0.690206 dis train loss: 0.70819 dis train accu: 0.452052 \n",
      "epoch: 15 gen train loss: 0.685684 dis train loss: 0.710434 dis train accu: 0.454797 \n",
      "epoch: 16 gen train loss: 0.687535 dis train loss: 0.707491 dis train accu: 0.456694 \n",
      "epoch: 17 gen train loss: 0.691483 dis train loss: 0.709448 dis train accu: 0.455296 \n",
      "epoch: 18 gen train loss: 0.689213 dis train loss: 0.707851 dis train accu: 0.459114 \n",
      "epoch: 19 gen train loss: 0.68867 dis train loss: 0.707061 dis train accu: 0.461485 \n",
      "epoch: 20 gen train loss: 0.688444 dis train loss: 0.707655 dis train accu: 0.463606 \n",
      "epoch: 21 gen train loss: 0.687581 dis train loss: 0.70836 dis train accu: 0.464055 \n",
      "epoch: 22 gen train loss: 0.687743 dis train loss: 0.707779 dis train accu: 0.464454 \n",
      "epoch: 23 gen train loss: 0.691494 dis train loss: 0.705571 dis train accu: 0.4666 \n",
      "epoch: 24 gen train loss: 0.690283 dis train loss: 0.706566 dis train accu: 0.470643 \n",
      "epoch: 25 gen train loss: 0.693872 dis train loss: 0.706665 dis train accu: 0.470343 \n",
      "epoch: 26 gen train loss: 0.691668 dis train loss: 0.70564 dis train accu: 0.470917 \n",
      "epoch: 27 gen train loss: 0.686957 dis train loss: 0.705522 dis train accu: 0.47199 \n",
      "epoch: 28 gen train loss: 0.688608 dis train loss: 0.704504 dis train accu: 0.474386 \n",
      "epoch: 29 gen train loss: 0.68989 dis train loss: 0.705305 dis train accu: 0.472439 \n",
      "epoch: 30 gen train loss: 0.687456 dis train loss: 0.703149 dis train accu: 0.475833 \n",
      "epoch: 31 gen train loss: 0.692085 dis train loss: 0.703693 dis train accu: 0.477555 \n",
      "epoch: 32 gen train loss: 0.69015 dis train loss: 0.703902 dis train accu: 0.477031 \n",
      "epoch: 33 gen train loss: 0.691204 dis train loss: 0.703806 dis train accu: 0.478453 \n",
      "epoch: 34 gen train loss: 0.686668 dis train loss: 0.704091 dis train accu: 0.478952 \n",
      "epoch: 35 gen train loss: 0.688816 dis train loss: 0.703964 dis train accu: 0.478428 \n",
      "epoch: 36 gen train loss: 0.693912 dis train loss: 0.703009 dis train accu: 0.480549 \n",
      "epoch: 37 gen train loss: 0.689979 dis train loss: 0.700975 dis train accu: 0.481597 \n",
      "epoch: 38 gen train loss: 0.692526 dis train loss: 0.701628 dis train accu: 0.482146 \n",
      "epoch: 39 gen train loss: 0.687386 dis train loss: 0.703136 dis train accu: 0.480998 \n",
      "epoch: 40 gen train loss: 0.686543 dis train loss: 0.701042 dis train accu: 0.483593 \n",
      "epoch: 41 gen train loss: 0.687647 dis train loss: 0.701763 dis train accu: 0.483918 \n",
      "epoch: 42 gen train loss: 0.69168 dis train loss: 0.701463 dis train accu: 0.484716 \n",
      "epoch: 43 gen train loss: 0.689669 dis train loss: 0.701559 dis train accu: 0.486288 \n",
      "epoch: 44 gen train loss: 0.691136 dis train loss: 0.701142 dis train accu: 0.486862 \n",
      "epoch: 45 gen train loss: 0.690752 dis train loss: 0.700227 dis train accu: 0.487486 \n",
      "epoch: 46 gen train loss: 0.687527 dis train loss: 0.701028 dis train accu: 0.486663 \n",
      "epoch: 47 gen train loss: 0.690078 dis train loss: 0.700535 dis train accu: 0.489183 \n",
      "epoch: 48 gen train loss: 0.686877 dis train loss: 0.700121 dis train accu: 0.488858 \n",
      "epoch: 49 gen train loss: 0.690289 dis train loss: 0.700129 dis train accu: 0.487711 \n",
      "epoch: 50 gen train loss: 0.691087 dis train loss: 0.699096 dis train accu: 0.488459 \n",
      "epoch: 51 gen train loss: 0.690623 dis train loss: 0.700327 dis train accu: 0.491628 \n",
      "epoch: 52 gen train loss: 0.689016 dis train loss: 0.699547 dis train accu: 0.492227 \n",
      "epoch: 53 gen train loss: 0.690709 dis train loss: 0.699308 dis train accu: 0.490306 \n",
      "epoch: 54 gen train loss: 0.68844 dis train loss: 0.699161 dis train accu: 0.490805 \n",
      "epoch: 55 gen train loss: 0.689249 dis train loss: 0.698789 dis train accu: 0.491329 \n",
      "epoch: 56 gen train loss: 0.688359 dis train loss: 0.699296 dis train accu: 0.492826 \n",
      "epoch: 57 gen train loss: 0.688688 dis train loss: 0.698795 dis train accu: 0.492726 \n",
      "epoch: 58 gen train loss: 0.686985 dis train loss: 0.698883 dis train accu: 0.492027 \n",
      "epoch: 59 gen train loss: 0.688478 dis train loss: 0.698756 dis train accu: 0.493125 \n",
      "epoch: 60 gen train loss: 0.68829 dis train loss: 0.699078 dis train accu: 0.492851 \n",
      "epoch: 61 gen train loss: 0.688669 dis train loss: 0.697588 dis train accu: 0.4935 \n",
      "epoch: 62 gen train loss: 0.688007 dis train loss: 0.698597 dis train accu: 0.493525 \n",
      "epoch: 63 gen train loss: 0.688765 dis train loss: 0.697816 dis train accu: 0.493624 \n",
      "epoch: 64 gen train loss: 0.689584 dis train loss: 0.697281 dis train accu: 0.495271 \n",
      "epoch: 65 gen train loss: 0.686728 dis train loss: 0.698309 dis train accu: 0.494323 \n",
      "epoch: 66 gen train loss: 0.687125 dis train loss: 0.697802 dis train accu: 0.494697 \n",
      "epoch: 67 gen train loss: 0.690357 dis train loss: 0.697051 dis train accu: 0.495321 \n",
      "epoch: 68 gen train loss: 0.686005 dis train loss: 0.698004 dis train accu: 0.495446 \n",
      "epoch: 69 gen train loss: 0.687532 dis train loss: 0.696719 dis train accu: 0.495346 \n",
      "epoch: 70 gen train loss: 0.687536 dis train loss: 0.69792 dis train accu: 0.495271 \n",
      "epoch: 71 gen train loss: 0.686011 dis train loss: 0.697083 dis train accu: 0.495346 \n",
      "epoch: 72 gen train loss: 0.686467 dis train loss: 0.697291 dis train accu: 0.495721 \n",
      "epoch: 73 gen train loss: 0.688265 dis train loss: 0.696998 dis train accu: 0.495496 \n",
      "epoch: 74 gen train loss: 0.688579 dis train loss: 0.696681 dis train accu: 0.496569 \n",
      "epoch: 75 gen train loss: 0.687954 dis train loss: 0.697158 dis train accu: 0.496419 \n",
      "epoch: 76 gen train loss: 0.685637 dis train loss: 0.697353 dis train accu: 0.497093 \n",
      "epoch: 77 gen train loss: 0.687005 dis train loss: 0.696472 dis train accu: 0.496444 \n",
      "epoch: 78 gen train loss: 0.687224 dis train loss: 0.69672 dis train accu: 0.496494 \n",
      "epoch: 79 gen train loss: 0.686623 dis train loss: 0.696605 dis train accu: 0.497717 \n",
      "epoch: 80 gen train loss: 0.685388 dis train loss: 0.69749 dis train accu: 0.497093 \n",
      "epoch: 81 gen train loss: 0.687038 dis train loss: 0.696985 dis train accu: 0.497018 \n",
      "epoch: 82 gen train loss: 0.686198 dis train loss: 0.696503 dis train accu: 0.498091 \n",
      "epoch: 83 gen train loss: 0.688391 dis train loss: 0.695523 dis train accu: 0.497392 \n",
      "epoch: 84 gen train loss: 0.686953 dis train loss: 0.696381 dis train accu: 0.497193 \n",
      "epoch: 85 gen train loss: 0.686504 dis train loss: 0.695859 dis train accu: 0.497692 \n",
      "epoch: 86 gen train loss: 0.686124 dis train loss: 0.695901 dis train accu: 0.497268 \n",
      "epoch: 87 gen train loss: 0.687296 dis train loss: 0.696328 dis train accu: 0.497442 \n",
      "epoch: 88 gen train loss: 0.687299 dis train loss: 0.695904 dis train accu: 0.498465 \n",
      "epoch: 89 gen train loss: 0.686553 dis train loss: 0.69614 dis train accu: 0.497367 \n",
      "epoch: 90 gen train loss: 0.68846 dis train loss: 0.696293 dis train accu: 0.498266 \n",
      "epoch: 91 gen train loss: 0.68633 dis train loss: 0.695423 dis train accu: 0.497642 \n",
      "epoch: 92 gen train loss: 0.686535 dis train loss: 0.69577 dis train accu: 0.497991 \n",
      "epoch: 93 gen train loss: 0.68727 dis train loss: 0.695739 dis train accu: 0.497891 \n",
      "epoch: 94 gen train loss: 0.687137 dis train loss: 0.69599 dis train accu: 0.49864 \n",
      "epoch: 95 gen train loss: 0.68671 dis train loss: 0.695573 dis train accu: 0.497692 \n",
      "epoch: 96 gen train loss: 0.687152 dis train loss: 0.69537 dis train accu: 0.498216 \n",
      "epoch: 97 gen train loss: 0.688177 dis train loss: 0.695889 dis train accu: 0.497817 \n",
      "epoch: 98 gen train loss: 0.685115 dis train loss: 0.695145 dis train accu: 0.498216 \n",
      "epoch: 99 gen train loss: 0.685981 dis train loss: 0.695445 dis train accu: 0.498565 \n",
      "Adversarial training for ST slide 151669: \n",
      "Start adversarial training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50333b1b4814233bee6f6af4c86be5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829e020fe1c2416c9d87bdcdd263c325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 gen train loss: 0.651826 dis train loss: 0.724318 dis train accu: 0.390236 \n",
      "epoch: 1 gen train loss: 0.646084 dis train loss: 0.717851 dis train accu: 0.40825 \n",
      "epoch: 2 gen train loss: 0.658287 dis train loss: 0.71573 dis train accu: 0.419867 \n",
      "epoch: 3 gen train loss: 0.663761 dis train loss: 0.714349 dis train accu: 0.426968 \n",
      "epoch: 4 gen train loss: 0.667496 dis train loss: 0.711922 dis train accu: 0.438141 \n",
      "epoch: 5 gen train loss: 0.674037 dis train loss: 0.713907 dis train accu: 0.438115 \n",
      "epoch: 6 gen train loss: 0.673471 dis train loss: 0.712213 dis train accu: 0.44827 \n",
      "epoch: 7 gen train loss: 0.670821 dis train loss: 0.711293 dis train accu: 0.451534 \n",
      "epoch: 8 gen train loss: 0.676747 dis train loss: 0.7119 dis train accu: 0.453596 \n",
      "epoch: 9 gen train loss: 0.67341 dis train loss: 0.711574 dis train accu: 0.454197 \n",
      "epoch: 10 gen train loss: 0.681215 dis train loss: 0.710306 dis train accu: 0.46088 \n",
      "epoch: 11 gen train loss: 0.683661 dis train loss: 0.709246 dis train accu: 0.465762 \n",
      "epoch: 12 gen train loss: 0.684017 dis train loss: 0.71037 dis train accu: 0.464195 \n",
      "epoch: 13 gen train loss: 0.683348 dis train loss: 0.709699 dis train accu: 0.467328 \n",
      "epoch: 14 gen train loss: 0.69023 dis train loss: 0.707947 dis train accu: 0.470461 \n",
      "epoch: 15 gen train loss: 0.680632 dis train loss: 0.706523 dis train accu: 0.473594 \n",
      "epoch: 16 gen train loss: 0.68432 dis train loss: 0.708946 dis train accu: 0.475029 \n",
      "epoch: 17 gen train loss: 0.685158 dis train loss: 0.707049 dis train accu: 0.478841 \n",
      "epoch: 18 gen train loss: 0.690969 dis train loss: 0.708394 dis train accu: 0.477875 \n",
      "epoch: 19 gen train loss: 0.68852 dis train loss: 0.70603 dis train accu: 0.482522 \n",
      "epoch: 20 gen train loss: 0.692408 dis train loss: 0.706137 dis train accu: 0.483592 \n",
      "epoch: 21 gen train loss: 0.691668 dis train loss: 0.707765 dis train accu: 0.481504 \n",
      "epoch: 22 gen train loss: 0.689581 dis train loss: 0.705772 dis train accu: 0.484898 \n",
      "epoch: 23 gen train loss: 0.688943 dis train loss: 0.704866 dis train accu: 0.488004 \n",
      "epoch: 24 gen train loss: 0.691822 dis train loss: 0.705525 dis train accu: 0.489101 \n",
      "epoch: 25 gen train loss: 0.685358 dis train loss: 0.705376 dis train accu: 0.491842 \n",
      "epoch: 26 gen train loss: 0.691821 dis train loss: 0.706741 dis train accu: 0.487795 \n",
      "epoch: 27 gen train loss: 0.689156 dis train loss: 0.703827 dis train accu: 0.490406 \n",
      "epoch: 28 gen train loss: 0.687789 dis train loss: 0.703952 dis train accu: 0.492782 \n",
      "epoch: 29 gen train loss: 0.693843 dis train loss: 0.705576 dis train accu: 0.494896 \n",
      "epoch: 30 gen train loss: 0.693201 dis train loss: 0.702491 dis train accu: 0.494844 \n",
      "epoch: 31 gen train loss: 0.690795 dis train loss: 0.703497 dis train accu: 0.495862 \n",
      "epoch: 32 gen train loss: 0.687529 dis train loss: 0.703036 dis train accu: 0.49581 \n",
      "epoch: 33 gen train loss: 0.692631 dis train loss: 0.703264 dis train accu: 0.497768 \n",
      "epoch: 34 gen train loss: 0.689304 dis train loss: 0.702344 dis train accu: 0.499204 \n",
      "epoch: 35 gen train loss: 0.69175 dis train loss: 0.702315 dis train accu: 0.501527 \n",
      "epoch: 36 gen train loss: 0.691684 dis train loss: 0.701862 dis train accu: 0.497951 \n",
      "epoch: 37 gen train loss: 0.689043 dis train loss: 0.701454 dis train accu: 0.501945 \n",
      "epoch: 38 gen train loss: 0.69171 dis train loss: 0.701196 dis train accu: 0.502806 \n",
      "epoch: 39 gen train loss: 0.690851 dis train loss: 0.701471 dis train accu: 0.504294 \n",
      "epoch: 40 gen train loss: 0.692503 dis train loss: 0.70061 dis train accu: 0.505626 \n",
      "epoch: 41 gen train loss: 0.691172 dis train loss: 0.700657 dis train accu: 0.505365 \n",
      "epoch: 42 gen train loss: 0.688 dis train loss: 0.701042 dis train accu: 0.50654 \n",
      "epoch: 43 gen train loss: 0.692483 dis train loss: 0.700892 dis train accu: 0.50654 \n",
      "epoch: 44 gen train loss: 0.691167 dis train loss: 0.700309 dis train accu: 0.507062 \n",
      "epoch: 45 gen train loss: 0.69073 dis train loss: 0.700763 dis train accu: 0.508811 \n",
      "epoch: 46 gen train loss: 0.688627 dis train loss: 0.701061 dis train accu: 0.508028 \n",
      "epoch: 47 gen train loss: 0.686674 dis train loss: 0.699775 dis train accu: 0.50902 \n",
      "epoch: 48 gen train loss: 0.690455 dis train loss: 0.69979 dis train accu: 0.508263 \n",
      "epoch: 49 gen train loss: 0.688791 dis train loss: 0.699765 dis train accu: 0.510064 \n",
      "epoch: 50 gen train loss: 0.690475 dis train loss: 0.699284 dis train accu: 0.509725 \n",
      "epoch: 51 gen train loss: 0.689339 dis train loss: 0.69907 dis train accu: 0.510194 \n",
      "epoch: 52 gen train loss: 0.68849 dis train loss: 0.699878 dis train accu: 0.512074 \n",
      "epoch: 53 gen train loss: 0.6858 dis train loss: 0.69938 dis train accu: 0.511604 \n",
      "epoch: 54 gen train loss: 0.693838 dis train loss: 0.699303 dis train accu: 0.512492 \n",
      "epoch: 55 gen train loss: 0.686893 dis train loss: 0.699448 dis train accu: 0.511604 \n",
      "epoch: 56 gen train loss: 0.689877 dis train loss: 0.698757 dis train accu: 0.512988 \n",
      "epoch: 57 gen train loss: 0.688782 dis train loss: 0.698485 dis train accu: 0.514789 \n",
      "epoch: 58 gen train loss: 0.686782 dis train loss: 0.698703 dis train accu: 0.515337 \n",
      "epoch: 59 gen train loss: 0.688154 dis train loss: 0.698138 dis train accu: 0.514606 \n",
      "epoch: 60 gen train loss: 0.689873 dis train loss: 0.697837 dis train accu: 0.516225 \n",
      "epoch: 61 gen train loss: 0.68806 dis train loss: 0.698463 dis train accu: 0.515938 \n",
      "epoch: 62 gen train loss: 0.68935 dis train loss: 0.697813 dis train accu: 0.516303 \n",
      "epoch: 63 gen train loss: 0.690105 dis train loss: 0.697893 dis train accu: 0.516121 \n",
      "epoch: 64 gen train loss: 0.690769 dis train loss: 0.698107 dis train accu: 0.516042 \n",
      "epoch: 65 gen train loss: 0.687562 dis train loss: 0.69762 dis train accu: 0.516199 \n",
      "epoch: 66 gen train loss: 0.691685 dis train loss: 0.697724 dis train accu: 0.517922 \n",
      "epoch: 67 gen train loss: 0.689068 dis train loss: 0.697848 dis train accu: 0.516251 \n",
      "epoch: 68 gen train loss: 0.690058 dis train loss: 0.696481 dis train accu: 0.517818 \n",
      "epoch: 69 gen train loss: 0.686254 dis train loss: 0.697362 dis train accu: 0.517791 \n",
      "epoch: 70 gen train loss: 0.687838 dis train loss: 0.696556 dis train accu: 0.518522 \n",
      "epoch: 71 gen train loss: 0.686606 dis train loss: 0.696514 dis train accu: 0.51787 \n",
      "epoch: 72 gen train loss: 0.685284 dis train loss: 0.696436 dis train accu: 0.517922 \n",
      "epoch: 73 gen train loss: 0.68877 dis train loss: 0.697004 dis train accu: 0.519332 \n",
      "epoch: 74 gen train loss: 0.688203 dis train loss: 0.697076 dis train accu: 0.518418 \n",
      "epoch: 75 gen train loss: 0.688776 dis train loss: 0.696958 dis train accu: 0.519488 \n",
      "epoch: 76 gen train loss: 0.688843 dis train loss: 0.696213 dis train accu: 0.519619 \n",
      "epoch: 77 gen train loss: 0.688584 dis train loss: 0.696049 dis train accu: 0.519567 \n",
      "epoch: 78 gen train loss: 0.68872 dis train loss: 0.696463 dis train accu: 0.520402 \n",
      "epoch: 79 gen train loss: 0.68533 dis train loss: 0.696008 dis train accu: 0.519619 \n",
      "epoch: 80 gen train loss: 0.687308 dis train loss: 0.696339 dis train accu: 0.520741 \n",
      "epoch: 81 gen train loss: 0.68844 dis train loss: 0.696181 dis train accu: 0.520976 \n",
      "epoch: 82 gen train loss: 0.688142 dis train loss: 0.695805 dis train accu: 0.520585 \n",
      "epoch: 83 gen train loss: 0.68772 dis train loss: 0.696091 dis train accu: 0.520559 \n",
      "epoch: 84 gen train loss: 0.689835 dis train loss: 0.695195 dis train accu: 0.520715 \n",
      "epoch: 85 gen train loss: 0.686316 dis train loss: 0.695139 dis train accu: 0.520141 \n",
      "epoch: 86 gen train loss: 0.687482 dis train loss: 0.696595 dis train accu: 0.520846 \n",
      "epoch: 87 gen train loss: 0.688 dis train loss: 0.69543 dis train accu: 0.520559 \n",
      "epoch: 88 gen train loss: 0.688692 dis train loss: 0.695572 dis train accu: 0.520533 \n",
      "epoch: 89 gen train loss: 0.686519 dis train loss: 0.69523 dis train accu: 0.52095 \n",
      "epoch: 90 gen train loss: 0.68904 dis train loss: 0.695196 dis train accu: 0.520924 \n",
      "epoch: 91 gen train loss: 0.688171 dis train loss: 0.695426 dis train accu: 0.52095 \n",
      "epoch: 92 gen train loss: 0.688224 dis train loss: 0.694818 dis train accu: 0.52082 \n",
      "epoch: 93 gen train loss: 0.688946 dis train loss: 0.694988 dis train accu: 0.521002 \n",
      "epoch: 94 gen train loss: 0.688277 dis train loss: 0.694783 dis train accu: 0.521525 \n",
      "epoch: 95 gen train loss: 0.688059 dis train loss: 0.695075 dis train accu: 0.521211 \n",
      "epoch: 96 gen train loss: 0.689225 dis train loss: 0.694581 dis train accu: 0.521342 \n",
      "epoch: 97 gen train loss: 0.68747 dis train loss: 0.695488 dis train accu: 0.52142 \n",
      "epoch: 98 gen train loss: 0.685683 dis train loss: 0.695253 dis train accu: 0.521055 \n",
      "epoch: 99 gen train loss: 0.687592 dis train loss: 0.694636 dis train accu: 0.521394 \n"
     ]
    }
   ],
   "source": [
    "if TRAIN_USING_ALL_ST_SAMPLES:\n",
    "    print(f\"Adversarial training for all ST slides\")\n",
    "    save_folder = advtrain_folder\n",
    "\n",
    "    best_checkpoint = torch.load(os.path.join(pretrain_folder, f\"final_model.pth\"))\n",
    "    model = best_checkpoint[\"model\"]\n",
    "    model.to(device)\n",
    "    model.advtraining()\n",
    "\n",
    "    train_adversarial_iters(\n",
    "        model,\n",
    "        save_folder,\n",
    "        dataloader_source_train,\n",
    "        dataloader_source_val,\n",
    "        dataloader_target_train,\n",
    "        dataloader_target_train_dis\n",
    "    )\n",
    "\n",
    "else:\n",
    "    for sample_id in st_sample_id_l:\n",
    "        print(f\"Adversarial training for ST slide {sample_id}: \")\n",
    "\n",
    "        save_folder = os.path.join(advtrain_folder, sample_id)\n",
    "        if not os.path.isdir(save_folder):\n",
    "            os.makedirs(save_folder)\n",
    "\n",
    "        best_checkpoint = torch.load(os.path.join(pretrain_folder, f\"final_model.pth\"))\n",
    "        model = best_checkpoint[\"model\"]\n",
    "        model.to(device)\n",
    "        model.advtraining()\n",
    "\n",
    "        train_adversarial_iters(\n",
    "            model,\n",
    "            save_folder,\n",
    "            dataloader_source_train,\n",
    "            dataloader_source_val,\n",
    "            dataloader_target_train_d[sample_id],\n",
    "            dataloader_target_train_dis_d[sample_id]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn import model_selection\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# for sample_id in st_sample_id_l:\n",
    "#     best_checkpoint = torch.load(\n",
    "#         os.path.join(advtrain_folder, sample_id, f\"final_model.pth\")\n",
    "#     )\n",
    "#     model = best_checkpoint[\"model\"]\n",
    "#     model.to(device)\n",
    "\n",
    "#     model.eval()\n",
    "#     model.target_inference()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         source_emb = model.source_encoder(torch.Tensor(sc_mix_train_s).to(device))\n",
    "#         target_emb = model.target_encoder(\n",
    "#             torch.Tensor(mat_sp_test_s_d[sample_id]).to(device)\n",
    "#         )\n",
    "\n",
    "#         y_dis = torch.cat(\n",
    "#             [\n",
    "#                 torch.zeros(source_emb.shape[0], device=device, dtype=torch.long),\n",
    "#                 torch.ones(target_emb.shape[0], device=device, dtype=torch.long),\n",
    "#             ]\n",
    "#         )\n",
    "\n",
    "#         emb = torch.cat([source_emb, target_emb])\n",
    "\n",
    "#         emb = emb.detach().cpu().numpy()\n",
    "#         y_dis = y_dis.detach().cpu().numpy()\n",
    "\n",
    "#     (emb_train, emb_test, y_dis_train, y_dis_test,) = model_selection.train_test_split(\n",
    "#         emb,\n",
    "#         y_dis,\n",
    "#         test_size=0.2,\n",
    "#         random_state=225,\n",
    "#         stratify=y_dis,\n",
    "#     )\n",
    "\n",
    "#     pca = PCA(n_components=50)\n",
    "#     pca.fit(emb_train)\n",
    "\n",
    "#     emb_train_50 = pca.transform(emb_train)\n",
    "#     emb_test_50 = pca.transform(emb_test)\n",
    "\n",
    "#     clf = RandomForestClassifier(random_state=145, n_jobs=-1)\n",
    "#     clf.fit(emb_train_50, y_dis_train)\n",
    "#     accu_train = clf.score(emb_train_50, y_dis_train)\n",
    "#     accu_test = clf.score(emb_test_50, y_dis_test)\n",
    "#     class_proportions = np.mean(y_dis)\n",
    "\n",
    "#     print(\n",
    "#         \"Training accuracy: {}, Test accuracy: {}, Class proportions: {}\".format(\n",
    "#             accu_train, accu_test, class_proportions\n",
    "#         )\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 4. Predict cell fraction of spots and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_sp_d, pred_sp_noda_d = {}, {}\n",
    "# if TRAIN_USING_ALL_ST_SAMPLES:\n",
    "#     best_checkpoint = torch.load(os.path.join(advtrain_folder, f\"final_model.pth\"))\n",
    "#     model = best_checkpoint[\"model\"]\n",
    "#     model.to(device)\n",
    "\n",
    "#     model.eval()\n",
    "#     model.target_inference()\n",
    "#     with torch.no_grad():\n",
    "#         for sample_id in st_sample_id_l:\n",
    "#             pred_sp_d[sample_id] = (\n",
    "#                 torch.exp(\n",
    "#                     model(torch.Tensor(mat_sp_test_s_d[sample_id]).to(device))\n",
    "#                 )\n",
    "#                 .detach()\n",
    "#                 .cpu()\n",
    "#                 .numpy()\n",
    "#             )\n",
    "\n",
    "# else:\n",
    "#     for sample_id in st_sample_id_l:\n",
    "#         best_checkpoint = torch.load(\n",
    "#             os.path.join(advtrain_folder, sample_id, f\"final_model.pth\")\n",
    "#         )\n",
    "#         model = best_checkpoint[\"model\"]\n",
    "#         model.to(device)\n",
    "\n",
    "#         model.eval()\n",
    "#         model.target_inference()\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             pred_sp_d[sample_id] = (\n",
    "#                 torch.exp(\n",
    "#                     model(torch.Tensor(mat_sp_test_s_d[sample_id]).to(device))\n",
    "#                 )\n",
    "#                 .detach()\n",
    "#                 .cpu()\n",
    "#                 .numpy()\n",
    "#             )\n",
    "\n",
    "\n",
    "# best_checkpoint = torch.load(os.path.join(pretrain_folder, f\"best_model.pth\"))\n",
    "# model = best_checkpoint[\"model\"]\n",
    "# model.to(device)\n",
    "\n",
    "# model.eval()\n",
    "# model.set_encoder(\"source\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for sample_id in st_sample_id_l:\n",
    "#         pred_sp_noda_d[sample_id] = (\n",
    "#             torch.exp(model(torch.Tensor(mat_sp_test_s_d[sample_id]).to(device)))\n",
    "#             .detach()\n",
    "#             .cpu()\n",
    "#             .numpy()\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata_spatialLIBD = sc.read_h5ad(\n",
    "#     os.path.join(PROCESSED_DATA_DIR, \"adata_spatialLIBD.h5ad\")\n",
    "# )\n",
    "\n",
    "# adata_spatialLIBD_d = {}\n",
    "# for sample_id in st_sample_id_l:\n",
    "#     adata_spatialLIBD_d[sample_id] = adata_spatialLIBD[\n",
    "#         adata_spatialLIBD.obs.sample_id == sample_id\n",
    "#     ]\n",
    "#     adata_spatialLIBD_d[sample_id].obsm[\"spatial\"] = (\n",
    "#         adata_spatialLIBD_d[sample_id].obs[[\"X\", \"Y\"]].values\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_name_exN_l = []\n",
    "# for k, v in sc_sub_dict.items():\n",
    "#     if \"Ex\" in v:\n",
    "#         num_name_exN_l.append((k, v, int(v.split(\"_\")[1])))\n",
    "# num_name_exN_l.sort(key=lambda a: a[2])\n",
    "# num_name_exN_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex_to_L_d = {\n",
    "#     1: {5, 6},\n",
    "#     2: {5},\n",
    "#     3: {4, 5},\n",
    "#     4: {6},\n",
    "#     5: {5},\n",
    "#     6: {4, 5, 6},\n",
    "#     7: {4, 5, 6},\n",
    "#     8: {5, 6},\n",
    "#     9: {5, 6},\n",
    "#     10: {2, 3, 4},\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numlist = [t[0] for t in num_name_exN_l]\n",
    "# Ex_l = [t[2] for t in num_name_exN_l]\n",
    "# num_to_ex_d = dict(zip(numlist, Ex_l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_cellfraction(visnum, adata, pred_sp, ax=None):\n",
    "#     \"\"\"Plot predicted cell fraction for a given visnum\"\"\"\n",
    "#     adata.obs[\"Pred_label\"] = pred_sp[:, visnum]\n",
    "#     # vmin = 0\n",
    "#     # vmax = np.amax(pred_sp)\n",
    "\n",
    "#     sc.pl.spatial(\n",
    "#         adata,\n",
    "#         img_key=\"hires\",\n",
    "#         color=\"Pred_label\",\n",
    "#         palette=\"Set1\",\n",
    "#         size=1.5,\n",
    "#         legend_loc=None,\n",
    "#         title=f\"{sc_sub_dict[visnum]}\",\n",
    "#         spot_size=100,\n",
    "#         show=False,\n",
    "#         # vmin=vmin,\n",
    "#         # vmax=vmax,\n",
    "#         ax=ax,\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_roc(visnum, adata, pred_sp, name, ax=None):\n",
    "#     \"\"\"Plot ROC for a given visnum\"\"\"\n",
    "\n",
    "#     def layer_to_layer_number(x):\n",
    "#         for char in x:\n",
    "#             if char.isdigit():\n",
    "#                 if int(char) in Ex_to_L_d[num_to_ex_d[visnum]]:\n",
    "#                     return 1\n",
    "#         return 0\n",
    "\n",
    "#     y_pred = pred_sp[:, visnum]\n",
    "#     y_true = adata.obs[\"spatialLIBD\"].map(layer_to_layer_number).fillna(0)\n",
    "#     # print(y_true)\n",
    "#     # print(y_true.isna().sum())\n",
    "#     RocCurveDisplay.from_predictions(y_true=y_true, y_pred=y_pred, name=name, ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 5), constrained_layout=True)\n",
    "\n",
    "# sc.pl.spatial(\n",
    "#     adata_spatialLIBD_d[SAMPLE_ID_N],\n",
    "#     img_key=None,\n",
    "#     color=\"spatialLIBD\",\n",
    "#     palette=\"Accent_r\",\n",
    "#     size=1.5,\n",
    "#     title=SAMPLE_ID_N,\n",
    "#     # legend_loc = 4,\n",
    "#     spot_size=100,\n",
    "#     show=False,\n",
    "#     ax=ax,\n",
    "# )\n",
    "\n",
    "# ax.axis(\"equal\")\n",
    "# ax.set_xlabel(\"\")\n",
    "# ax.set_ylabel(\"\")\n",
    "\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(2, 5, figsize=(20, 8), constrained_layout=True)\n",
    "\n",
    "# for i, num in enumerate(numlist):\n",
    "#     plot_cellfraction(\n",
    "#         num, adata_spatialLIBD_d[SAMPLE_ID_N], pred_sp_d[SAMPLE_ID_N], ax.flat[i]\n",
    "#     )\n",
    "#     ax.flat[i].axis(\"equal\")\n",
    "#     ax.flat[i].set_xlabel(\"\")\n",
    "#     ax.flat[i].set_ylabel(\"\")\n",
    "\n",
    "# fig.show()\n",
    "\n",
    "# fig, ax = plt.subplots(\n",
    "#     2, 5, figsize=(20, 8), constrained_layout=True, sharex=True, sharey=True\n",
    "# )\n",
    "\n",
    "# for i, num in enumerate(numlist):\n",
    "#     plot_roc(\n",
    "#         num,\n",
    "#         adata_spatialLIBD_d[SAMPLE_ID_N],\n",
    "#         pred_sp_d[SAMPLE_ID_N],\n",
    "#         \"ADDA\",\n",
    "#         ax.flat[i],\n",
    "#     )\n",
    "#     plot_roc(\n",
    "#         num,\n",
    "#         adata_spatialLIBD_d[SAMPLE_ID_N],\n",
    "#         pred_sp_noda_d[SAMPLE_ID_N],\n",
    "#         \"NN_wo_da\",\n",
    "#         ax.flat[i],\n",
    "#     )\n",
    "#     ax.flat[i].plot([0, 1], [0, 1], transform=ax.flat[i].transAxes, ls=\"--\", color=\"k\")\n",
    "#     ax.flat[i].set_aspect(\"equal\")\n",
    "#     ax.flat[i].set_xlim([0, 1])\n",
    "#     ax.flat[i].set_ylim([0, 1])\n",
    "\n",
    "#     ax.flat[i].set_title(f\"{sc_sub_dict[num]}\")\n",
    "\n",
    "#     if i >= len(numlist) - 5:\n",
    "#         ax.flat[i].set_xlabel(\"FPR\")\n",
    "#     else:\n",
    "#         ax.flat[i].set_xlabel(\"\")\n",
    "#     if i % 5 == 0:\n",
    "#         ax.flat[i].set_ylabel(\"TPR\")\n",
    "#     else:\n",
    "#         ax.flat[i].set_ylabel(\"\")\n",
    "\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if TRAIN_USING_ALL_ST_SAMPLES:\n",
    "#     best_checkpoint = torch.load(os.path.join(advtrain_folder, f\"final_model.pth\"))\n",
    "# else:\n",
    "#     best_checkpoint = torch.load(\n",
    "#         os.path.join(advtrain_folder, SAMPLE_ID_N, f\"final_model.pth\")\n",
    "#     )\n",
    "\n",
    "# model = best_checkpoint[\"model\"]\n",
    "# model.to(device)\n",
    "\n",
    "# model.eval()\n",
    "# model.set_encoder(\"source\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     pred_mix = (\n",
    "#         torch.exp(model(torch.Tensor(sc_mix_test_s).to(device)))\n",
    "#         .detach()\n",
    "#         .cpu()\n",
    "#         .numpy()\n",
    "#     )\n",
    "\n",
    "# cell_type_nums = sc_sub_dict.keys()\n",
    "# nrows = ceil(len(cell_type_nums) / 5)\n",
    "\n",
    "# line_kws = {\"color\": \"tab:orange\"}\n",
    "# scatter_kws = {\"s\": 5}\n",
    "\n",
    "# props = dict(facecolor=\"w\", alpha=0.5)\n",
    "\n",
    "# fig, ax = plt.subplots(\n",
    "#     nrows,\n",
    "#     5,\n",
    "#     figsize=(25, 5 * nrows),\n",
    "#     constrained_layout=True,\n",
    "#     sharex=False,\n",
    "#     sharey=True,\n",
    "# )\n",
    "# for i, visnum in enumerate(cell_type_nums):\n",
    "#     sns.regplot(\n",
    "#         x=pred_mix[:, visnum],\n",
    "#         y=lab_mix_test[:, visnum],\n",
    "#         line_kws=line_kws,\n",
    "#         scatter_kws=scatter_kws,\n",
    "#         ax=ax.flat[i],\n",
    "#     ).set_title(sc_sub_dict[visnum])\n",
    "\n",
    "#     ax.flat[i].set_aspect(\"equal\")\n",
    "#     ax.flat[i].set_xlabel(\"Predicted Proportion\")\n",
    "\n",
    "#     if i % 5 == 0:\n",
    "#         ax.flat[i].set_ylabel(\"True Proportion\")\n",
    "#     else:\n",
    "#         ax.flat[i].set_ylabel(\"\")\n",
    "#     ax.flat[i].set_xlim([0, 1])\n",
    "#     ax.flat[i].set_ylim([0, 1])\n",
    "\n",
    "#     textstr = (\n",
    "#         f\"MSE: {mean_squared_error(pred_mix[:,visnum], lab_mix_test[:,visnum]):.5f}\"\n",
    "#     )\n",
    "\n",
    "#     # place a text box in upper left in axes coords\n",
    "#     ax.flat[i].text(\n",
    "#         0.95,\n",
    "#         0.05,\n",
    "#         textstr,\n",
    "#         transform=ax.flat[i].transAxes,\n",
    "#         verticalalignment=\"bottom\",\n",
    "#         horizontalalignment=\"right\",\n",
    "#         bbox=props,\n",
    "#     )\n",
    "\n",
    "# for i in range(len(cell_type_nums), nrows * 5):\n",
    "#     ax.flat[i].axis(\"off\")\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('agreda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c8a91b640e5c43bacdcbf87782ad770b561ed71a46153862bbc20bda0bebf44f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
