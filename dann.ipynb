{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # DANN for ST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Creating something like CellDART but it actually follows Adda in PyTorch as a first step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "from itertools import count\n",
    "import warnings\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "from src.da_models.dann import DANN\n",
    "from src.da_models.datasets import SpotDataset\n",
    "from src.da_models.utils import set_requires_grad\n",
    "from src.da_models.utils import initialize_weights\n",
    "from src.utils.dupstdout import DupStdout\n",
    "\n",
    "\n",
    "# datetime object containing current date and time\n",
    "script_start_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%Hh%Mm%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cpu\":\n",
    "    warnings.warn(\"Using CPU\", stacklevel=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_USING_ALL_ST_SAMPLES = False\n",
    "\n",
    "SAMPLE_ID_N = \"151673\"\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "PRETRAINING = False\n",
    "INITIAL_TRAIN_LR = 0.002\n",
    "NUM_WORKERS = 4\n",
    "INITIAL_TRAIN_EPOCHS = 100\n",
    "\n",
    "\n",
    "MIN_EPOCHS = 0.4 * INITIAL_TRAIN_EPOCHS\n",
    "EARLY_STOP_CRIT = INITIAL_TRAIN_EPOCHS\n",
    "\n",
    "PROCESSED_DATA_DIR = \"data/preprocessed\"\n",
    "\n",
    "MODEL_NAME = \"DANN\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adversarial Hyperparameters\n",
    "\n",
    "EPOCHS = 200\n",
    "MIN_EPOCHS_ADV = 0.4 * EPOCHS\n",
    "EARLY_STOP_CRIT_ADV = EPOCHS\n",
    "ADV_LR = 1e-3\n",
    "\n",
    "ALPHA = 0.1\n",
    "LAMBDA = 1\n",
    "TWO_STEP = True\n",
    "\n",
    "adv_opt_kwargs = {\"lr\":ADV_LR, \"betas\":(0.5, 0.999), \"eps\":1e-07}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = os.path.join(\"model\", MODEL_NAME, script_start_time)\n",
    "\n",
    "model_folder = os.path.join(\"model\", MODEL_NAME, \"V1\")\n",
    "\n",
    "if not os.path.isdir(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "    print(model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spatial data\n",
    "mat_sp_test_s_d = {}\n",
    "with h5py.File(os.path.join(PROCESSED_DATA_DIR, \"mat_sp_test_s_d.hdf5\"), \"r\") as f:\n",
    "    for sample_id in f:\n",
    "        mat_sp_test_s_d[sample_id] = f[sample_id][()]\n",
    "\n",
    "if TRAIN_USING_ALL_ST_SAMPLES:\n",
    "    with h5py.File(os.path.join(PROCESSED_DATA_DIR, \"mat_sp_train_s.hdf5\"), \"r\") as f:\n",
    "        mat_sp_train_s = f[\"all\"][()]\n",
    "else:\n",
    "    mat_sp_train_s_d = mat_sp_test_s_d\n",
    "\n",
    "# Load sc data\n",
    "with h5py.File(os.path.join(PROCESSED_DATA_DIR, \"sc.hdf5\"), \"r\") as f:\n",
    "    sc_mix_train_s = f[\"X/train\"][()]\n",
    "    sc_mix_val_s = f[\"X/val\"][()]\n",
    "    sc_mix_test_s = f[\"X/test\"][()]\n",
    "\n",
    "    lab_mix_train = f[\"y/train\"][()]\n",
    "    lab_mix_val = f[\"y/val\"][()]\n",
    "    lab_mix_test = f[\"y/test\"][()]\n",
    "\n",
    "# Load helper dicts / lists\n",
    "with open(os.path.join(PROCESSED_DATA_DIR, \"sc_sub_dict.pkl\"), \"rb\") as f:\n",
    "    sc_sub_dict = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(PROCESSED_DATA_DIR, \"sc_sub_dict2.pkl\"), \"rb\") as f:\n",
    "    sc_sub_dict2 = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(PROCESSED_DATA_DIR, \"st_sample_id_l.pkl\"), \"rb\") as f:\n",
    "    st_sample_id_l = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Training: Adversarial domain adaptation for cell fraction estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### source dataloaders\n",
    "source_train_set = SpotDataset(sc_mix_train_s, lab_mix_train)\n",
    "source_val_set = SpotDataset(sc_mix_val_s, lab_mix_val)\n",
    "source_test_set = SpotDataset(sc_mix_test_s, lab_mix_test)\n",
    "\n",
    "dataloader_source_train = torch.utils.data.DataLoader(\n",
    "    source_train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False,\n",
    ")\n",
    "dataloader_source_val = torch.utils.data.DataLoader(\n",
    "    source_val_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False,\n",
    ")\n",
    "dataloader_source_test = torch.utils.data.DataLoader(\n",
    "    source_test_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "### target dataloaders\n",
    "target_test_set_d = {}\n",
    "for sample_id in st_sample_id_l:\n",
    "    target_test_set_d[sample_id] = SpotDataset(mat_sp_test_s_d[sample_id])\n",
    "\n",
    "dataloader_target_test_d = {}\n",
    "for sample_id in st_sample_id_l:\n",
    "    dataloader_target_test_d[sample_id] = torch.utils.data.DataLoader(\n",
    "        target_test_set_d[sample_id],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "if TRAIN_USING_ALL_ST_SAMPLES:\n",
    "    target_train_set = SpotDataset(mat_sp_train_s)\n",
    "    dataloader_target_train = torch.utils.data.DataLoader(\n",
    "        target_train_set,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    target_train_set_dis = SpotDataset(deepcopy(mat_sp_train_s))\n",
    "    dataloader_target_train_dis = torch.utils.data.DataLoader(\n",
    "        target_train_set_dis,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "else:\n",
    "    target_train_set_d = {}\n",
    "    dataloader_target_train_d = {}\n",
    "\n",
    "    target_train_set_dis_d = {}\n",
    "    dataloader_target_train_dis_d = {}\n",
    "    for sample_id in st_sample_id_l:\n",
    "        target_train_set_d[sample_id] = SpotDataset(\n",
    "            deepcopy(mat_sp_test_s_d[sample_id])\n",
    "        )\n",
    "        dataloader_target_train_d[sample_id] = torch.utils.data.DataLoader(\n",
    "            target_train_set_d[sample_id],\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "\n",
    "        target_train_set_dis_d[sample_id] = SpotDataset(\n",
    "            deepcopy(mat_sp_test_s_d[sample_id])\n",
    "        )\n",
    "        dataloader_target_train_dis_d[sample_id] = torch.utils.data.DataLoader(\n",
    "            target_train_set_dis_d[sample_id],\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=False,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_clf = nn.KLDivLoss(reduction=\"batchmean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_iters(nested_list, startpoint = False, endpoint = True, ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        ax = plt\n",
    "\n",
    "    x = []\n",
    "    if startpoint:\n",
    "        for i, l in enumerate(nested_list):\n",
    "            if endpoint and i == len(nested_list) - 1:\n",
    "                x_i = np.linspace(i-1, i, len(l), endpoint=True, dtype=np.float32)\n",
    "            else:\n",
    "                x_i = np.linspace(i-1, i, len(l), endpoint=False, dtype=np.float32)\n",
    "            x.append(x_i)\n",
    "    else:\n",
    "        for i, l in enumerate(nested_list):\n",
    "            if not endpoint and i == len(nested_list) - 1:\n",
    "                x_i = np.linspace(i, i-1, len(l+1), endpoint=False, dtype=np.float32)\n",
    "                x_i = x_i[1:]\n",
    "            else:\n",
    "                x_i = np.linspace(i, i-1, len(l), endpoint=False, dtype=np.float32)\n",
    "            \n",
    "            # Flip to not include startpoint i.e. shift to end of iteration\n",
    "            x_i = np.flip(x_i)\n",
    "            x.append(x_i)\n",
    "\n",
    "    x = np.asarray(list(itertools.chain(*x)))\n",
    "    y = np.asarray(list(itertools.chain(*nested_list)))\n",
    "    ax.plot(x, y, **kwargs)\n",
    "\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(x, y_true, model):\n",
    "    x = x.to(torch.float32).to(device)\n",
    "    y_true = y_true.to(torch.float32).to(device)\n",
    "\n",
    "    y_pred, _ = model(x)\n",
    "\n",
    "    loss = criterion_clf(y_pred, y_true)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_acc(dataloader, model):\n",
    "    loss_running = []\n",
    "    mean_weights = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(dataloader):\n",
    "\n",
    "            loss = model_loss(*batch, model)\n",
    "\n",
    "            loss_running.append(loss.item())\n",
    "\n",
    "            # we will weight average by batch size later\n",
    "            mean_weights.append(len(batch))\n",
    "\n",
    "    return np.average(loss_running, weights=mean_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRETRAINING:\n",
    "\n",
    "    pretrain_folder = os.path.join(model_folder, \"pretrain\")\n",
    "\n",
    "    model = DANN(\n",
    "        sc_mix_train_s.shape[1],\n",
    "        emb_dim=64,\n",
    "        ncls_source=lab_mix_train.shape[1],\n",
    "        alpha_=ALPHA,\n",
    "    )\n",
    "    model.apply(initialize_weights)\n",
    "    model.to(device)\n",
    "\n",
    "    pre_optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=INITIAL_TRAIN_LR, betas=(0.9, 0.999), eps=1e-07\n",
    "    )\n",
    "\n",
    "    pre_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        pre_optimizer,\n",
    "        max_lr=INITIAL_TRAIN_LR,\n",
    "        steps_per_epoch=len(dataloader_source_train),\n",
    "        epochs=INITIAL_TRAIN_EPOCHS,\n",
    "    )\n",
    "\n",
    "    model.pretraining()\n",
    "\n",
    "    if not os.path.isdir(pretrain_folder):\n",
    "        os.makedirs(pretrain_folder)\n",
    "    # Initialize lists to store loss and accuracy values\n",
    "    loss_history = []\n",
    "    loss_history_val = []\n",
    "\n",
    "    loss_history_running = []\n",
    "\n",
    "    lr_history_running = []\n",
    "\n",
    "    # Early Stopping\n",
    "    best_loss_val = np.inf\n",
    "    early_stop_count = 0\n",
    "\n",
    "    # Train\n",
    "    with DupStdout().dup_to_file(\n",
    "        os.path.join(pretrain_folder, \"log.txt\"), \"w\"\n",
    "    ) as f_log:\n",
    "        print(\"Start pretrain...\")\n",
    "        outer = tqdm(total=INITIAL_TRAIN_EPOCHS, desc=\"Epochs\", position=0)\n",
    "        inner = tqdm(total=len(dataloader_source_train), desc=f\"Batch\", position=1)\n",
    "\n",
    "        print(\" Epoch | Train Loss | Val Loss   | Next LR    \")\n",
    "        print(\"----------------------------------------------\")\n",
    "        checkpoint = {\n",
    "            \"epoch\": -1,\n",
    "            \"model\": model,\n",
    "            \"optimizer\": pre_optimizer,\n",
    "            \"scheduler\": pre_scheduler,\n",
    "            # 'scaler': scaler\n",
    "        }\n",
    "        for epoch in range(INITIAL_TRAIN_EPOCHS):\n",
    "            checkpoint[\"epoch\"] = epoch\n",
    "\n",
    "            # Train mode\n",
    "            model.train()\n",
    "            loss_running = []\n",
    "            mean_weights = []\n",
    "            lr_running = []\n",
    "\n",
    "            inner.refresh()  # force print final state\n",
    "            inner.reset()  # reuse bar\n",
    "            for _, batch in enumerate(dataloader_source_train):\n",
    "                lr_running.append(pre_scheduler.get_last_lr()[-1])\n",
    "\n",
    "                pre_optimizer.zero_grad()\n",
    "                loss = model_loss(*batch, model)\n",
    "                loss_running.append(loss.item())\n",
    "                mean_weights.append(\n",
    "                    len(batch)\n",
    "                )  # we will weight average by batch size later\n",
    "\n",
    "                # scaler.scale(loss).backward()\n",
    "                # scaler.step(optimizer)\n",
    "                # scaler.update()\n",
    "\n",
    "                loss.backward()\n",
    "                pre_optimizer.step()\n",
    "                pre_scheduler.step()\n",
    "\n",
    "                inner.update(1)\n",
    "\n",
    "            loss_history.append(np.average(loss_running, weights=mean_weights))\n",
    "            loss_history_running.append(loss_running)\n",
    "            lr_history_running.append(lr_running)\n",
    "\n",
    "            # Evaluate mode\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                curr_loss_val = compute_acc(dataloader_source_val, model)\n",
    "                loss_history_val.append(curr_loss_val)\n",
    "\n",
    "            # Print the results\n",
    "            outer.update(1)\n",
    "\n",
    "            print(\n",
    "                f\" {epoch:5d}\",\n",
    "                f\"| {loss_history[-1]:<10.8f}\",\n",
    "                f\"| {curr_loss_val:<10.8f}\",\n",
    "                f\"| {pre_scheduler.get_last_lr()[-1]:<10.5}\",\n",
    "                end=\" \",\n",
    "            )\n",
    "            # print(\n",
    "            #     \"epoch:\",\n",
    "            #     epoch,\n",
    "            #     \"train loss:\",\n",
    "            #     round(loss_history[-1], 6),\n",
    "            #     \"validation loss:\",\n",
    "            #     round(loss_history_val[-1], 6),\n",
    "            #     # \"next_lr:\", scheduler.get_last_lr(),\n",
    "            #     end=\" \",\n",
    "            # )\n",
    "            # Save the best weights\n",
    "            if curr_loss_val < best_loss_val:\n",
    "                best_loss_val = curr_loss_val\n",
    "                torch.save(checkpoint, os.path.join(pretrain_folder, f\"best_model.pth\"))\n",
    "                early_stop_count = 0\n",
    "\n",
    "                print(\"<-- new best val loss\")\n",
    "            else:\n",
    "                print(\"\")\n",
    "\n",
    "            # Save checkpoint every 10\n",
    "            if epoch % 10 == 0 or epoch >= INITIAL_TRAIN_EPOCHS - 1:\n",
    "                torch.save(\n",
    "                    checkpoint, os.path.join(pretrain_folder, f\"checkpt{epoch}.pth\")\n",
    "                )\n",
    "\n",
    "            # check to see if validation loss has plateau'd\n",
    "            if early_stop_count >= EARLY_STOP_CRIT and epoch >= MIN_EPOCHS - 1:\n",
    "                print(\n",
    "                    f\"Validation loss plateaued after {early_stop_count} at epoch {epoch}\"\n",
    "                )\n",
    "                torch.save(\n",
    "                    checkpoint, os.path.join(pretrain_folder, f\"earlystop{epoch}.pth\")\n",
    "                )\n",
    "                break\n",
    "\n",
    "            early_stop_count += 1\n",
    "\n",
    "    lr_history_running[-1].append(pre_scheduler.get_last_lr()[-1])\n",
    "\n",
    "    # Save final model\n",
    "    best_checkpoint = torch.load(os.path.join(pretrain_folder, f\"best_model.pth\"))\n",
    "    torch.save(best_checkpoint, os.path.join(pretrain_folder, f\"final_model.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRETRAINING:\n",
    "\n",
    "    best_checkpoint = torch.load(os.path.join(pretrain_folder, f\"final_model.pth\"))\n",
    "\n",
    "    best_epoch = best_checkpoint[\"epoch\"]\n",
    "    best_loss_val = loss_history_val[best_epoch]\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, sharex=True, figsize=(6, 4), layout=\"constrained\")\n",
    "\n",
    "    # loss\n",
    "    # loss_history_running_flat = np.ravel(loss_history_running)\n",
    "    plot_iters(loss_history_running, ax=axs[0], label=\"Training\")\n",
    "    # axs[0].plot(\n",
    "    #     # Flip to not include startpoint i.e. shift to end of iteration\n",
    "    #     np.flip(\n",
    "    #         np.linspace(\n",
    "    #             len(loss_history) - 1,\n",
    "    #             -1,\n",
    "    #             endpoint=False,\n",
    "    #             num=len(loss_history_running_flat),\n",
    "    #             dtype=np.float32\n",
    "    #         )\n",
    "    #     ),\n",
    "    #     loss_history_running_flat,\n",
    "    #     ,\n",
    "    # )\n",
    "    axs[0].plot(loss_history_val, label=\"Validation\")\n",
    "    axs[0].axvline(best_epoch, color=\"tab:green\")\n",
    "\n",
    "    axs[0].set_ylim(bottom=0)\n",
    "    axs[0].grid(which=\"major\")\n",
    "    axs[0].minorticks_on()\n",
    "    axs[0].grid(which=\"minor\", alpha=0.2)\n",
    "\n",
    "    axs[0].text(\n",
    "        x=best_epoch + (2 if best_epoch < len(loss_history) * 0.75 else -2),\n",
    "        y=max(loss_history + loss_history_val) * 0.5,\n",
    "        s=f\"Best val. loss:\\n{best_loss_val:.4f} at epoch {best_epoch}\",\n",
    "        ha=\"left\" if best_epoch < len(loss_history) * 0.75 else \"right\",\n",
    "        size=\"x-small\",\n",
    "    )\n",
    "\n",
    "    # axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_title(\"Cross-Entropy Loss\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    # lr history\n",
    "    iters_by_epoch, lr_history_running_flat = plot_iters(\n",
    "        lr_history_running, startpoint=True, ax=axs[1]\n",
    "    )\n",
    "    # iters_by_epoch = np.linspace(\n",
    "    #     -1,\n",
    "    #     len(loss_history_running)-1,\n",
    "    #     endpoint=True,\n",
    "    #     num=len(np.ravel(lr_history_running)),\n",
    "    #     dtype=np.float32\n",
    "    # )\n",
    "    # axs[1].plot(\n",
    "    #     iters_by_epoch,\n",
    "    #     np.ravel(lr_history_running),\n",
    "    # )\n",
    "    axs[1].axvline(best_checkpoint[\"epoch\"], ymax=2, clip_on=False, color=\"tab:green\")\n",
    "\n",
    "    axs[1].set_ylim(bottom=0)\n",
    "    axs[1].grid(which=\"major\")\n",
    "    axs[1].minorticks_on()\n",
    "    axs[1].grid(which=\"minor\", alpha=0.2)\n",
    "\n",
    "    best_epoch_idx = np.where(iters_by_epoch == best_epoch)[0][0]\n",
    "    axs[1].text(\n",
    "        x=best_epoch + (2 if best_epoch < len(loss_history) * 0.75 else -2),\n",
    "        y=np.median(lr_history_running_flat),\n",
    "        s=f\"LR:\\n{lr_history_running_flat[best_epoch_idx]:.4} at epoch {best_epoch}\",\n",
    "        ha=\"left\" if best_epoch < len(loss_history) * 0.75 else \"right\",\n",
    "        size=\"x-small\",\n",
    "    )\n",
    "\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_title(\"Learning Rate\")\n",
    "\n",
    "    plt.savefig(os.path.join(pretrain_folder, \"train_plots.png\"), bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show(block=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Adversarial Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "advtrain_folder = os.path.join(model_folder, \"advtrain\")\n",
    "\n",
    "if not os.path.isdir(advtrain_folder):\n",
    "    os.makedirs(advtrain_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_dis = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_adv_loss(x_source, x_target, y_source, model, two_step=False, optimizer=None):\n",
    "\n",
    "    if optimizer is not None:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    y_dis_source = torch.zeros(\n",
    "        x_source.shape[0], device=device, dtype=x_source.dtype\n",
    "    ).view(-1, 1)\n",
    "    y_clf, y_dis_source_pred = model(x_source, clf=True)\n",
    "    loss_clf = criterion_clf(y_clf, y_source)\n",
    "    loss_dis_source = criterion_dis(y_dis_source_pred, y_dis_source)\n",
    "\n",
    "    if two_step and optimizer is not None:\n",
    "        loss = loss_clf + loss_dis_source / LAMBDA\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    y_dis_target = torch.ones(\n",
    "        x_target.shape[0], device=device, dtype=x_target.dtype\n",
    "    ).view(-1, 1)\n",
    "    _, y_dis_target_pred = model(x_target, clf=False)\n",
    "    loss_dis_target = criterion_dis(y_dis_target_pred, y_dis_target)\n",
    "\n",
    "    if two_step and optimizer is not None:\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_dis_target / LAMBDA\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    elif optimizer is not None:\n",
    "        loss = loss_clf + (loss_dis_source + loss_dis_target) / LAMBDA\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    accu_source = torch.mean(\n",
    "        (\n",
    "            torch.round(y_dis_source_pred.detach()).to(torch.long)\n",
    "            == y_dis_source.detach()\n",
    "        ).to(torch.float32)\n",
    "    ).cpu()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    accu_target = torch.mean(\n",
    "        (\n",
    "            torch.round(y_dis_target_pred.detach()).to(torch.long)\n",
    "            == y_dis_target.detach()\n",
    "        ).to(torch.float32)\n",
    "    ).cpu()\n",
    "\n",
    "    return (loss_dis_source, loss_dis_target, loss_clf), (accu_source, accu_target)\n",
    "\n",
    "\n",
    "def run_epoch(\n",
    "    dataloader_source, dataloader_target, model, optimizer=None, tqdm_bar=None, two_step=False\n",
    "):\n",
    "    source_results = {}\n",
    "    target_results = {}\n",
    "\n",
    "    source_results[\"clf_loss\"] = []\n",
    "    source_results[\"dis_loss\"] = []\n",
    "    source_results[\"dis_accu\"] = []\n",
    "    source_results[\"weights\"] = []\n",
    "\n",
    "    target_results[\"dis_loss\"] = []\n",
    "    target_results[\"dis_accu\"] = []\n",
    "    target_results[\"weights\"] = []\n",
    "\n",
    "    n_iters = max(len(dataloader_source), len(dataloader_target))\n",
    "\n",
    "    s_iter = iter(dataloader_source)\n",
    "    t_iter = iter(dataloader_target)\n",
    "    for i in range(n_iters):\n",
    "        try:\n",
    "            x_source, y_source = next(s_iter)\n",
    "        except StopIteration:\n",
    "            s_iter = iter(dataloader_source)\n",
    "            x_source, y_source = next(s_iter)\n",
    "        try:\n",
    "            x_target, _ = next(t_iter)\n",
    "        except StopIteration:\n",
    "            t_iter = iter(dataloader_target)\n",
    "            x_target, _ = next(t_iter)\n",
    "\n",
    "\n",
    "        x_source = x_source.to(torch.float32).to(device)\n",
    "        x_target = x_target.to(torch.float32).to(device)\n",
    "        y_source = y_source.to(torch.float32).to(device)\n",
    "\n",
    "        (loss_dis_source, loss_dis_target, loss_clf), (\n",
    "            accu_source,\n",
    "            accu_target,\n",
    "        ) = model_adv_loss(x_source, x_target, y_source, model, two_step=two_step, optimizer=optimizer)\n",
    "\n",
    "        source_results[\"dis_loss\"].append(loss_dis_source.item())\n",
    "        target_results[\"dis_loss\"].append(loss_dis_target.item())\n",
    "        source_results[\"clf_loss\"].append(loss_clf.item())\n",
    "\n",
    "        source_results[\"dis_accu\"].append(accu_source)\n",
    "        target_results[\"dis_accu\"].append(accu_target)\n",
    "\n",
    "        source_results[\"weights\"].append(len(x_source))\n",
    "        target_results[\"weights\"].append(len(x_target))\n",
    "\n",
    "        if tqdm_bar is not None:\n",
    "            tqdm_bar.update(1)\n",
    "\n",
    "    return source_results, target_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversarial_iters(\n",
    "    model,\n",
    "    save_folder,\n",
    "    dataloader_source_train,\n",
    "    dataloader_source_val,\n",
    "    dataloader_target_train,\n",
    "):\n",
    "    model.to(device)\n",
    "    model.advtraining()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), **adv_opt_kwargs)\n",
    "\n",
    "    max_len_dataloader = max(len(dataloader_source_train), len(dataloader_target_train))\n",
    "\n",
    "    iters_val = max(len(dataloader_source_val), len(dataloader_target_train))\n",
    "\n",
    "    # Initialize lists to store loss and accuracy values\n",
    "    results_running_history_source = {}\n",
    "    results_running_history_source[\"clf_loss\"] = []\n",
    "    results_running_history_source[\"dis_loss\"] = []\n",
    "    results_running_history_source[\"dis_accu\"] = []\n",
    "\n",
    "    results_running_history_target = {}\n",
    "    results_running_history_target[\"dis_loss\"] = []\n",
    "    results_running_history_target[\"dis_accu\"] = []\n",
    "\n",
    "    results_history_source = {}\n",
    "    results_history_source[\"clf_loss\"] = []\n",
    "    results_history_source[\"dis_loss\"] = []\n",
    "    results_history_source[\"dis_accu\"] = []\n",
    "\n",
    "    results_history_target = {}\n",
    "    results_history_target[\"dis_loss\"] = []\n",
    "    results_history_target[\"dis_accu\"] = []\n",
    "\n",
    "    results_history_source_val = {}\n",
    "    results_history_source_val[\"clf_loss\"] = []\n",
    "    results_history_source_val[\"dis_loss\"] = []\n",
    "    results_history_source_val[\"dis_accu\"] = []\n",
    "\n",
    "    results_history_target_val = {}\n",
    "    results_history_target_val[\"dis_loss\"] = []\n",
    "    results_history_target_val[\"dis_accu\"] = []\n",
    "\n",
    "    early_stop_count = 0\n",
    "    with DupStdout().dup_to_file(os.path.join(save_folder, \"log.txt\"), \"w\") as f_log:\n",
    "        # Train\n",
    "        print(\"Start adversarial training...\")\n",
    "        outer = tqdm(total=EPOCHS, desc=\"Epochs\", position=0)\n",
    "        inner = tqdm(total=max_len_dataloader, desc=f\"Batch\", position=1)\n",
    "        print(\" Epoch ||| Predictor       ||| Discriminator \")\n",
    "        print(\n",
    "            \"       ||| Loss            ||| Loss                              || Accuracy      \"\n",
    "        )\n",
    "        print(\n",
    "            \"       ||| Source          ||| Source          | Target          || Source          | Target          \"\n",
    "        )\n",
    "        print(\n",
    "            \"       ||| Train  | Val.   ||| Train  | Val.   | Train  | Val.   || Train  | Val.   | Train  | Val.   \"\n",
    "        )\n",
    "        print(\n",
    "            \"------------------------------------------------------------------------------------------------------\"\n",
    "        )\n",
    "        checkpoint = {\n",
    "            \"epoch\": -1,\n",
    "            \"model\": model,\n",
    "            \"optimizer\": optimizer,\n",
    "        }\n",
    "        for epoch in range(EPOCHS):\n",
    "            checkpoint[\"epoch\"] = epoch\n",
    "\n",
    "            # Train mode\n",
    "            model.train()\n",
    "            inner.refresh()  # force print final state\n",
    "            inner.reset()  # reuse bar\n",
    "\n",
    "            source_results, target_results = run_epoch(\n",
    "                dataloader_source_train,\n",
    "                dataloader_target_train,\n",
    "                model,\n",
    "                optimizer=optimizer,\n",
    "                tqdm_bar=inner,\n",
    "                two_step=TWO_STEP,\n",
    "            )\n",
    "\n",
    "            for k in results_running_history_source:\n",
    "                results_running_history_source[k].append(source_results[k])\n",
    "            for k in results_running_history_target:\n",
    "                results_running_history_target[k].append(target_results[k])\n",
    "\n",
    "            for k in results_history_source:\n",
    "                results_history_source[k].append(\n",
    "                    np.average(source_results[k], weights=source_results[\"weights\"])\n",
    "                )\n",
    "            for k in results_history_target:\n",
    "                results_history_target[k].append(\n",
    "                    np.average(target_results[k], weights=target_results[\"weights\"])\n",
    "                )\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                source_results_val, target_results_val = run_epoch(\n",
    "                    dataloader_source_val, dataloader_target_train, model\n",
    "                )\n",
    "\n",
    "            for k in results_history_source:\n",
    "                results_history_source_val[k].append(\n",
    "                    np.average(\n",
    "                        source_results_val[k], weights=source_results_val[\"weights\"]\n",
    "                    )\n",
    "                )\n",
    "            for k in results_history_target:\n",
    "                results_history_target_val[k].append(\n",
    "                    np.average(\n",
    "                        target_results_val[k], weights=target_results_val[\"weights\"]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # Print the results\n",
    "            outer.update(1)\n",
    "            print(\n",
    "                f\" {epoch:5d}\",\n",
    "                f\"||| {results_history_source['clf_loss'][-1]:6.4f}\",\n",
    "                f\"| {results_history_source_val['clf_loss'][-1]:6.4f}\",\n",
    "                f\"||| {results_history_source['dis_loss'][-1]:6.4f}\",\n",
    "                f\"| {results_history_source_val['dis_loss'][-1]:6.4f}\",\n",
    "                f\"| {results_history_target['dis_loss'][-1]:6.4f}\",\n",
    "                f\"| {results_history_target_val['dis_loss'][-1]:6.4f}\",\n",
    "                f\"|| {results_history_source['dis_accu'][-1]:6.4f}\",\n",
    "                f\"| {results_history_source_val['dis_accu'][-1]:6.4f}\",\n",
    "                f\"| {results_history_target['dis_accu'][-1]:6.4f}\",\n",
    "                f\"| {results_history_target_val['dis_accu'][-1]:6.4f}\",\n",
    "                end=\" \",\n",
    "            )\n",
    "\n",
    "            # # Save the best weights\n",
    "            # if diff_from_rand < best_loss_val:\n",
    "            #     best_loss_val = diff_from_rand\n",
    "            #     torch.save(checkpoint, os.path.join(save_folder, f\"best_model.pth\"))\n",
    "            #     early_stop_count = 0\n",
    "\n",
    "            #     print(\"<-- new best difference from random loss\")\n",
    "            # else:\n",
    "            #     print(\"\")\n",
    "\n",
    "            print(\"\")\n",
    "\n",
    "            # Save checkpoint every 10\n",
    "            if epoch % 10 == 0 or epoch >= EPOCHS - 1:\n",
    "                torch.save(checkpoint, os.path.join(save_folder, f\"checkpt{epoch}.pth\"))\n",
    "\n",
    "            # check to see if validation loss has plateau'd\n",
    "            # if early_stop_count >= EARLY_STOP_CRIT_ADV and epoch > MIN_EPOCHS_ADV - 1:\n",
    "            #     print(\n",
    "            #         f\"Loss plateaued after {early_stop_count} at epoch {epoch}\"\n",
    "            #     )\n",
    "            #     torch.save(checkpoint, os.path.join(save_folder, f\"earlystop_{epoch}.pth\"))\n",
    "            #     break\n",
    "\n",
    "            early_stop_count += 1\n",
    "\n",
    "    # Save final model\n",
    "    torch.save(checkpoint, os.path.join(save_folder, f\"final_model.pth\"))\n",
    "\n",
    "    return (\n",
    "        results_running_history_source,\n",
    "        results_running_history_target,\n",
    "        results_history_source,\n",
    "        results_history_target,\n",
    "        results_history_source_val,\n",
    "        results_history_target_val,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_sample_id_l = [SAMPLE_ID_N]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial training for ST slide 151673: \n",
      "Start adversarial training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6273ebe8a184b0aa7465ab2e2ab3eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f345ad9dce54b669245aa472a09dded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch ||| Predictor       ||| Discriminator \n",
      "       ||| Loss            ||| Loss                              || Accuracy      \n",
      "       ||| Source          ||| Source          | Target          || Source          | Target          \n",
      "       ||| Train  | Val.   ||| Train  | Val.   | Train  | Val.   || Train  | Val.   | Train  | Val.   \n",
      "------------------------------------------------------------------------------------------------------\n",
      "     0 ||| 2.0030 | 1.9154 ||| 0.8415 | 0.7447 | 0.9059 | 0.6768 || 0.3512 | 0.9421 | 0.2122 | 0.0008 \n",
      "     1 ||| 1.5325 | 1.5137 ||| 0.8546 | 0.7557 | 0.8496 | 0.7032 || 0.3784 | 0.9549 | 0.2223 | 0.0027 \n",
      "     2 ||| 1.3662 | 1.6428 ||| 0.8525 | 0.7852 | 0.8491 | 0.7593 || 0.3868 | 0.9262 | 0.2226 | 0.0003 \n",
      "     3 ||| 1.2696 | 1.5797 ||| 0.8429 | 0.9749 | 0.8359 | 0.7370 || 0.3936 | 0.5031 | 0.2229 | 0.0000 \n",
      "     4 ||| 1.1956 | 1.4823 ||| 0.8400 | 0.8955 | 0.8347 | 0.7291 || 0.4090 | 0.7696 | 0.2207 | 0.0003 \n",
      "     5 ||| 1.1482 | 1.5690 ||| 0.8328 | 0.8136 | 0.8248 | 0.7148 || 0.4140 | 0.8516 | 0.2176 | 0.0000 \n",
      "     6 ||| 1.1095 | 1.5436 ||| 0.8203 | 0.7399 | 0.8215 | 0.7060 || 0.4222 | 0.9711 | 0.2199 | 0.0000 \n",
      "     7 ||| 1.0778 | 1.6545 ||| 0.8163 | 0.8355 | 0.8176 | 0.6849 || 0.4356 | 0.9282 | 0.2177 | 0.0005 \n",
      "     8 ||| 1.0587 | 1.6073 ||| 0.8170 | 0.8265 | 0.8148 | 0.6918 || 0.4457 | 0.9282 | 0.2173 | 0.0000 \n",
      "     9 ||| 1.0398 | 1.3746 ||| 0.8060 | 0.8097 | 0.8045 | 0.6832 || 0.4591 | 0.8629 | 0.2112 | 0.0003 \n",
      "    10 ||| 1.0195 | 1.5185 ||| 0.8026 | 0.7382 | 0.8019 | 0.7073 || 0.4745 | 0.9645 | 0.2132 | 0.0000 \n",
      "    11 ||| 1.0028 | 1.4747 ||| 0.7972 | 0.7720 | 0.7966 | 0.6791 || 0.4808 | 0.9685 | 0.2087 | 0.0000 \n",
      "    12 ||| 0.9857 | 1.5615 ||| 0.7929 | 0.7281 | 0.7941 | 0.6589 || 0.4908 | 0.9918 | 0.2096 | 0.0000 \n",
      "    13 ||| 0.9762 | 1.3043 ||| 0.7932 | 0.8361 | 0.7901 | 0.6805 || 0.5012 | 0.9310 | 0.2020 | 0.0000 \n",
      "    14 ||| 0.9648 | 1.2857 ||| 0.7864 | 0.8567 | 0.7838 | 0.7151 || 0.5086 | 0.8028 | 0.1993 | 0.0000 \n",
      "    15 ||| 0.9544 | 1.3522 ||| 0.7806 | 0.8797 | 0.7824 | 0.6987 || 0.5273 | 0.7968 | 0.1937 | 0.0000 \n",
      "    16 ||| 0.9444 | 1.3419 ||| 0.7841 | 0.8357 | 0.7831 | 0.6774 || 0.5329 | 0.8723 | 0.1874 | 0.0000 \n",
      "    17 ||| 0.9315 | 1.4352 ||| 0.7785 | 0.8712 | 0.7775 | 0.6914 || 0.5485 | 0.8522 | 0.1872 | 0.0000 \n",
      "    18 ||| 0.9257 | 1.3596 ||| 0.7781 | 0.7454 | 0.7763 | 0.6854 || 0.5585 | 0.9852 | 0.1799 | 0.0000 \n",
      "    19 ||| 0.9196 | 1.3366 ||| 0.7721 | 0.6835 | 0.7735 | 0.6517 || 0.5730 | 0.9994 | 0.1766 | 0.0000 \n",
      "    20 ||| 0.9144 | 1.3608 ||| 0.7698 | 0.6311 | 0.7685 | 0.6654 || 0.5842 | 0.9966 | 0.1758 | 0.0000 \n",
      "    21 ||| 0.9025 | 1.4568 ||| 0.7683 | 0.6611 | 0.7691 | 0.6708 || 0.5947 | 0.9906 | 0.1672 | 0.0000 \n",
      "    22 ||| 0.8991 | 1.3644 ||| 0.7703 | 0.7276 | 0.7690 | 0.6871 || 0.6058 | 0.9977 | 0.1575 | 0.0000 \n",
      "    23 ||| 0.8933 | 1.4042 ||| 0.7656 | 0.7348 | 0.7653 | 0.6812 || 0.6138 | 0.9991 | 0.1570 | 0.0000 \n",
      "    24 ||| 0.8863 | 1.3295 ||| 0.7591 | 0.6614 | 0.7632 | 0.6731 || 0.6297 | 0.9989 | 0.1499 | 0.0000 \n",
      "    25 ||| 0.8781 | 1.4405 ||| 0.7595 | 0.6345 | 0.7594 | 0.6727 || 0.6430 | 0.9940 | 0.1451 | 0.0000 \n",
      "    26 ||| 0.8771 | 1.4667 ||| 0.7573 | 0.6622 | 0.7556 | 0.6879 || 0.6560 | 1.0000 | 0.1384 | 0.0000 \n",
      "    27 ||| 0.8725 | 1.4645 ||| 0.7542 | 0.7160 | 0.7538 | 0.6783 || 0.6688 | 0.9997 | 0.1427 | 0.0000 \n",
      "    28 ||| 0.8687 | 1.5265 ||| 0.7541 | 0.6528 | 0.7543 | 0.6830 || 0.6809 | 0.9991 | 0.1296 | 0.0000 \n",
      "    29 ||| 0.8618 | 1.4650 ||| 0.7518 | 0.6755 | 0.7528 | 0.6544 || 0.6894 | 1.0000 | 0.1257 | 0.0000 \n"
     ]
    }
   ],
   "source": [
    "if TRAIN_USING_ALL_ST_SAMPLES:\n",
    "    print(f\"Adversarial training for all ST slides\")\n",
    "    save_folder = advtrain_folder\n",
    "\n",
    "    model = DANN(\n",
    "        sc_mix_train_s.shape[1],\n",
    "        emb_dim=64,\n",
    "        ncls_source=lab_mix_train.shape[1],\n",
    "        alpha_=ALPHA,\n",
    "    )\n",
    "    model.apply(initialize_weights)\n",
    "    if PRETRAINING:\n",
    "        best_pre_checkpoint = torch.load(\n",
    "            os.path.join(pretrain_folder, f\"final_model.pth\")\n",
    "        )\n",
    "        model.load_state_dict(best_pre_checkpoint[\"model\"].state_dict())\n",
    "    model.to(device)\n",
    "\n",
    "    model.advtraining()\n",
    "\n",
    "    train_adversarial_iters(\n",
    "        model,\n",
    "        save_folder,\n",
    "        dataloader_source_train,\n",
    "        dataloader_source_val,\n",
    "        dataloader_target_train,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    for sample_id in st_sample_id_l:\n",
    "        print(f\"Adversarial training for ST slide {sample_id}: \")\n",
    "\n",
    "        save_folder = os.path.join(advtrain_folder, sample_id)\n",
    "        if not os.path.isdir(save_folder):\n",
    "            os.makedirs(save_folder)\n",
    "\n",
    "        model = DANN(\n",
    "            sc_mix_train_s.shape[1],\n",
    "            emb_dim=64,\n",
    "            ncls_source=lab_mix_train.shape[1],\n",
    "            alpha_=ALPHA,\n",
    "        )\n",
    "        model.apply(initialize_weights)\n",
    "\n",
    "        if PRETRAINING:\n",
    "            best_pre_checkpoint = torch.load(\n",
    "                os.path.join(pretrain_folder, f\"final_model.pth\")\n",
    "            )\n",
    "            model.load_state_dict(best_pre_checkpoint[\"model\"].state_dict())\n",
    "        model.to(device)\n",
    "\n",
    "        model.advtraining()\n",
    "\n",
    "        (\n",
    "            results_running_history_source,\n",
    "            results_running_history_target,\n",
    "            results_history_source,\n",
    "            results_history_target,\n",
    "            results_history_source_val,\n",
    "            results_history_target_val,\n",
    "        ) = train_adversarial_iters(\n",
    "            model,\n",
    "            save_folder,\n",
    "            dataloader_source_train,\n",
    "            dataloader_source_val,\n",
    "            dataloader_target_train_d[sample_id],\n",
    "        )\n",
    "\n",
    "        # best_checkpoint = torch.load(os.path.join(save_folder, f\"final_model.pth\"))\n",
    "        # best_loss_val = loss_history_val[best_epoch]\n",
    "        # best_acc_val = accu_history_val[best_epoch]\n",
    "\n",
    "        fig, axs = plt.subplots(3, 1, sharex=True, figsize=(9, 4), layout=\"constrained\")\n",
    "\n",
    "        # prediction loss\n",
    "        plot_iters(\n",
    "            results_running_history_source[\"clf_loss\"],\n",
    "            ax=axs[0],\n",
    "            label=\"Training\",\n",
    "            ls=\"-\",\n",
    "            c=\"tab:blue\",\n",
    "        )\n",
    "        axs[0].plot(\n",
    "            results_history_source_val[\"clf_loss\"],\n",
    "            label=\"Validation\",\n",
    "            ls=\"--\",\n",
    "            c=\"tab:blue\",\n",
    "        )\n",
    "\n",
    "        axs[0].set_ylim(bottom=0)\n",
    "        axs[0].grid(which=\"major\")\n",
    "        axs[0].minorticks_on()\n",
    "        axs[0].grid(which=\"minor\", alpha=0.2)\n",
    "\n",
    "        axs[0].set_title(\"Source KL-Divergence Prediction Loss\")\n",
    "        axs[0].legend()\n",
    "\n",
    "        # discriminator loss\n",
    "        plot_iters(\n",
    "            results_running_history_source[\"dis_loss\"],\n",
    "            ax=axs[1],\n",
    "            label=\"Source train\",\n",
    "            ls=\"-\",\n",
    "            c=\"tab:blue\",\n",
    "        )\n",
    "        plot_iters(\n",
    "            results_running_history_target[\"dis_loss\"],\n",
    "            ax=axs[1],\n",
    "            label=\"Target train\",\n",
    "            ls=\"-\",\n",
    "            c=\"tab:orange\",\n",
    "        )\n",
    "        axs[1].plot(\n",
    "            results_history_source_val[\"dis_loss\"],\n",
    "            label=\"Source val\",\n",
    "            ls=\"--\",\n",
    "            c=\"tab:blue\",\n",
    "        )\n",
    "        axs[1].plot(\n",
    "            results_history_target_val[\"dis_loss\"],\n",
    "            label=\"Target train eval\",\n",
    "            ls=\"--\",\n",
    "            c=\"tab:orange\",\n",
    "        )\n",
    "\n",
    "        axs[1].set_ylim(bottom=0)\n",
    "        axs[1].grid(which=\"major\")\n",
    "        axs[1].minorticks_on()\n",
    "        axs[1].grid(which=\"minor\", alpha=0.2)\n",
    "\n",
    "        axs[1].set_xlabel(\"Epoch\")\n",
    "        axs[1].set_title(\"Discriminator BCE Loss\")\n",
    "        axs[1].legend()\n",
    "\n",
    "        # discriminator accuracy\n",
    "        plot_iters(\n",
    "            results_running_history_source[\"dis_accu\"],\n",
    "            ax=axs[2],\n",
    "            label=\"Source train\",\n",
    "            ls=\"-\",\n",
    "            c=\"tab:blue\",\n",
    "        )\n",
    "        plot_iters(\n",
    "            results_running_history_target[\"dis_accu\"],\n",
    "            ax=axs[2],\n",
    "            label=\"Target train\",\n",
    "            ls=\"-\",\n",
    "            c=\"tab:orange\",\n",
    "        )\n",
    "        axs[2].plot(\n",
    "            results_history_source_val[\"dis_accu\"],\n",
    "            label=\"Source val\",\n",
    "            ls=\"--\",\n",
    "            c=\"tab:blue\",\n",
    "        )\n",
    "        axs[2].plot(\n",
    "            results_history_target_val[\"dis_accu\"],\n",
    "            label=\"Target train eval\",\n",
    "            ls=\"--\",\n",
    "            c=\"tab:orange\",\n",
    "        )\n",
    "\n",
    "        axs[2].set_ylim([0, 1])\n",
    "        axs[2].grid(which=\"major\")\n",
    "        axs[2].minorticks_on()\n",
    "        axs[2].grid(which=\"minor\", alpha=0.2)\n",
    "\n",
    "        axs[2].set_xlabel(\"Epoch\")\n",
    "        axs[2].set_title(\"Discriminator Accuracy\")\n",
    "        axs[2].legend()\n",
    "\n",
    "        plt.savefig(os.path.join(save_folder, \"adv_train.png\"))\n",
    "\n",
    "        plt.show(block=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('agreda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c8a91b640e5c43bacdcbf87782ad770b561ed71a46153862bbc20bda0bebf44f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
