{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # ADDA for ST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Creating something like CellDART but it actually follows Adda in PyTorch as a first step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_836197/3119678908.py:5: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import h5py\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "from src.da_models.adda import ADDAST\n",
    "from src.da_models.datasets import SpotDataset\n",
    "\n",
    "# datetime object containing current date and time\n",
    "script_start_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%Hh%Mm%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NUM_MARKERS = 20\n",
    "# N_MIX = 8\n",
    "# N_SPOTS = 20000\n",
    "TRAIN_USING_ALL_ST_SAMPLES = False\n",
    "\n",
    "SAMPLE_ID_N = \"151673\"\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "NUM_WORKERS = 8\n",
    "INITIAL_TRAIN_EPOCHS = 100\n",
    "\n",
    "\n",
    "MIN_EPOCHS = 0.4 * INITIAL_TRAIN_EPOCHS\n",
    "EARLY_STOP_CRIT = INITIAL_TRAIN_EPOCHS\n",
    "\n",
    "PROCESSED_DATA_DIR = \"data/preprocessed\"\n",
    "\n",
    "MODEL_NAME = \"ADDA\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/ADDA/TESTING\n"
     ]
    }
   ],
   "source": [
    "model_folder = os.path.join(\"model\", MODEL_NAME, script_start_time)\n",
    "\n",
    "model_folder = os.path.join(\"model\", MODEL_NAME, \"TESTING\")\n",
    "\n",
    "if not os.path.isdir(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "    print(model_folder)\n",
    "\n",
    "# if not os.path.isdir(results_folder):\n",
    "#     os.makedirs(results_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sc.logging.print_versions()\n",
    "# sc.set_figure_params(facecolor=\"white\", figsize=(8, 8))\n",
    "# sc.settings.verbosity = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spatial data\n",
    "mat_sp_test_s_d = {}\n",
    "with h5py.File(os.path.join(PROCESSED_DATA_DIR, \"mat_sp_test_s_d.hdf5\"), \"r\") as f:\n",
    "    for sample_id in f:\n",
    "        mat_sp_test_s_d[sample_id] = f[sample_id][()]\n",
    "\n",
    "if TRAIN_USING_ALL_ST_SAMPLES:\n",
    "    with h5py.File(os.path.join(PROCESSED_DATA_DIR, \"mat_sp_train_s.hdf5\"), \"r\") as f:\n",
    "        mat_sp_train_s = f[\"all\"][()]\n",
    "else:\n",
    "    mat_sp_train_s_d = mat_sp_test_s_d\n",
    "\n",
    "# Load sc data\n",
    "with h5py.File(os.path.join(PROCESSED_DATA_DIR, \"sc.hdf5\"), \"r\") as f:\n",
    "    sc_mix_train_s = f[\"X/train\"][()]\n",
    "    sc_mix_val_s = f[\"X/val\"][()]\n",
    "    sc_mix_test_s = f[\"X/test\"][()]\n",
    "\n",
    "    lab_mix_train = f[\"y/train\"][()]\n",
    "    lab_mix_val = f[\"y/val\"][()]\n",
    "    lab_mix_test = f[\"y/test\"][()]\n",
    "\n",
    "# Load helper dicts / lists\n",
    "with open(os.path.join(PROCESSED_DATA_DIR, \"sc_sub_dict.pkl\"), \"rb\") as f:\n",
    "    sc_sub_dict = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(PROCESSED_DATA_DIR, \"sc_sub_dict2.pkl\"), \"rb\") as f:\n",
    "    sc_sub_dict2 = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(PROCESSED_DATA_DIR, \"st_sample_id_l.pkl\"), \"rb\") as f:\n",
    "    st_sample_id_l = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Training: Adversarial domain adaptation for cell fraction estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### source dataloaders\n",
    "source_train_set = SpotDataset(sc_mix_train_s, lab_mix_train)\n",
    "source_val_set = SpotDataset(sc_mix_val_s, lab_mix_val)\n",
    "source_test_set = SpotDataset(sc_mix_test_s, lab_mix_test)\n",
    "\n",
    "dataloader_source_train = torch.utils.data.DataLoader(\n",
    "    source_train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "dataloader_source_val = torch.utils.data.DataLoader(\n",
    "    source_val_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "dataloader_source_test = torch.utils.data.DataLoader(\n",
    "    source_test_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "### target dataloaders\n",
    "target_test_set_d = {}\n",
    "for sample_id in st_sample_id_l:\n",
    "    target_test_set_d[sample_id] = SpotDataset(mat_sp_test_s_d[sample_id])\n",
    "\n",
    "dataloader_target_test_d = {}\n",
    "for sample_id in st_sample_id_l:\n",
    "    dataloader_target_test_d[sample_id] = torch.utils.data.DataLoader(\n",
    "        target_test_set_d[sample_id],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "if TRAIN_USING_ALL_ST_SAMPLES:\n",
    "    target_train_set = SpotDataset(mat_sp_train_s)\n",
    "    dataloader_target_train = torch.utils.data.DataLoader(\n",
    "        target_train_set,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "else:\n",
    "    target_train_set_d = {}\n",
    "    dataloader_target_train_d = {}\n",
    "    for sample_id in st_sample_id_l:\n",
    "        target_train_set_d[sample_id] = SpotDataset(mat_sp_test_s_d[sample_id])\n",
    "        dataloader_target_train_d[sample_id] = torch.utils.data.DataLoader(\n",
    "            target_train_set_d[sample_id],\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=True,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADDAST(\n",
       "  (source_encoder): MLPEncoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): Linear(in_features=367, out_features=1024, bias=True)\n",
       "      (1): BatchNorm1d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (2): ELU(alpha=1.0)\n",
       "      (3): Linear(in_features=1024, out_features=64, bias=True)\n",
       "      (4): BatchNorm1d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (5): ELU(alpha=1.0)\n",
       "    )\n",
       "  )\n",
       "  (target_encoder): MLPEncoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): Linear(in_features=367, out_features=1024, bias=True)\n",
       "      (1): BatchNorm1d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (2): ELU(alpha=1.0)\n",
       "      (3): Linear(in_features=1024, out_features=64, bias=True)\n",
       "      (4): BatchNorm1d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (5): ELU(alpha=1.0)\n",
       "    )\n",
       "  )\n",
       "  (clf): Predictor(\n",
       "    (head): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=33, bias=True)\n",
       "      (1): LogSoftmax(dim=1)\n",
       "    )\n",
       "  )\n",
       "  (dis): Discriminator(\n",
       "    (head): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (1): BatchNorm1d(32, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (2): ELU(alpha=1.0)\n",
       "      (3): Dropout(p=0.5, inplace=False)\n",
       "      (4): Linear(in_features=32, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ADDAST(sc_mix_train_s.shape[1], emb_dim=64, ncls_source=lab_mix_train.shape[1])\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_folder = os.path.join(model_folder, \"pretrain\")\n",
    "\n",
    "if not os.path.isdir(pretrain_folder):\n",
    "    os.makedirs(pretrain_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=0.002, betas=(0.9, 0.999), eps=1e-07\n",
    ")\n",
    "\n",
    "pre_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    pre_optimizer,\n",
    "    max_lr=0.002,\n",
    "    steps_per_epoch=len(dataloader_source_train),\n",
    "    epochs=INITIAL_TRAIN_EPOCHS,\n",
    ")\n",
    "\n",
    "criterion_clf = nn.KLDivLoss(reduction=\"batchmean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(x, y_true, model):\n",
    "    x = x.to(torch.float32).to(device)\n",
    "    y_true = y_true.to(torch.float32).to(device)\n",
    "\n",
    "    y_pred = model(x)\n",
    "\n",
    "    loss = criterion_clf(y_pred, y_true)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_acc(dataloader, model):\n",
    "    loss_running = []\n",
    "    mean_weights = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(dataloader):\n",
    "\n",
    "            loss = model_loss(*batch, model)\n",
    "\n",
    "            loss_running.append(loss.item())\n",
    "\n",
    "            # we will weight average by batch size later\n",
    "            mean_weights.append(len(batch))\n",
    "\n",
    "    return np.average(loss_running, weights=mean_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pretraining()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start pretrain...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58ae39af292417f93d141c9ebb82211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb57e5066a1f4f61a2e7afb19dd7ff15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train loss: 1.782791 validation loss: 1.593287 <-- new best val loss\n",
      "epoch: 1 train loss: 1.511454 validation loss: 1.431101 <-- new best val loss\n",
      "epoch: 2 train loss: 1.400768 validation loss: 1.340491 <-- new best val loss\n",
      "epoch: 3 train loss: 1.328198 validation loss: 1.280501 <-- new best val loss\n",
      "epoch: 4 train loss: 1.267213 validation loss: 1.221347 <-- new best val loss\n",
      "epoch: 5 train loss: 1.207307 validation loss: 1.18924 <-- new best val loss\n",
      "epoch: 6 train loss: 1.144814 validation loss: 1.112537 <-- new best val loss\n",
      "epoch: 7 train loss: 1.079883 validation loss: 1.052676 <-- new best val loss\n",
      "epoch: 8 train loss: 1.00977 validation loss: 0.993249 <-- new best val loss\n",
      "epoch: 9 train loss: 0.934632 validation loss: 0.927142 <-- new best val loss\n",
      "epoch: 10 train loss: 0.85874 validation loss: 0.883097 <-- new best val loss\n",
      "epoch: 11 train loss: 0.782869 validation loss: 0.759503 <-- new best val loss\n",
      "epoch: 12 train loss: 0.712473 validation loss: 0.730624 <-- new best val loss\n",
      "epoch: 13 train loss: 0.65038 validation loss: 0.68265 <-- new best val loss\n",
      "epoch: 14 train loss: 0.603905 validation loss: 0.652436 <-- new best val loss\n",
      "epoch: 15 train loss: 0.571034 validation loss: 0.674053 \n",
      "epoch: 16 train loss: 0.547547 validation loss: 0.657891 \n",
      "epoch: 17 train loss: 0.530713 validation loss: 0.666456 \n",
      "epoch: 18 train loss: 0.513758 validation loss: 0.670145 \n",
      "epoch: 19 train loss: 0.49871 validation loss: 0.688612 \n",
      "epoch: 20 train loss: 0.480364 validation loss: 0.699616 \n",
      "epoch: 21 train loss: 0.466602 validation loss: 0.698264 \n",
      "epoch: 22 train loss: 0.452318 validation loss: 0.871888 \n",
      "epoch: 23 train loss: 0.436829 validation loss: 0.757494 \n",
      "epoch: 24 train loss: 0.426085 validation loss: 0.799868 \n",
      "epoch: 25 train loss: 0.408027 validation loss: 0.753575 \n",
      "epoch: 26 train loss: 0.395357 validation loss: 0.773512 \n",
      "epoch: 27 train loss: 0.378842 validation loss: 1.035183 \n",
      "epoch: 28 train loss: 0.368217 validation loss: 0.839478 \n",
      "epoch: 29 train loss: 0.34779 validation loss: 0.744177 \n",
      "epoch: 30 train loss: 0.336069 validation loss: 0.926307 \n",
      "epoch: 31 train loss: 0.321064 validation loss: 0.954309 \n",
      "epoch: 32 train loss: 0.309145 validation loss: 0.903508 \n",
      "epoch: 33 train loss: 0.302356 validation loss: 0.910546 \n",
      "epoch: 34 train loss: 0.286411 validation loss: 1.333393 \n",
      "epoch: 35 train loss: 0.273093 validation loss: 0.968246 \n",
      "epoch: 36 train loss: 0.26857 validation loss: 0.930108 \n",
      "epoch: 37 train loss: 0.25816 validation loss: 0.945409 \n",
      "epoch: 38 train loss: 0.250146 validation loss: 0.818318 \n",
      "epoch: 39 train loss: 0.241704 validation loss: 1.081287 \n",
      "epoch: 40 train loss: 0.232676 validation loss: 1.043313 \n",
      "epoch: 41 train loss: 0.225397 validation loss: 1.060292 \n",
      "epoch: 42 train loss: 0.216503 validation loss: 1.002436 \n",
      "epoch: 43 train loss: 0.210384 validation loss: 1.103621 \n",
      "epoch: 44 train loss: 0.204202 validation loss: 0.938675 \n",
      "epoch: 45 train loss: 0.197415 validation loss: 1.181785 \n",
      "epoch: 46 train loss: 0.191482 validation loss: 0.873438 \n",
      "epoch: 47 train loss: 0.185355 validation loss: 0.876809 \n",
      "epoch: 48 train loss: 0.180579 validation loss: 1.033496 \n",
      "epoch: 49 train loss: 0.175529 validation loss: 0.901594 \n",
      "epoch: 50 train loss: 0.169276 validation loss: 1.010722 \n",
      "epoch: 51 train loss: 0.159867 validation loss: 0.850256 \n",
      "epoch: 52 train loss: 0.155155 validation loss: 0.915689 \n",
      "epoch: 53 train loss: 0.153277 validation loss: 0.855591 \n",
      "epoch: 54 train loss: 0.148492 validation loss: 1.051054 \n",
      "epoch: 55 train loss: 0.144791 validation loss: 0.945755 \n",
      "epoch: 56 train loss: 0.139682 validation loss: 0.911216 \n",
      "epoch: 57 train loss: 0.134048 validation loss: 0.947112 \n",
      "epoch: 58 train loss: 0.130745 validation loss: 0.952399 \n",
      "epoch: 59 train loss: 0.125432 validation loss: 0.899518 \n",
      "epoch: 60 train loss: 0.119496 validation loss: 0.937179 \n",
      "epoch: 61 train loss: 0.116897 validation loss: 0.93645 \n",
      "epoch: 62 train loss: 0.115379 validation loss: 0.955634 \n",
      "epoch: 63 train loss: 0.110906 validation loss: 0.976521 \n",
      "epoch: 64 train loss: 0.106445 validation loss: 1.045671 \n",
      "epoch: 65 train loss: 0.102976 validation loss: 1.011848 \n",
      "epoch: 66 train loss: 0.099729 validation loss: 0.981342 \n",
      "epoch: 67 train loss: 0.096043 validation loss: 0.968723 \n",
      "epoch: 68 train loss: 0.093215 validation loss: 1.054092 \n",
      "epoch: 69 train loss: 0.089385 validation loss: 0.973096 \n",
      "epoch: 70 train loss: 0.086705 validation loss: 0.974999 \n",
      "epoch: 71 train loss: 0.083684 validation loss: 1.038789 \n",
      "epoch: 72 train loss: 0.080909 validation loss: 1.017959 \n",
      "epoch: 73 train loss: 0.078605 validation loss: 1.026869 \n",
      "epoch: 74 train loss: 0.075857 validation loss: 1.012464 \n",
      "epoch: 75 train loss: 0.073647 validation loss: 1.024731 \n",
      "epoch: 76 train loss: 0.071888 validation loss: 1.035538 \n",
      "epoch: 77 train loss: 0.06993 validation loss: 1.024628 \n",
      "epoch: 78 train loss: 0.067793 validation loss: 1.039576 \n",
      "epoch: 79 train loss: 0.066503 validation loss: 1.044467 \n",
      "epoch: 80 train loss: 0.064502 validation loss: 1.042031 \n",
      "epoch: 81 train loss: 0.06322 validation loss: 1.048487 \n",
      "epoch: 82 train loss: 0.062209 validation loss: 1.06584 \n",
      "epoch: 83 train loss: 0.060571 validation loss: 1.065472 \n",
      "epoch: 84 train loss: 0.059303 validation loss: 1.068976 \n",
      "epoch: 85 train loss: 0.058508 validation loss: 1.079417 \n",
      "epoch: 86 train loss: 0.058098 validation loss: 1.076929 \n",
      "epoch: 87 train loss: 0.05688 validation loss: 1.062367 \n",
      "epoch: 88 train loss: 0.056475 validation loss: 1.078251 \n",
      "epoch: 89 train loss: 0.055944 validation loss: 1.087925 \n",
      "epoch: 90 train loss: 0.055654 validation loss: 1.089438 \n",
      "epoch: 91 train loss: 0.054916 validation loss: 1.078398 \n",
      "epoch: 92 train loss: 0.054845 validation loss: 1.088214 \n",
      "epoch: 93 train loss: 0.054612 validation loss: 1.092867 \n",
      "epoch: 94 train loss: 0.05423 validation loss: 1.087412 \n",
      "epoch: 95 train loss: 0.053883 validation loss: 1.097173 \n",
      "epoch: 96 train loss: 0.053903 validation loss: 1.090862 \n",
      "epoch: 97 train loss: 0.053926 validation loss: 1.076976 \n",
      "epoch: 98 train loss: 0.054021 validation loss: 1.084907 \n",
      "epoch: 99 train loss: 0.053817 validation loss: 1.077379 \n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store loss and accuracy values\n",
    "loss_history = []\n",
    "loss_history_val = []\n",
    "\n",
    "loss_history_running = []\n",
    "\n",
    "# Early Stopping\n",
    "best_loss_val = np.inf\n",
    "early_stop_count = 0\n",
    "\n",
    "\n",
    "# Train\n",
    "print(\"Start pretrain...\")\n",
    "outer = tqdm(total=INITIAL_TRAIN_EPOCHS, desc=\"Epochs\", position=0)\n",
    "inner = tqdm(total=len(dataloader_source_train), desc=f\"Batch\", position=1)\n",
    "\n",
    "checkpoint = {\n",
    "    \"epoch\": -1,\n",
    "    \"model\": model,\n",
    "    \"optimizer\": pre_optimizer,\n",
    "    \"scheduler\": pre_scheduler,\n",
    "    # 'scaler': scaler\n",
    "}\n",
    "for epoch in range(INITIAL_TRAIN_EPOCHS):\n",
    "    checkpoint[\"epoch\"] = epoch\n",
    "\n",
    "    # Train mode\n",
    "    model.train()\n",
    "    loss_running = []\n",
    "    mean_weights = []\n",
    "\n",
    "    inner.refresh()  # force print final state\n",
    "    inner.reset()  # reuse bar\n",
    "    for _, batch in enumerate(dataloader_source_train):\n",
    "        # lr_history_running.append(scheduler.get_last_lr())\n",
    "\n",
    "        pre_optimizer.zero_grad()\n",
    "        loss = model_loss(*batch, model)\n",
    "        loss_running.append(loss.item())\n",
    "        mean_weights.append(len(batch))  # we will weight average by batch size later\n",
    "\n",
    "        # scaler.scale(loss).backward()\n",
    "        # scaler.step(optimizer)\n",
    "        # scaler.update()\n",
    "\n",
    "        loss.backward()\n",
    "        pre_optimizer.step()\n",
    "        pre_scheduler.step()\n",
    "\n",
    "        inner.update(1)\n",
    "\n",
    "    loss_history.append(np.average(loss_running, weights=mean_weights))\n",
    "    loss_history_running.append(loss_running)\n",
    "\n",
    "    # Evaluate mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        curr_loss_val = compute_acc(dataloader_source_val, model)\n",
    "        loss_history_val.append(curr_loss_val)\n",
    "\n",
    "    # Print the results\n",
    "    outer.update(1)\n",
    "    print(\n",
    "        \"epoch:\",\n",
    "        epoch,\n",
    "        \"train loss:\",\n",
    "        round(loss_history[-1], 6),\n",
    "        \"validation loss:\",\n",
    "        round(loss_history_val[-1], 6),\n",
    "        # \"next_lr:\", scheduler.get_last_lr(),\n",
    "        end=\" \",\n",
    "    )\n",
    "    # Save the best weights\n",
    "    if curr_loss_val < best_loss_val:\n",
    "        best_loss_val = curr_loss_val\n",
    "        torch.save(checkpoint, os.path.join(pretrain_folder, f\"best_model.pth\"))\n",
    "        early_stop_count = 0\n",
    "\n",
    "        print(\"<-- new best val loss\")\n",
    "    else:\n",
    "        print(\"\")\n",
    "\n",
    "    # Save checkpoint every 10\n",
    "    if epoch % 10 == 0 or epoch >= INITIAL_TRAIN_EPOCHS - 1:\n",
    "        torch.save(checkpoint, os.path.join(pretrain_folder, f\"checkpt{epoch}.pth\"))\n",
    "\n",
    "    # check to see if validation loss has plateau'd\n",
    "    if early_stop_count >= EARLY_STOP_CRIT and epoch >= MIN_EPOCHS - 1:\n",
    "        print(f\"Validation loss plateaued after {early_stop_count} at epoch {epoch}\")\n",
    "        torch.save(checkpoint, os.path.join(pretrain_folder, f\"earlystop{epoch}.pth\"))\n",
    "        break\n",
    "\n",
    "    early_stop_count += 1\n",
    "\n",
    "\n",
    "# Save final model\n",
    "best_checkpoint = torch.load(os.path.join(pretrain_folder, f\"best_model.pth\"))\n",
    "torch.save(best_checkpoint, os.path.join(pretrain_folder, f\"final_model.pth\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Adversarial Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "MIN_EPOCHS_ADV = 0.4 * EPOCHS\n",
    "EARLY_STOP_CRIT_ADV = EPOCHS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "advtrain_folder = os.path.join(model_folder, \"advtrain\")\n",
    "\n",
    "if not os.path.isdir(advtrain_folder):\n",
    "    os.makedirs(advtrain_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_iter(iter):\n",
    "    while True:\n",
    "        yield from iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_dis = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrim_loss_accu(x_source, x_target, model):\n",
    "    x_source, x_target = x_source.to(device), x_target.to(device)\n",
    "    y_dis = torch.cat(\n",
    "        [\n",
    "            torch.zeros(x_source.shape[0], device=device, dtype=torch.long),\n",
    "            torch.ones(x_target.shape[0], device=device, dtype=torch.long),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    emb_source = model.source_encoder(x_source).view(x_source.shape[0], -1)\n",
    "    emb_target = model.target_encoder(x_target).view(x_target.shape[0], -1)\n",
    "\n",
    "    emb_all = torch.cat((emb_source, emb_target))\n",
    "\n",
    "    y_pred = model.dis(emb_all)\n",
    "\n",
    "    loss = criterion_dis(y_pred, y_dis)\n",
    "    accu = torch.mean(\n",
    "        (torch.flatten(torch.argmax(y_pred, dim=1)) == y_dis).to(torch.float32)\n",
    "    ).cpu()\n",
    "\n",
    "    return loss, accu\n",
    "\n",
    "\n",
    "def encoder_loss(x_target, model):\n",
    "    x_target = x_target.to(device)\n",
    "\n",
    "    # flip label\n",
    "    y_dis = torch.zeros(x_target.shape[0], device=device, dtype=torch.long)\n",
    "\n",
    "    emb_target = model.target_encoder(x_target).view(x_target.shape[0], -1)\n",
    "    y_pred = model.dis(emb_target)\n",
    "    loss = criterion_dis(y_pred, y_dis)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversarial(\n",
    "    model,\n",
    "    save_folder,\n",
    "    dataloader_source_train,\n",
    "    dataloader_source_val,\n",
    "    dataloader_target_train,\n",
    "):\n",
    "    model.to(device)\n",
    "    model.advtraining()\n",
    "\n",
    "    target_optimizer = torch.optim.Adam(\n",
    "        model.target_encoder.parameters(), lr=0.0005, betas=(0.9, 0.999), eps=1e-07\n",
    "    )\n",
    "    dis_optimizer = torch.optim.Adam(\n",
    "        model.dis.parameters(), lr=0.00025, betas=(0.9, 0.999), eps=1e-07\n",
    "    )\n",
    "\n",
    "    iters = max(len(dataloader_source_train), len(dataloader_target_train))\n",
    "\n",
    "    dis_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        dis_optimizer, max_lr=0.0005, steps_per_epoch=iters, epochs=EPOCHS\n",
    "    )\n",
    "    target_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        target_optimizer, max_lr=0.0005, steps_per_epoch=iters, epochs=EPOCHS\n",
    "    )\n",
    "\n",
    "    n_samples_source = len(dataloader_source_train.dataset)\n",
    "    n_samples_target = len(dataloader_target_train.dataset)\n",
    "    p = n_samples_source / (n_samples_source + n_samples_target)\n",
    "    rand_loss = -(p * np.log(0.5)) - (1 - p) * np.log(0.5)\n",
    "\n",
    "    # Initialize lists to store loss and accuracy values\n",
    "    loss_history = []\n",
    "    accu_history = []\n",
    "    loss_history_running = []\n",
    "\n",
    "    # Early Stopping\n",
    "    best_loss_val = np.inf\n",
    "    early_stop_count = 0\n",
    "\n",
    "    # Train\n",
    "    print(\"Start adversarial training...\")\n",
    "    print(\"Discriminator target loss:\", rand_loss)\n",
    "    outer = tqdm(total=EPOCHS, desc=\"Epochs\", position=0)\n",
    "    inner1 = tqdm(total=iters, desc=f\"Batch (Discriminator)\", position=1)\n",
    "    inner2 = tqdm(total=iters, desc=f\"Batch (Encoder)\", position=2)\n",
    "    checkpoint = {\n",
    "        \"epoch\": -1,\n",
    "        \"model\": model,\n",
    "        \"dis_optimizer\": dis_optimizer,\n",
    "        \"target_optimizer\": target_optimizer,\n",
    "        \"dis_scheduler\": dis_scheduler,\n",
    "        \"target_scheduler\": target_scheduler,\n",
    "    }\n",
    "    for epoch in range(EPOCHS):\n",
    "        checkpoint[\"epoch\"] = epoch\n",
    "\n",
    "        # Train mode\n",
    "        model.train()\n",
    "\n",
    "        loss_running = []\n",
    "        accu_running = []\n",
    "        mean_weights = []\n",
    "\n",
    "        inner1.refresh()  # force print final state\n",
    "        inner1.reset()  # reuse bar\n",
    "        inner2.refresh()  # force print final state\n",
    "        inner2.reset()  # reuse bar\n",
    "\n",
    "        model.train_discriminator()\n",
    "        model.target_encoder.eval()\n",
    "        model.source_encoder.eval()\n",
    "        model.dis.train()\n",
    "        batch_cycler = zip(\n",
    "            cycle_iter(dataloader_source_train), cycle_iter(dataloader_target_train)\n",
    "        )\n",
    "        for _ in range(iters):\n",
    "            # lr_history_running.append(scheduler.get_last_lr())\n",
    "            dis_optimizer.zero_grad()\n",
    "\n",
    "            (x_source, _), (x_target, _) = next(batch_cycler)\n",
    "            loss, accu = discrim_loss_accu(x_source, x_target, model)\n",
    "            loss_running.append(loss.item())\n",
    "            accu_running.append(accu)\n",
    "            mean_weights.append(len(x_source) + len(x_target))\n",
    "\n",
    "            # scaler.scale(loss).backward()\n",
    "            # scaler.step(optimizer)\n",
    "            # scaler.update()\n",
    "\n",
    "            loss.backward()\n",
    "            dis_optimizer.step()\n",
    "            dis_scheduler.step()\n",
    "\n",
    "            inner1.update(1)\n",
    "\n",
    "        loss_history.append(np.average(loss_running, weights=mean_weights))\n",
    "        accu_history.append(np.average(accu_running, weights=mean_weights))\n",
    "        loss_history_running.append(loss_running)\n",
    "\n",
    "        model.train_target_encoder()\n",
    "        model.target_encoder.train()\n",
    "        model.source_encoder.eval()\n",
    "        model.dis.eval()\n",
    "        batch_cycler = zip(\n",
    "            cycle_iter(dataloader_source_train), cycle_iter(dataloader_target_train)\n",
    "        )\n",
    "        for _ in range(iters):\n",
    "            target_optimizer.zero_grad()\n",
    "\n",
    "            _, (x_target, _) = next(batch_cycler)\n",
    "            loss = encoder_loss(x_target, model)\n",
    "\n",
    "            loss.backward()\n",
    "            target_optimizer.step()\n",
    "            target_scheduler.step()\n",
    "\n",
    "            inner2.update(1)\n",
    "\n",
    "        diff_from_rand = math.fabs(loss_history[-1] - rand_loss)\n",
    "\n",
    "        # Print the results\n",
    "        outer.update(1)\n",
    "        print(\n",
    "            \"epoch:\",\n",
    "            epoch,\n",
    "            \"dis loss:\",\n",
    "            round(loss_history[-1], 6),\n",
    "            \"dis accu:\",\n",
    "            round(accu_history[-1], 6),\n",
    "            \"difference from random loss:\",\n",
    "            round(diff_from_rand, 6),\n",
    "            # \"next_lr:\", scheduler.get_last_lr(),\n",
    "            end=\" \",\n",
    "        )\n",
    "\n",
    "        # Save the best weights\n",
    "        if diff_from_rand < best_loss_val:\n",
    "            best_loss_val = diff_from_rand\n",
    "            torch.save(checkpoint, os.path.join(save_folder, f\"best_model.pth\"))\n",
    "            early_stop_count = 0\n",
    "\n",
    "            print(\"<-- new best difference from random loss\")\n",
    "        else:\n",
    "            print(\"\")\n",
    "\n",
    "        # Save checkpoint every 10\n",
    "        if epoch % 10 == 0 or epoch >= EPOCHS - 1:\n",
    "            torch.save(checkpoint, os.path.join(save_folder, f\"checkpt{epoch}.pth\"))\n",
    "\n",
    "        # check to see if validation loss has plateau'd\n",
    "        if early_stop_count >= EARLY_STOP_CRIT_ADV and epoch > MIN_EPOCHS_ADV - 1:\n",
    "            print(\n",
    "                f\"Discriminator loss plateaued after {early_stop_count} at epoch {epoch}\"\n",
    "            )\n",
    "            torch.save(checkpoint, os.path.join(save_folder, f\"earlystop_{epoch}.pth\"))\n",
    "            break\n",
    "\n",
    "        early_stop_count += 1\n",
    "\n",
    "    # Save final model\n",
    "    torch.save(checkpoint, os.path.join(save_folder, f\"final_model.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# st_sample_id_l = [SAMPLE_ID_N]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial training for ST slide 151509: \n",
      "Start adversarial training...\n",
      "Discriminator target loss: 0.6931471805599453\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478ef184cff14d8a8212cbf617d999ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf62431dbfb48cfab8363874eec0cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Discriminator):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3659f96c05d49dc87be70c2005f74f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Encoder):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 dis loss: 0.861351 dis accu: 0.342195 difference from random loss: 0.168204 <-- new best difference from random loss\n",
      "epoch: 1 dis loss: 0.781955 dis accu: 0.497982 difference from random loss: 0.088807 <-- new best difference from random loss\n",
      "epoch: 2 dis loss: 0.802414 dis accu: 0.47819 difference from random loss: 0.109266 \n",
      "epoch: 3 dis loss: 0.82428 dis accu: 0.457503 difference from random loss: 0.131133 \n",
      "epoch: 4 dis loss: 0.822478 dis accu: 0.459317 difference from random loss: 0.129331 \n",
      "epoch: 5 dis loss: 0.785358 dis accu: 0.49788 difference from random loss: 0.09221 \n",
      "epoch: 6 dis loss: 0.74312 dis accu: 0.542037 difference from random loss: 0.049973 <-- new best difference from random loss\n",
      "epoch: 7 dis loss: 0.69406 dis accu: 0.601926 difference from random loss: 0.000912 <-- new best difference from random loss\n",
      "epoch: 8 dis loss: 0.649146 dis accu: 0.658622 difference from random loss: 0.044002 \n",
      "epoch: 9 dis loss: 0.623918 dis accu: 0.689013 difference from random loss: 0.069229 \n",
      "epoch: 10 dis loss: 0.584617 dis accu: 0.73248 difference from random loss: 0.108531 \n",
      "epoch: 11 dis loss: 0.573018 dis accu: 0.747625 difference from random loss: 0.120129 \n",
      "epoch: 12 dis loss: 0.540553 dis accu: 0.771938 difference from random loss: 0.152594 \n",
      "epoch: 13 dis loss: 0.411506 dis accu: 0.860277 difference from random loss: 0.281641 \n",
      "epoch: 14 dis loss: 0.524571 dis accu: 0.794974 difference from random loss: 0.168576 \n",
      "epoch: 15 dis loss: 0.648438 dis accu: 0.698514 difference from random loss: 0.044709 \n",
      "epoch: 16 dis loss: 0.532815 dis accu: 0.784452 difference from random loss: 0.160333 \n",
      "epoch: 17 dis loss: 0.643329 dis accu: 0.696215 difference from random loss: 0.049819 \n",
      "epoch: 18 dis loss: 0.652367 dis accu: 0.677521 difference from random loss: 0.04078 \n",
      "epoch: 19 dis loss: 0.658702 dis accu: 0.659235 difference from random loss: 0.034445 \n",
      "epoch: 20 dis loss: 0.733292 dis accu: 0.561191 difference from random loss: 0.040145 \n",
      "epoch: 21 dis loss: 0.746664 dis accu: 0.513101 difference from random loss: 0.053516 \n",
      "epoch: 22 dis loss: 0.753909 dis accu: 0.481893 difference from random loss: 0.060762 \n",
      "epoch: 23 dis loss: 0.708015 dis accu: 0.522347 difference from random loss: 0.014868 \n",
      "epoch: 24 dis loss: 0.751373 dis accu: 0.447926 difference from random loss: 0.058226 \n",
      "epoch: 25 dis loss: 0.723515 dis accu: 0.479186 difference from random loss: 0.030368 \n",
      "epoch: 26 dis loss: 0.748352 dis accu: 0.419221 difference from random loss: 0.055205 \n",
      "epoch: 27 dis loss: 0.729616 dis accu: 0.424328 difference from random loss: 0.036468 \n",
      "epoch: 28 dis loss: 0.710622 dis accu: 0.458755 difference from random loss: 0.017474 \n",
      "epoch: 29 dis loss: 0.706173 dis accu: 0.480131 difference from random loss: 0.013026 \n",
      "epoch: 30 dis loss: 0.70226 dis accu: 0.500638 difference from random loss: 0.009113 \n",
      "epoch: 31 dis loss: 0.699863 dis accu: 0.511237 difference from random loss: 0.006716 \n",
      "epoch: 32 dis loss: 0.697185 dis accu: 0.519588 difference from random loss: 0.004038 \n",
      "epoch: 33 dis loss: 0.699876 dis accu: 0.518618 difference from random loss: 0.006729 \n",
      "epoch: 34 dis loss: 0.70232 dis accu: 0.511365 difference from random loss: 0.009173 \n",
      "epoch: 35 dis loss: 0.703855 dis accu: 0.500868 difference from random loss: 0.010708 \n",
      "epoch: 36 dis loss: 0.703883 dis accu: 0.498978 difference from random loss: 0.010736 \n",
      "epoch: 37 dis loss: 0.702714 dis accu: 0.494739 difference from random loss: 0.009567 \n",
      "epoch: 38 dis loss: 0.701576 dis accu: 0.495812 difference from random loss: 0.008429 \n",
      "epoch: 39 dis loss: 0.699338 dis accu: 0.50286 difference from random loss: 0.00619 \n",
      "epoch: 40 dis loss: 0.700046 dis accu: 0.494279 difference from random loss: 0.006899 \n",
      "epoch: 41 dis loss: 0.698461 dis accu: 0.499387 difference from random loss: 0.005314 \n",
      "epoch: 42 dis loss: 0.698389 dis accu: 0.495505 difference from random loss: 0.005242 \n",
      "epoch: 43 dis loss: 0.69876 dis accu: 0.494024 difference from random loss: 0.005613 \n",
      "epoch: 44 dis loss: 0.697456 dis accu: 0.499464 difference from random loss: 0.004309 \n",
      "epoch: 45 dis loss: 0.697761 dis accu: 0.495199 difference from random loss: 0.004614 \n",
      "epoch: 46 dis loss: 0.69924 dis accu: 0.489504 difference from random loss: 0.006093 \n",
      "epoch: 47 dis loss: 0.6974 dis accu: 0.496169 difference from random loss: 0.004252 \n",
      "epoch: 48 dis loss: 0.699386 dis accu: 0.484728 difference from random loss: 0.006238 \n",
      "epoch: 49 dis loss: 0.697835 dis accu: 0.493692 difference from random loss: 0.004688 \n",
      "Adversarial training for ST slide 151510: \n",
      "Start adversarial training...\n",
      "Discriminator target loss: 0.6931471805599452\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e8e83754ed479f937dd45128907112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594f2ef41e664cef92f02b300706b32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Discriminator):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea684d198ced4dd9b28e231f3cd39797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Encoder):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 dis loss: 0.858061 dis accu: 0.345599 difference from random loss: 0.164913 <-- new best difference from random loss\n",
      "epoch: 1 dis loss: 0.784206 dis accu: 0.494551 difference from random loss: 0.091059 <-- new best difference from random loss\n",
      "epoch: 2 dis loss: 0.809584 dis accu: 0.470469 difference from random loss: 0.116437 \n",
      "epoch: 3 dis loss: 0.823818 dis accu: 0.455989 difference from random loss: 0.130671 \n",
      "epoch: 4 dis loss: 0.825017 dis accu: 0.454251 difference from random loss: 0.13187 \n",
      "epoch: 5 dis loss: 0.800218 dis accu: 0.478929 difference from random loss: 0.107071 \n",
      "epoch: 6 dis loss: 0.753258 dis accu: 0.529349 difference from random loss: 0.060111 <-- new best difference from random loss\n",
      "epoch: 7 dis loss: 0.702144 dis accu: 0.588774 difference from random loss: 0.008997 <-- new best difference from random loss\n",
      "epoch: 8 dis loss: 0.64184 dis accu: 0.664392 difference from random loss: 0.051307 \n",
      "epoch: 9 dis loss: 0.616174 dis accu: 0.69932 difference from random loss: 0.076974 \n",
      "epoch: 10 dis loss: 0.588258 dis accu: 0.726282 difference from random loss: 0.104889 \n",
      "epoch: 11 dis loss: 0.617764 dis accu: 0.699657 difference from random loss: 0.075383 \n",
      "epoch: 12 dis loss: 0.485566 dis accu: 0.812435 difference from random loss: 0.207581 \n",
      "epoch: 13 dis loss: 0.675501 dis accu: 0.648666 difference from random loss: 0.017646 \n",
      "epoch: 14 dis loss: 0.573397 dis accu: 0.739309 difference from random loss: 0.11975 \n",
      "epoch: 15 dis loss: 0.754787 dis accu: 0.569312 difference from random loss: 0.061639 \n",
      "epoch: 16 dis loss: 0.623289 dis accu: 0.678794 difference from random loss: 0.069859 \n",
      "epoch: 17 dis loss: 0.665276 dis accu: 0.63756 difference from random loss: 0.027871 \n",
      "epoch: 18 dis loss: 0.672171 dis accu: 0.626168 difference from random loss: 0.020976 \n",
      "epoch: 19 dis loss: 0.697911 dis accu: 0.58976 difference from random loss: 0.004764 <-- new best difference from random loss\n",
      "epoch: 20 dis loss: 0.6737 dis accu: 0.59604 difference from random loss: 0.019447 \n",
      "epoch: 21 dis loss: 0.740476 dis accu: 0.487674 difference from random loss: 0.047329 \n",
      "epoch: 22 dis loss: 0.710024 dis accu: 0.510484 difference from random loss: 0.016877 \n",
      "epoch: 23 dis loss: 0.732972 dis accu: 0.474517 difference from random loss: 0.039825 \n",
      "epoch: 24 dis loss: 0.715871 dis accu: 0.49058 difference from random loss: 0.022724 \n",
      "epoch: 25 dis loss: 0.70916 dis accu: 0.495589 difference from random loss: 0.016012 \n",
      "epoch: 26 dis loss: 0.702465 dis accu: 0.511184 difference from random loss: 0.009317 \n",
      "epoch: 27 dis loss: 0.715526 dis accu: 0.473635 difference from random loss: 0.022379 \n",
      "epoch: 28 dis loss: 0.71073 dis accu: 0.478047 difference from random loss: 0.017583 \n",
      "epoch: 29 dis loss: 0.712399 dis accu: 0.477865 difference from random loss: 0.019252 \n",
      "epoch: 30 dis loss: 0.703577 dis accu: 0.507681 difference from random loss: 0.01043 \n",
      "epoch: 31 dis loss: 0.701955 dis accu: 0.52076 difference from random loss: 0.008808 \n",
      "epoch: 32 dis loss: 0.700896 dis accu: 0.512612 difference from random loss: 0.007749 \n",
      "epoch: 33 dis loss: 0.694003 dis accu: 0.528052 difference from random loss: 0.000856 <-- new best difference from random loss\n",
      "epoch: 34 dis loss: 0.693214 dis accu: 0.531062 difference from random loss: 6.7e-05 <-- new best difference from random loss\n",
      "epoch: 35 dis loss: 0.694007 dis accu: 0.522369 difference from random loss: 0.00086 \n",
      "epoch: 36 dis loss: 0.692096 dis accu: 0.527351 difference from random loss: 0.001051 \n",
      "epoch: 37 dis loss: 0.693117 dis accu: 0.521824 difference from random loss: 3.1e-05 <-- new best difference from random loss\n",
      "epoch: 38 dis loss: 0.693239 dis accu: 0.519125 difference from random loss: 9.2e-05 \n",
      "epoch: 39 dis loss: 0.693532 dis accu: 0.518554 difference from random loss: 0.000385 \n",
      "epoch: 40 dis loss: 0.691091 dis accu: 0.532333 difference from random loss: 0.002056 \n",
      "epoch: 41 dis loss: 0.694568 dis accu: 0.520111 difference from random loss: 0.001421 \n",
      "epoch: 42 dis loss: 0.694796 dis accu: 0.517957 difference from random loss: 0.001649 \n",
      "epoch: 43 dis loss: 0.690395 dis accu: 0.531036 difference from random loss: 0.002752 \n",
      "epoch: 44 dis loss: 0.698309 dis accu: 0.505216 difference from random loss: 0.005162 \n",
      "epoch: 45 dis loss: 0.697281 dis accu: 0.511677 difference from random loss: 0.004134 \n",
      "epoch: 46 dis loss: 0.694831 dis accu: 0.52185 difference from random loss: 0.001684 \n",
      "epoch: 47 dis loss: 0.696484 dis accu: 0.513364 difference from random loss: 0.003337 \n",
      "epoch: 48 dis loss: 0.694272 dis accu: 0.52281 difference from random loss: 0.001125 \n",
      "epoch: 49 dis loss: 0.694037 dis accu: 0.520993 difference from random loss: 0.000889 \n",
      "Adversarial training for ST slide 151671: \n",
      "Start adversarial training...\n",
      "Discriminator target loss: 0.6931471805599453\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284dc784c01c4cd68ade67184a0e7d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a120b6abb8994be49ded2c5d771169b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Discriminator):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eece776ae6442cc8fb94083086cc7ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Encoder):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 dis loss: 0.849724 dis accu: 0.360318 difference from random loss: 0.156577 <-- new best difference from random loss\n",
      "epoch: 1 dis loss: 0.694781 dis accu: 0.580241 difference from random loss: 0.001634 <-- new best difference from random loss\n",
      "epoch: 2 dis loss: 0.799919 dis accu: 0.476015 difference from random loss: 0.106772 \n",
      "epoch: 3 dis loss: 0.786574 dis accu: 0.494676 difference from random loss: 0.093427 \n",
      "epoch: 4 dis loss: 0.810594 dis accu: 0.463913 difference from random loss: 0.117446 \n",
      "epoch: 5 dis loss: 0.791189 dis accu: 0.479116 difference from random loss: 0.098042 \n",
      "epoch: 6 dis loss: 0.772685 dis accu: 0.501647 difference from random loss: 0.079538 \n",
      "epoch: 7 dis loss: 0.734353 dis accu: 0.54635 difference from random loss: 0.041206 \n",
      "epoch: 8 dis loss: 0.623491 dis accu: 0.665093 difference from random loss: 0.069656 \n",
      "epoch: 9 dis loss: 0.626134 dis accu: 0.66781 difference from random loss: 0.067013 \n",
      "epoch: 10 dis loss: 0.697657 dis accu: 0.595993 difference from random loss: 0.004509 \n",
      "epoch: 11 dis loss: 0.607158 dis accu: 0.677964 difference from random loss: 0.085989 \n",
      "epoch: 12 dis loss: 0.621283 dis accu: 0.666877 difference from random loss: 0.071864 \n",
      "epoch: 13 dis loss: 0.605323 dis accu: 0.679308 difference from random loss: 0.087824 \n",
      "epoch: 14 dis loss: 0.611793 dis accu: 0.672585 difference from random loss: 0.081354 \n",
      "epoch: 15 dis loss: 0.624065 dis accu: 0.669045 difference from random loss: 0.069082 \n",
      "epoch: 16 dis loss: 0.671688 dis accu: 0.60826 difference from random loss: 0.021459 \n",
      "epoch: 17 dis loss: 0.582266 dis accu: 0.685895 difference from random loss: 0.110881 \n",
      "epoch: 18 dis loss: 0.670703 dis accu: 0.613117 difference from random loss: 0.022444 \n",
      "epoch: 19 dis loss: 0.636429 dis accu: 0.641438 difference from random loss: 0.056718 \n",
      "epoch: 20 dis loss: 0.654797 dis accu: 0.619429 difference from random loss: 0.03835 \n",
      "epoch: 21 dis loss: 0.728869 dis accu: 0.537788 difference from random loss: 0.035722 \n",
      "epoch: 22 dis loss: 0.730193 dis accu: 0.521542 difference from random loss: 0.037046 \n",
      "epoch: 23 dis loss: 0.69113 dis accu: 0.548628 difference from random loss: 0.002017 \n",
      "epoch: 24 dis loss: 0.705347 dis accu: 0.530571 difference from random loss: 0.0122 \n",
      "epoch: 25 dis loss: 0.709669 dis accu: 0.525796 difference from random loss: 0.016522 \n",
      "epoch: 26 dis loss: 0.716798 dis accu: 0.50472 difference from random loss: 0.02365 \n",
      "epoch: 27 dis loss: 0.655335 dis accu: 0.581531 difference from random loss: 0.037812 \n",
      "epoch: 28 dis loss: 0.692449 dis accu: 0.534742 difference from random loss: 0.000699 <-- new best difference from random loss\n",
      "epoch: 29 dis loss: 0.707161 dis accu: 0.515121 difference from random loss: 0.014014 \n",
      "epoch: 30 dis loss: 0.716472 dis accu: 0.50815 difference from random loss: 0.023325 \n",
      "epoch: 31 dis loss: 0.682287 dis accu: 0.564188 difference from random loss: 0.010861 \n",
      "epoch: 32 dis loss: 0.684161 dis accu: 0.552662 difference from random loss: 0.008986 \n",
      "epoch: 33 dis loss: 0.718202 dis accu: 0.496789 difference from random loss: 0.025055 \n",
      "epoch: 34 dis loss: 0.708616 dis accu: 0.489846 difference from random loss: 0.015469 \n",
      "epoch: 35 dis loss: 0.702603 dis accu: 0.521844 difference from random loss: 0.009456 \n",
      "epoch: 36 dis loss: 0.703665 dis accu: 0.530982 difference from random loss: 0.010518 \n",
      "epoch: 37 dis loss: 0.705342 dis accu: 0.534934 difference from random loss: 0.012195 \n",
      "epoch: 38 dis loss: 0.709246 dis accu: 0.517481 difference from random loss: 0.016099 \n",
      "epoch: 39 dis loss: 0.714175 dis accu: 0.514133 difference from random loss: 0.021028 \n",
      "epoch: 40 dis loss: 0.709678 dis accu: 0.533397 difference from random loss: 0.016531 \n",
      "epoch: 41 dis loss: 0.71604 dis accu: 0.504501 difference from random loss: 0.022893 \n",
      "epoch: 42 dis loss: 0.705113 dis accu: 0.531229 difference from random loss: 0.011966 \n",
      "epoch: 43 dis loss: 0.707158 dis accu: 0.536553 difference from random loss: 0.01401 \n",
      "epoch: 44 dis loss: 0.702968 dis accu: 0.53573 difference from random loss: 0.00982 \n",
      "epoch: 45 dis loss: 0.71397 dis accu: 0.539984 difference from random loss: 0.020823 \n",
      "epoch: 46 dis loss: 0.692176 dis accu: 0.536828 difference from random loss: 0.000972 \n",
      "epoch: 47 dis loss: 0.72776 dis accu: 0.495774 difference from random loss: 0.034613 \n",
      "epoch: 48 dis loss: 0.6889 dis accu: 0.554857 difference from random loss: 0.004247 \n",
      "epoch: 49 dis loss: 0.692533 dis accu: 0.55472 difference from random loss: 0.000614 <-- new best difference from random loss\n",
      "Adversarial training for ST slide 151508: \n",
      "Start adversarial training...\n",
      "Discriminator target loss: 0.6931471805599453\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9833d4f6be6485997687d235522acb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce515e60d9e44cfbb57a1879ee34bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Discriminator):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff2c6436c6649908e347e8a251e0446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Encoder):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 dis loss: 0.85796 dis accu: 0.343111 difference from random loss: 0.164813 <-- new best difference from random loss\n",
      "epoch: 1 dis loss: 0.789391 dis accu: 0.487825 difference from random loss: 0.096244 <-- new best difference from random loss\n",
      "epoch: 2 dis loss: 0.797438 dis accu: 0.479886 difference from random loss: 0.104291 \n",
      "epoch: 3 dis loss: 0.824424 dis accu: 0.452499 difference from random loss: 0.131277 \n",
      "epoch: 4 dis loss: 0.823332 dis accu: 0.456895 difference from random loss: 0.130185 \n",
      "epoch: 5 dis loss: 0.784623 dis accu: 0.495098 difference from random loss: 0.091476 <-- new best difference from random loss\n",
      "epoch: 6 dis loss: 0.748717 dis accu: 0.531943 difference from random loss: 0.05557 <-- new best difference from random loss\n",
      "epoch: 7 dis loss: 0.706936 dis accu: 0.582108 difference from random loss: 0.013789 <-- new best difference from random loss\n",
      "epoch: 8 dis loss: 0.674813 dis accu: 0.623854 difference from random loss: 0.018335 \n",
      "epoch: 9 dis loss: 0.639333 dis accu: 0.65998 difference from random loss: 0.053814 \n",
      "epoch: 10 dis loss: 0.604058 dis accu: 0.699275 difference from random loss: 0.089089 \n",
      "epoch: 11 dis loss: 0.581456 dis accu: 0.72232 difference from random loss: 0.111691 \n",
      "epoch: 12 dis loss: 0.561032 dis accu: 0.740622 difference from random loss: 0.132115 \n",
      "epoch: 13 dis loss: 0.594707 dis accu: 0.707694 difference from random loss: 0.09844 \n",
      "epoch: 14 dis loss: 0.554921 dis accu: 0.746004 difference from random loss: 0.138226 \n",
      "epoch: 15 dis loss: 0.69824 dis accu: 0.611813 difference from random loss: 0.005093 <-- new best difference from random loss\n",
      "epoch: 16 dis loss: 0.650625 dis accu: 0.646473 difference from random loss: 0.042522 \n",
      "epoch: 17 dis loss: 0.671593 dis accu: 0.617034 difference from random loss: 0.021555 \n",
      "epoch: 18 dis loss: 0.731002 dis accu: 0.538017 difference from random loss: 0.037855 \n",
      "epoch: 19 dis loss: 0.713413 dis accu: 0.533221 difference from random loss: 0.020265 \n",
      "epoch: 20 dis loss: 0.697407 dis accu: 0.542866 difference from random loss: 0.00426 <-- new best difference from random loss\n",
      "epoch: 21 dis loss: 0.720063 dis accu: 0.510443 difference from random loss: 0.026915 \n",
      "epoch: 22 dis loss: 0.730148 dis accu: 0.486253 difference from random loss: 0.037001 \n",
      "epoch: 23 dis loss: 0.74252 dis accu: 0.459106 difference from random loss: 0.049373 \n",
      "epoch: 24 dis loss: 0.728183 dis accu: 0.475197 difference from random loss: 0.035036 \n",
      "epoch: 25 dis loss: 0.734891 dis accu: 0.470775 difference from random loss: 0.041744 \n",
      "epoch: 26 dis loss: 0.722948 dis accu: 0.47176 difference from random loss: 0.029801 \n",
      "epoch: 27 dis loss: 0.716126 dis accu: 0.485028 difference from random loss: 0.022979 \n",
      "epoch: 28 dis loss: 0.708359 dis accu: 0.508472 difference from random loss: 0.015211 \n",
      "epoch: 29 dis loss: 0.699534 dis accu: 0.527627 difference from random loss: 0.006387 \n",
      "epoch: 30 dis loss: 0.697351 dis accu: 0.533221 difference from random loss: 0.004203 <-- new best difference from random loss\n",
      "epoch: 31 dis loss: 0.69335 dis accu: 0.53775 difference from random loss: 0.000203 <-- new best difference from random loss\n",
      "epoch: 32 dis loss: 0.69584 dis accu: 0.533994 difference from random loss: 0.002693 \n",
      "epoch: 33 dis loss: 0.694441 dis accu: 0.53847 difference from random loss: 0.001294 \n",
      "epoch: 34 dis loss: 0.693614 dis accu: 0.531756 difference from random loss: 0.000467 \n",
      "epoch: 35 dis loss: 0.69216 dis accu: 0.542892 difference from random loss: 0.000987 \n",
      "epoch: 36 dis loss: 0.692267 dis accu: 0.547474 difference from random loss: 0.00088 \n",
      "epoch: 37 dis loss: 0.693306 dis accu: 0.544144 difference from random loss: 0.000158 <-- new best difference from random loss\n",
      "epoch: 38 dis loss: 0.689606 dis accu: 0.558717 difference from random loss: 0.003541 \n",
      "epoch: 39 dis loss: 0.694463 dis accu: 0.549126 difference from random loss: 0.001316 \n",
      "epoch: 40 dis loss: 0.694828 dis accu: 0.546995 difference from random loss: 0.00168 \n",
      "epoch: 41 dis loss: 0.694316 dis accu: 0.554534 difference from random loss: 0.001169 \n",
      "epoch: 42 dis loss: 0.697146 dis accu: 0.545636 difference from random loss: 0.003998 \n",
      "epoch: 43 dis loss: 0.698171 dis accu: 0.54196 difference from random loss: 0.005024 \n",
      "epoch: 44 dis loss: 0.6937 dis accu: 0.556346 difference from random loss: 0.000553 \n",
      "epoch: 45 dis loss: 0.698546 dis accu: 0.544704 difference from random loss: 0.005399 \n",
      "epoch: 46 dis loss: 0.698078 dis accu: 0.545556 difference from random loss: 0.004931 \n",
      "epoch: 47 dis loss: 0.694219 dis accu: 0.552137 difference from random loss: 0.001072 \n",
      "epoch: 48 dis loss: 0.697743 dis accu: 0.547501 difference from random loss: 0.004596 \n",
      "epoch: 49 dis loss: 0.699991 dis accu: 0.540601 difference from random loss: 0.006844 \n",
      "Adversarial training for ST slide 151670: \n",
      "Start adversarial training...\n",
      "Discriminator target loss: 0.6931471805599453\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335997ac4e8e4e81aef62421a047cac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a08cb421c548248b703a040e4442d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Discriminator):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517c9c29fd0142d6a5a38a76da8e6ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Encoder):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 dis loss: 0.855638 dis accu: 0.352894 difference from random loss: 0.162491 <-- new best difference from random loss\n",
      "epoch: 1 dis loss: 0.783577 dis accu: 0.49992 difference from random loss: 0.090429 <-- new best difference from random loss\n",
      "epoch: 2 dis loss: 0.811625 dis accu: 0.470926 difference from random loss: 0.118478 \n",
      "epoch: 3 dis loss: 0.829318 dis accu: 0.453614 difference from random loss: 0.13617 \n",
      "epoch: 4 dis loss: 0.826895 dis accu: 0.454948 difference from random loss: 0.133747 \n",
      "epoch: 5 dis loss: 0.800832 dis accu: 0.475967 difference from random loss: 0.107684 \n",
      "epoch: 6 dis loss: 0.745957 dis accu: 0.536997 difference from random loss: 0.05281 <-- new best difference from random loss\n",
      "epoch: 7 dis loss: 0.713225 dis accu: 0.575834 difference from random loss: 0.020078 <-- new best difference from random loss\n",
      "epoch: 8 dis loss: 0.662328 dis accu: 0.638517 difference from random loss: 0.030819 \n",
      "epoch: 9 dis loss: 0.61558 dis accu: 0.689677 difference from random loss: 0.077567 \n",
      "epoch: 10 dis loss: 0.562258 dis accu: 0.748733 difference from random loss: 0.130889 \n",
      "epoch: 11 dis loss: 0.566746 dis accu: 0.743905 difference from random loss: 0.126401 \n",
      "epoch: 12 dis loss: 0.68405 dis accu: 0.626727 difference from random loss: 0.009097 <-- new best difference from random loss\n",
      "epoch: 13 dis loss: 0.57557 dis accu: 0.728141 difference from random loss: 0.117577 \n",
      "epoch: 14 dis loss: 0.700107 dis accu: 0.608136 difference from random loss: 0.00696 <-- new best difference from random loss\n",
      "epoch: 15 dis loss: 0.572728 dis accu: 0.728701 difference from random loss: 0.120419 \n",
      "epoch: 16 dis loss: 0.59611 dis accu: 0.710883 difference from random loss: 0.097037 \n",
      "epoch: 17 dis loss: 0.695286 dis accu: 0.601921 difference from random loss: 0.002139 <-- new best difference from random loss\n",
      "epoch: 18 dis loss: 0.659054 dis accu: 0.623606 difference from random loss: 0.034093 \n",
      "epoch: 19 dis loss: 0.641336 dis accu: 0.637983 difference from random loss: 0.051811 \n",
      "epoch: 20 dis loss: 0.67185 dis accu: 0.592531 difference from random loss: 0.021297 \n",
      "epoch: 21 dis loss: 0.730216 dis accu: 0.505041 difference from random loss: 0.037069 \n",
      "epoch: 22 dis loss: 0.765205 dis accu: 0.440117 difference from random loss: 0.072058 \n",
      "epoch: 23 dis loss: 0.754732 dis accu: 0.436863 difference from random loss: 0.061585 \n",
      "epoch: 24 dis loss: 0.747708 dis accu: 0.443825 difference from random loss: 0.054561 \n",
      "epoch: 25 dis loss: 0.718664 dis accu: 0.47394 difference from random loss: 0.025517 \n",
      "epoch: 26 dis loss: 0.722732 dis accu: 0.467271 difference from random loss: 0.029585 \n",
      "epoch: 27 dis loss: 0.719172 dis accu: 0.469752 difference from random loss: 0.026024 \n",
      "epoch: 28 dis loss: 0.708815 dis accu: 0.499387 difference from random loss: 0.015668 \n",
      "epoch: 29 dis loss: 0.70302 dis accu: 0.532729 difference from random loss: 0.009873 \n",
      "epoch: 30 dis loss: 0.697881 dis accu: 0.542918 difference from random loss: 0.004734 \n",
      "epoch: 31 dis loss: 0.698737 dis accu: 0.541611 difference from random loss: 0.00559 \n",
      "epoch: 32 dis loss: 0.697563 dis accu: 0.550467 difference from random loss: 0.004416 \n",
      "epoch: 33 dis loss: 0.700186 dis accu: 0.544065 difference from random loss: 0.007039 \n",
      "epoch: 34 dis loss: 0.694402 dis accu: 0.557322 difference from random loss: 0.001255 <-- new best difference from random loss\n",
      "epoch: 35 dis loss: 0.69558 dis accu: 0.556869 difference from random loss: 0.002433 \n",
      "epoch: 36 dis loss: 0.695627 dis accu: 0.543852 difference from random loss: 0.00248 \n",
      "epoch: 37 dis loss: 0.693259 dis accu: 0.547479 difference from random loss: 0.000112 <-- new best difference from random loss\n",
      "epoch: 38 dis loss: 0.694033 dis accu: 0.543852 difference from random loss: 0.000886 \n",
      "epoch: 39 dis loss: 0.695593 dis accu: 0.536436 difference from random loss: 0.002445 \n",
      "epoch: 40 dis loss: 0.692919 dis accu: 0.538277 difference from random loss: 0.000228 \n",
      "epoch: 41 dis loss: 0.690985 dis accu: 0.546919 difference from random loss: 0.002162 \n",
      "epoch: 42 dis loss: 0.693024 dis accu: 0.542518 difference from random loss: 0.000123 \n",
      "epoch: 43 dis loss: 0.692304 dis accu: 0.546626 difference from random loss: 0.000843 \n",
      "epoch: 44 dis loss: 0.691811 dis accu: 0.54844 difference from random loss: 0.001336 \n",
      "epoch: 45 dis loss: 0.694437 dis accu: 0.542705 difference from random loss: 0.00129 \n",
      "epoch: 46 dis loss: 0.691789 dis accu: 0.547879 difference from random loss: 0.001358 \n",
      "epoch: 47 dis loss: 0.695971 dis accu: 0.537797 difference from random loss: 0.002824 \n",
      "epoch: 48 dis loss: 0.693038 dis accu: 0.543825 difference from random loss: 0.00011 <-- new best difference from random loss\n",
      "epoch: 49 dis loss: 0.692855 dis accu: 0.547293 difference from random loss: 0.000292 \n",
      "Adversarial training for ST slide 151507: \n",
      "Start adversarial training...\n",
      "Discriminator target loss: 0.6931471805599453\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7119d8bfc5b6456481dac623d208f3d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efeac71c13c442d19ec9d4c96bf055e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Discriminator):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d7921e6aba46ea898fbb1893e43da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Encoder):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 dis loss: 0.855094 dis accu: 0.348472 difference from random loss: 0.161947 <-- new best difference from random loss\n",
      "epoch: 1 dis loss: 0.781461 dis accu: 0.496586 difference from random loss: 0.088314 <-- new best difference from random loss\n",
      "epoch: 2 dis loss: 0.808495 dis accu: 0.472361 difference from random loss: 0.115348 \n",
      "epoch: 3 dis loss: 0.831367 dis accu: 0.447296 difference from random loss: 0.13822 \n",
      "epoch: 4 dis loss: 0.828088 dis accu: 0.451333 difference from random loss: 0.13494 \n",
      "epoch: 5 dis loss: 0.784251 dis accu: 0.491437 difference from random loss: 0.091104 \n",
      "epoch: 6 dis loss: 0.751643 dis accu: 0.525471 difference from random loss: 0.058496 <-- new best difference from random loss\n",
      "epoch: 7 dis loss: 0.699306 dis accu: 0.58617 difference from random loss: 0.006159 <-- new best difference from random loss\n",
      "epoch: 8 dis loss: 0.676205 dis accu: 0.61389 difference from random loss: 0.016942 \n",
      "epoch: 9 dis loss: 0.660332 dis accu: 0.633725 difference from random loss: 0.032815 \n",
      "epoch: 10 dis loss: 0.623283 dis accu: 0.672068 difference from random loss: 0.069864 \n",
      "epoch: 11 dis loss: 0.582206 dis accu: 0.715044 difference from random loss: 0.110941 \n",
      "epoch: 12 dis loss: 0.59824 dis accu: 0.693123 difference from random loss: 0.094907 \n",
      "epoch: 13 dis loss: 0.558375 dis accu: 0.734202 difference from random loss: 0.134772 \n",
      "epoch: 14 dis loss: 0.685463 dis accu: 0.605111 difference from random loss: 0.007684 \n",
      "epoch: 15 dis loss: 0.631605 dis accu: 0.655214 difference from random loss: 0.061542 \n",
      "epoch: 16 dis loss: 0.67247 dis accu: 0.614324 difference from random loss: 0.020677 \n",
      "epoch: 17 dis loss: 0.613248 dis accu: 0.670253 difference from random loss: 0.0799 \n",
      "epoch: 18 dis loss: 0.632101 dis accu: 0.648006 difference from random loss: 0.061047 \n",
      "epoch: 19 dis loss: 0.735416 dis accu: 0.52745 difference from random loss: 0.042269 \n",
      "epoch: 20 dis loss: 0.680827 dis accu: 0.56292 difference from random loss: 0.01232 \n",
      "epoch: 21 dis loss: 0.712772 dis accu: 0.529536 difference from random loss: 0.019625 \n",
      "epoch: 22 dis loss: 0.675575 dis accu: 0.554195 difference from random loss: 0.017572 \n",
      "epoch: 23 dis loss: 0.703325 dis accu: 0.518209 difference from random loss: 0.010178 \n",
      "epoch: 24 dis loss: 0.718733 dis accu: 0.498455 difference from random loss: 0.025586 \n",
      "epoch: 25 dis loss: 0.732256 dis accu: 0.473688 difference from random loss: 0.039109 \n",
      "epoch: 26 dis loss: 0.731147 dis accu: 0.466752 difference from random loss: 0.038 \n",
      "epoch: 27 dis loss: 0.734316 dis accu: 0.472659 difference from random loss: 0.041169 \n",
      "epoch: 28 dis loss: 0.709941 dis accu: 0.495556 difference from random loss: 0.016793 \n",
      "epoch: 29 dis loss: 0.707785 dis accu: 0.513901 difference from random loss: 0.014638 \n",
      "epoch: 30 dis loss: 0.718313 dis accu: 0.515202 difference from random loss: 0.025165 \n",
      "epoch: 31 dis loss: 0.704256 dis accu: 0.525986 difference from random loss: 0.011109 \n",
      "epoch: 32 dis loss: 0.701876 dis accu: 0.518751 difference from random loss: 0.008729 \n",
      "epoch: 33 dis loss: 0.696611 dis accu: 0.51783 difference from random loss: 0.003464 <-- new best difference from random loss\n",
      "epoch: 34 dis loss: 0.69687 dis accu: 0.519564 difference from random loss: 0.003723 \n",
      "epoch: 35 dis loss: 0.69787 dis accu: 0.514931 difference from random loss: 0.004722 \n",
      "epoch: 36 dis loss: 0.693856 dis accu: 0.52352 difference from random loss: 0.000708 <-- new best difference from random loss\n",
      "epoch: 37 dis loss: 0.690207 dis accu: 0.532165 difference from random loss: 0.002941 \n",
      "epoch: 38 dis loss: 0.703699 dis accu: 0.511462 difference from random loss: 0.010552 \n",
      "epoch: 39 dis loss: 0.691375 dis accu: 0.53967 difference from random loss: 0.001772 \n",
      "epoch: 40 dis loss: 0.690673 dis accu: 0.542841 difference from random loss: 0.002475 \n",
      "epoch: 41 dis loss: 0.703484 dis accu: 0.517369 difference from random loss: 0.010337 \n",
      "epoch: 42 dis loss: 0.693501 dis accu: 0.531054 difference from random loss: 0.000354 <-- new best difference from random loss\n",
      "epoch: 43 dis loss: 0.692837 dis accu: 0.539698 difference from random loss: 0.00031 <-- new best difference from random loss\n",
      "epoch: 44 dis loss: 0.697011 dis accu: 0.524171 difference from random loss: 0.003864 \n",
      "epoch: 45 dis loss: 0.696774 dis accu: 0.524848 difference from random loss: 0.003627 \n",
      "epoch: 46 dis loss: 0.692649 dis accu: 0.534549 difference from random loss: 0.000498 \n",
      "epoch: 47 dis loss: 0.689527 dis accu: 0.538912 difference from random loss: 0.00362 \n",
      "epoch: 48 dis loss: 0.69713 dis accu: 0.523791 difference from random loss: 0.003983 \n",
      "epoch: 49 dis loss: 0.694703 dis accu: 0.525011 difference from random loss: 0.001556 \n",
      "Adversarial training for ST slide 151674: \n",
      "Start adversarial training...\n",
      "Discriminator target loss: 0.6931471805599453\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "401cee9d1dfe449d86848b953da704b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e2bff627514507816256f393ae764a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Discriminator):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6757f971e1ca4f3d851fa4a6344fd57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Encoder):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 dis loss: 0.857255 dis accu: 0.349902 difference from random loss: 0.164108 <-- new best difference from random loss\n",
      "epoch: 1 dis loss: 0.8011 dis accu: 0.482758 difference from random loss: 0.107953 <-- new best difference from random loss\n",
      "epoch: 2 dis loss: 0.823445 dis accu: 0.458308 difference from random loss: 0.130298 \n",
      "epoch: 3 dis loss: 0.823482 dis accu: 0.453486 difference from random loss: 0.130335 \n",
      "epoch: 4 dis loss: 0.821932 dis accu: 0.452939 difference from random loss: 0.128785 \n",
      "epoch: 5 dis loss: 0.78212 dis accu: 0.495426 difference from random loss: 0.088973 <-- new best difference from random loss\n",
      "epoch: 6 dis loss: 0.745809 dis accu: 0.537287 difference from random loss: 0.052661 <-- new best difference from random loss\n",
      "epoch: 7 dis loss: 0.706847 dis accu: 0.58337 difference from random loss: 0.0137 <-- new best difference from random loss\n",
      "epoch: 8 dis loss: 0.66092 dis accu: 0.640245 difference from random loss: 0.032227 \n",
      "epoch: 9 dis loss: 0.624444 dis accu: 0.679004 difference from random loss: 0.068703 \n",
      "epoch: 10 dis loss: 0.629852 dis accu: 0.675564 difference from random loss: 0.063295 \n",
      "epoch: 11 dis loss: 0.588212 dis accu: 0.723211 difference from random loss: 0.104935 \n",
      "epoch: 12 dis loss: 0.624636 dis accu: 0.685547 difference from random loss: 0.068511 \n",
      "epoch: 13 dis loss: 0.469369 dis accu: 0.829402 difference from random loss: 0.223778 \n",
      "epoch: 14 dis loss: 0.589072 dis accu: 0.74203 difference from random loss: 0.104075 \n",
      "epoch: 15 dis loss: 0.63131 dis accu: 0.690682 difference from random loss: 0.061837 \n",
      "epoch: 16 dis loss: 0.568872 dis accu: 0.743021 difference from random loss: 0.124275 \n",
      "epoch: 17 dis loss: 0.688974 dis accu: 0.628568 difference from random loss: 0.004173 <-- new best difference from random loss\n",
      "epoch: 18 dis loss: 0.679115 dis accu: 0.613502 difference from random loss: 0.014032 \n",
      "epoch: 19 dis loss: 0.753364 dis accu: 0.518546 difference from random loss: 0.060216 \n",
      "epoch: 20 dis loss: 0.740273 dis accu: 0.502932 difference from random loss: 0.047126 \n",
      "epoch: 21 dis loss: 0.757306 dis accu: 0.464851 difference from random loss: 0.064159 \n",
      "epoch: 22 dis loss: 0.734979 dis accu: 0.482419 difference from random loss: 0.041832 \n",
      "epoch: 23 dis loss: 0.735801 dis accu: 0.464642 difference from random loss: 0.042654 \n",
      "epoch: 24 dis loss: 0.724594 dis accu: 0.467457 difference from random loss: 0.031447 \n",
      "epoch: 25 dis loss: 0.721366 dis accu: 0.46605 difference from random loss: 0.028219 \n",
      "epoch: 26 dis loss: 0.711875 dis accu: 0.475042 difference from random loss: 0.018728 \n",
      "epoch: 27 dis loss: 0.700988 dis accu: 0.491125 difference from random loss: 0.007841 \n",
      "epoch: 28 dis loss: 0.695289 dis accu: 0.50679 difference from random loss: 0.002142 <-- new best difference from random loss\n",
      "epoch: 29 dis loss: 0.697063 dis accu: 0.501082 difference from random loss: 0.003916 \n",
      "epoch: 30 dis loss: 0.695203 dis accu: 0.50202 difference from random loss: 0.002056 <-- new best difference from random loss\n",
      "epoch: 31 dis loss: 0.694941 dis accu: 0.499101 difference from random loss: 0.001794 <-- new best difference from random loss\n",
      "epoch: 32 dis loss: 0.693691 dis accu: 0.50361 difference from random loss: 0.000544 <-- new best difference from random loss\n",
      "epoch: 33 dis loss: 0.68972 dis accu: 0.518624 difference from random loss: 0.003427 \n",
      "epoch: 34 dis loss: 0.693789 dis accu: 0.516043 difference from random loss: 0.000642 \n",
      "epoch: 35 dis loss: 0.698188 dis accu: 0.510126 difference from random loss: 0.005041 \n",
      "epoch: 36 dis loss: 0.691586 dis accu: 0.535905 difference from random loss: 0.001561 \n",
      "epoch: 37 dis loss: 0.688605 dis accu: 0.543099 difference from random loss: 0.004542 \n",
      "epoch: 38 dis loss: 0.692624 dis accu: 0.538851 difference from random loss: 0.000523 <-- new best difference from random loss\n",
      "epoch: 39 dis loss: 0.692986 dis accu: 0.535436 difference from random loss: 0.000161 <-- new best difference from random loss\n",
      "epoch: 40 dis loss: 0.694815 dis accu: 0.535671 difference from random loss: 0.001668 \n",
      "epoch: 41 dis loss: 0.695708 dis accu: 0.534419 difference from random loss: 0.00256 \n",
      "epoch: 42 dis loss: 0.700973 dis accu: 0.521804 difference from random loss: 0.007825 \n",
      "epoch: 43 dis loss: 0.699305 dis accu: 0.528789 difference from random loss: 0.006158 \n",
      "epoch: 44 dis loss: 0.697195 dis accu: 0.528581 difference from random loss: 0.004048 \n",
      "epoch: 45 dis loss: 0.695118 dis accu: 0.537469 difference from random loss: 0.001971 \n",
      "epoch: 46 dis loss: 0.702163 dis accu: 0.520605 difference from random loss: 0.009016 \n",
      "epoch: 47 dis loss: 0.698971 dis accu: 0.531213 difference from random loss: 0.005824 \n",
      "epoch: 48 dis loss: 0.699963 dis accu: 0.528685 difference from random loss: 0.006816 \n",
      "epoch: 49 dis loss: 0.699416 dis accu: 0.528294 difference from random loss: 0.006269 \n",
      "Adversarial training for ST slide 151676: \n",
      "Start adversarial training...\n",
      "Discriminator target loss: 0.6931471805599453\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937c33d95c2d4263b3eece2a4c9a7e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2c09dcde724eab8bc858743d1197f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Discriminator):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d343fcc2822f4cbfb21f448db93966f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Encoder):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 dis loss: 0.854911 dis accu: 0.352761 difference from random loss: 0.161764 <-- new best difference from random loss\n",
      "epoch: 1 dis loss: 0.796866 dis accu: 0.483298 difference from random loss: 0.103719 <-- new best difference from random loss\n",
      "epoch: 2 dis loss: 0.8158 dis accu: 0.464745 difference from random loss: 0.122653 \n",
      "epoch: 3 dis loss: 0.826793 dis accu: 0.450885 difference from random loss: 0.133646 \n",
      "epoch: 4 dis loss: 0.819125 dis accu: 0.455871 difference from random loss: 0.125978 \n",
      "epoch: 5 dis loss: 0.79016 dis accu: 0.485389 difference from random loss: 0.097013 <-- new best difference from random loss\n",
      "epoch: 6 dis loss: 0.750263 dis accu: 0.533995 difference from random loss: 0.057115 <-- new best difference from random loss\n",
      "epoch: 7 dis loss: 0.728542 dis accu: 0.560563 difference from random loss: 0.035395 <-- new best difference from random loss\n",
      "epoch: 8 dis loss: 0.658782 dis accu: 0.638874 difference from random loss: 0.034366 <-- new best difference from random loss\n",
      "epoch: 9 dis loss: 0.623729 dis accu: 0.676997 difference from random loss: 0.069418 \n",
      "epoch: 10 dis loss: 0.600108 dis accu: 0.705228 difference from random loss: 0.093039 \n",
      "epoch: 11 dis loss: 0.565335 dis accu: 0.741206 difference from random loss: 0.127812 \n",
      "epoch: 12 dis loss: 0.529686 dis accu: 0.773753 difference from random loss: 0.163462 \n",
      "epoch: 13 dis loss: 0.443381 dis accu: 0.838338 difference from random loss: 0.249766 \n",
      "epoch: 14 dis loss: 0.59628 dis accu: 0.716622 difference from random loss: 0.096867 \n",
      "epoch: 15 dis loss: 0.555388 dis accu: 0.749383 difference from random loss: 0.137759 \n",
      "epoch: 16 dis loss: 0.590166 dis accu: 0.716354 difference from random loss: 0.102981 \n",
      "epoch: 17 dis loss: 0.734656 dis accu: 0.56807 difference from random loss: 0.041509 \n",
      "epoch: 18 dis loss: 0.718203 dis accu: 0.564504 difference from random loss: 0.025056 <-- new best difference from random loss\n",
      "epoch: 19 dis loss: 0.648499 dis accu: 0.634745 difference from random loss: 0.044648 \n",
      "epoch: 20 dis loss: 0.72097 dis accu: 0.539893 difference from random loss: 0.027823 \n",
      "epoch: 21 dis loss: 0.715122 dis accu: 0.522949 difference from random loss: 0.021975 <-- new best difference from random loss\n",
      "epoch: 22 dis loss: 0.712733 dis accu: 0.512118 difference from random loss: 0.019586 <-- new best difference from random loss\n",
      "epoch: 23 dis loss: 0.708593 dis accu: 0.505255 difference from random loss: 0.015446 <-- new best difference from random loss\n",
      "epoch: 24 dis loss: 0.724089 dis accu: 0.479303 difference from random loss: 0.030942 \n",
      "epoch: 25 dis loss: 0.727153 dis accu: 0.466595 difference from random loss: 0.034006 \n",
      "epoch: 26 dis loss: 0.718137 dis accu: 0.473485 difference from random loss: 0.02499 \n",
      "epoch: 27 dis loss: 0.718876 dis accu: 0.476488 difference from random loss: 0.025729 \n",
      "epoch: 28 dis loss: 0.712396 dis accu: 0.490134 difference from random loss: 0.019248 \n",
      "epoch: 29 dis loss: 0.712351 dis accu: 0.491501 difference from random loss: 0.019204 \n",
      "epoch: 30 dis loss: 0.711831 dis accu: 0.492601 difference from random loss: 0.018684 \n",
      "epoch: 31 dis loss: 0.706233 dis accu: 0.506568 difference from random loss: 0.013086 <-- new best difference from random loss\n",
      "epoch: 32 dis loss: 0.702917 dis accu: 0.52504 difference from random loss: 0.00977 <-- new best difference from random loss\n",
      "epoch: 33 dis loss: 0.701077 dis accu: 0.544397 difference from random loss: 0.00793 <-- new best difference from random loss\n",
      "epoch: 34 dis loss: 0.703012 dis accu: 0.549115 difference from random loss: 0.009864 \n",
      "epoch: 35 dis loss: 0.698987 dis accu: 0.561072 difference from random loss: 0.005839 <-- new best difference from random loss\n",
      "epoch: 36 dis loss: 0.698654 dis accu: 0.561153 difference from random loss: 0.005507 <-- new best difference from random loss\n",
      "epoch: 37 dis loss: 0.698343 dis accu: 0.556702 difference from random loss: 0.005196 <-- new best difference from random loss\n",
      "epoch: 38 dis loss: 0.694367 dis accu: 0.559196 difference from random loss: 0.00122 <-- new best difference from random loss\n",
      "epoch: 39 dis loss: 0.694686 dis accu: 0.553968 difference from random loss: 0.001539 \n",
      "epoch: 40 dis loss: 0.69389 dis accu: 0.548901 difference from random loss: 0.000743 <-- new best difference from random loss\n",
      "epoch: 41 dis loss: 0.69563 dis accu: 0.54496 difference from random loss: 0.002483 \n",
      "epoch: 42 dis loss: 0.691989 dis accu: 0.549088 difference from random loss: 0.001158 \n",
      "epoch: 43 dis loss: 0.69608 dis accu: 0.540134 difference from random loss: 0.002933 \n",
      "epoch: 44 dis loss: 0.695839 dis accu: 0.539786 difference from random loss: 0.002691 \n",
      "epoch: 45 dis loss: 0.694687 dis accu: 0.543271 difference from random loss: 0.00154 \n",
      "epoch: 46 dis loss: 0.694168 dis accu: 0.544531 difference from random loss: 0.001021 \n",
      "epoch: 47 dis loss: 0.693659 dis accu: 0.543405 difference from random loss: 0.000511 <-- new best difference from random loss\n",
      "epoch: 48 dis loss: 0.695947 dis accu: 0.536863 difference from random loss: 0.0028 \n",
      "epoch: 49 dis loss: 0.695038 dis accu: 0.542869 difference from random loss: 0.00189 \n",
      "Adversarial training for ST slide 151675: \n",
      "Start adversarial training...\n",
      "Discriminator target loss: 0.6931471805599454\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6da3535544c4ec2a027cb26f5e67315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934d3c97b3104da4afed8a802522a904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Discriminator):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b547e5144bbc4011be71f0ea231556c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Encoder):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 dis loss: 0.856805 dis accu: 0.349868 difference from random loss: 0.163658 <-- new best difference from random loss\n",
      "epoch: 1 dis loss: 0.795674 dis accu: 0.485116 difference from random loss: 0.102526 <-- new best difference from random loss\n",
      "epoch: 2 dis loss: 0.811558 dis accu: 0.469415 difference from random loss: 0.118411 \n",
      "epoch: 3 dis loss: 0.827525 dis accu: 0.449499 difference from random loss: 0.134378 \n",
      "epoch: 4 dis loss: 0.816585 dis accu: 0.455638 difference from random loss: 0.123437 \n",
      "epoch: 5 dis loss: 0.784193 dis accu: 0.490332 difference from random loss: 0.091046 <-- new best difference from random loss\n",
      "epoch: 6 dis loss: 0.737924 dis accu: 0.54568 difference from random loss: 0.044776 <-- new best difference from random loss\n",
      "epoch: 7 dis loss: 0.705346 dis accu: 0.587671 difference from random loss: 0.012199 <-- new best difference from random loss\n",
      "epoch: 8 dis loss: 0.653277 dis accu: 0.645179 difference from random loss: 0.03987 \n",
      "epoch: 9 dis loss: 0.632301 dis accu: 0.672524 difference from random loss: 0.060846 \n",
      "epoch: 10 dis loss: 0.601772 dis accu: 0.702529 difference from random loss: 0.091375 \n",
      "epoch: 11 dis loss: 0.588878 dis accu: 0.721338 difference from random loss: 0.10427 \n",
      "epoch: 12 dis loss: 0.592127 dis accu: 0.719731 difference from random loss: 0.101021 \n",
      "epoch: 13 dis loss: 0.617734 dis accu: 0.694731 difference from random loss: 0.075413 \n",
      "epoch: 14 dis loss: 0.61048 dis accu: 0.693414 difference from random loss: 0.082667 \n",
      "epoch: 15 dis loss: 0.575265 dis accu: 0.733614 difference from random loss: 0.117882 \n",
      "epoch: 16 dis loss: 0.555732 dis accu: 0.754268 difference from random loss: 0.137416 \n",
      "epoch: 17 dis loss: 0.630436 dis accu: 0.682034 difference from random loss: 0.062711 \n",
      "epoch: 18 dis loss: 0.514209 dis accu: 0.786196 difference from random loss: 0.178938 \n",
      "epoch: 19 dis loss: 0.774005 dis accu: 0.525132 difference from random loss: 0.080858 \n",
      "epoch: 20 dis loss: 0.674389 dis accu: 0.603398 difference from random loss: 0.018758 \n",
      "epoch: 21 dis loss: 0.729439 dis accu: 0.524658 difference from random loss: 0.036292 \n",
      "epoch: 22 dis loss: 0.739115 dis accu: 0.48941 difference from random loss: 0.045968 \n",
      "epoch: 23 dis loss: 0.685786 dis accu: 0.543177 difference from random loss: 0.007361 <-- new best difference from random loss\n",
      "epoch: 24 dis loss: 0.768132 dis accu: 0.440095 difference from random loss: 0.074984 \n",
      "epoch: 25 dis loss: 0.712682 dis accu: 0.508351 difference from random loss: 0.019535 \n",
      "epoch: 26 dis loss: 0.738953 dis accu: 0.445943 difference from random loss: 0.045805 \n",
      "epoch: 27 dis loss: 0.724728 dis accu: 0.457165 difference from random loss: 0.03158 \n",
      "epoch: 28 dis loss: 0.716325 dis accu: 0.475738 difference from random loss: 0.023178 \n",
      "epoch: 29 dis loss: 0.725629 dis accu: 0.46304 difference from random loss: 0.032482 \n",
      "epoch: 30 dis loss: 0.717117 dis accu: 0.479057 difference from random loss: 0.02397 \n",
      "epoch: 31 dis loss: 0.718066 dis accu: 0.465885 difference from random loss: 0.024919 \n",
      "epoch: 32 dis loss: 0.705923 dis accu: 0.477608 difference from random loss: 0.012776 \n",
      "epoch: 33 dis loss: 0.700468 dis accu: 0.497234 difference from random loss: 0.007321 <-- new best difference from random loss\n",
      "epoch: 34 dis loss: 0.697715 dis accu: 0.516307 difference from random loss: 0.004568 <-- new best difference from random loss\n",
      "epoch: 35 dis loss: 0.696933 dis accu: 0.522102 difference from random loss: 0.003786 <-- new best difference from random loss\n",
      "epoch: 36 dis loss: 0.69948 dis accu: 0.515674 difference from random loss: 0.006333 \n",
      "epoch: 37 dis loss: 0.701196 dis accu: 0.505506 difference from random loss: 0.008049 \n",
      "epoch: 38 dis loss: 0.701987 dis accu: 0.505295 difference from random loss: 0.00884 \n",
      "epoch: 39 dis loss: 0.701594 dis accu: 0.506797 difference from random loss: 0.008446 \n",
      "epoch: 40 dis loss: 0.697994 dis accu: 0.516438 difference from random loss: 0.004847 \n",
      "epoch: 41 dis loss: 0.69881 dis accu: 0.511249 difference from random loss: 0.005662 \n",
      "epoch: 42 dis loss: 0.696591 dis accu: 0.520522 difference from random loss: 0.003444 <-- new best difference from random loss\n",
      "epoch: 43 dis loss: 0.695306 dis accu: 0.522234 difference from random loss: 0.002159 <-- new best difference from random loss\n",
      "epoch: 44 dis loss: 0.696263 dis accu: 0.519573 difference from random loss: 0.003116 \n",
      "epoch: 45 dis loss: 0.695231 dis accu: 0.522972 difference from random loss: 0.002084 <-- new best difference from random loss\n",
      "epoch: 46 dis loss: 0.695181 dis accu: 0.524631 difference from random loss: 0.002034 <-- new best difference from random loss\n",
      "epoch: 47 dis loss: 0.696334 dis accu: 0.521154 difference from random loss: 0.003187 \n",
      "epoch: 48 dis loss: 0.694593 dis accu: 0.526344 difference from random loss: 0.001446 <-- new best difference from random loss\n",
      "epoch: 49 dis loss: 0.694933 dis accu: 0.521654 difference from random loss: 0.001786 \n",
      "Adversarial training for ST slide 151673: \n",
      "Start adversarial training...\n",
      "Discriminator target loss: 0.6931471805599453\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d00d9d89c54219b105d65716547d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7cb25eacce405c9cc2d73b68b4c84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Discriminator):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d54c3169664275919bfc71a70647de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Encoder):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 dis loss: 0.857787 dis accu: 0.346171 difference from random loss: 0.164639 <-- new best difference from random loss\n",
      "epoch: 1 dis loss: 0.783913 dis accu: 0.501872 difference from random loss: 0.090766 <-- new best difference from random loss\n",
      "epoch: 2 dis loss: 0.821676 dis accu: 0.460846 difference from random loss: 0.128529 \n",
      "epoch: 3 dis loss: 0.831412 dis accu: 0.447205 difference from random loss: 0.138265 \n",
      "epoch: 4 dis loss: 0.818262 dis accu: 0.457782 difference from random loss: 0.125115 \n",
      "epoch: 5 dis loss: 0.784895 dis accu: 0.486294 difference from random loss: 0.091748 \n",
      "epoch: 6 dis loss: 0.743844 dis accu: 0.537191 difference from random loss: 0.050696 <-- new best difference from random loss\n",
      "epoch: 7 dis loss: 0.693684 dis accu: 0.599293 difference from random loss: 0.000536 <-- new best difference from random loss\n",
      "epoch: 8 dis loss: 0.660399 dis accu: 0.638958 difference from random loss: 0.032748 \n",
      "epoch: 9 dis loss: 0.641871 dis accu: 0.656945 difference from random loss: 0.051276 \n",
      "epoch: 10 dis loss: 0.613566 dis accu: 0.691949 difference from random loss: 0.079581 \n",
      "epoch: 11 dis loss: 0.591531 dis accu: 0.718654 difference from random loss: 0.101616 \n",
      "epoch: 12 dis loss: 0.464039 dis accu: 0.829402 difference from random loss: 0.229108 \n",
      "epoch: 13 dis loss: 0.501619 dis accu: 0.808718 difference from random loss: 0.191528 \n",
      "epoch: 14 dis loss: 0.531022 dis accu: 0.781018 difference from random loss: 0.162125 \n",
      "epoch: 15 dis loss: 0.732856 dis accu: 0.601152 difference from random loss: 0.039709 \n",
      "epoch: 16 dis loss: 0.578817 dis accu: 0.72643 difference from random loss: 0.11433 \n",
      "epoch: 17 dis loss: 0.667042 dis accu: 0.641602 difference from random loss: 0.026105 \n",
      "epoch: 18 dis loss: 0.728145 dis accu: 0.552428 difference from random loss: 0.034998 \n",
      "epoch: 19 dis loss: 0.720279 dis accu: 0.542689 difference from random loss: 0.027132 \n",
      "epoch: 20 dis loss: 0.719446 dis accu: 0.527923 difference from random loss: 0.026298 \n",
      "epoch: 21 dis loss: 0.700431 dis accu: 0.547297 difference from random loss: 0.007284 \n",
      "epoch: 22 dis loss: 0.705455 dis accu: 0.520382 difference from random loss: 0.012308 \n",
      "epoch: 23 dis loss: 0.722272 dis accu: 0.485587 difference from random loss: 0.029124 \n",
      "epoch: 24 dis loss: 0.732812 dis accu: 0.47391 difference from random loss: 0.039665 \n",
      "epoch: 25 dis loss: 0.716663 dis accu: 0.498207 difference from random loss: 0.023516 \n",
      "epoch: 26 dis loss: 0.728671 dis accu: 0.471894 difference from random loss: 0.035524 \n",
      "epoch: 27 dis loss: 0.716542 dis accu: 0.495431 difference from random loss: 0.023395 \n",
      "epoch: 28 dis loss: 0.707346 dis accu: 0.512187 difference from random loss: 0.014199 \n",
      "epoch: 29 dis loss: 0.705686 dis accu: 0.514989 difference from random loss: 0.012539 \n",
      "epoch: 30 dis loss: 0.70159 dis accu: 0.526325 difference from random loss: 0.008443 \n",
      "epoch: 31 dis loss: 0.704872 dis accu: 0.528917 difference from random loss: 0.011725 \n",
      "epoch: 32 dis loss: 0.705553 dis accu: 0.522948 difference from random loss: 0.012405 \n",
      "epoch: 33 dis loss: 0.707714 dis accu: 0.51601 difference from random loss: 0.014567 \n",
      "epoch: 34 dis loss: 0.706958 dis accu: 0.510041 difference from random loss: 0.013811 \n",
      "epoch: 35 dis loss: 0.702577 dis accu: 0.516743 difference from random loss: 0.00943 \n",
      "epoch: 36 dis loss: 0.697964 dis accu: 0.535751 difference from random loss: 0.004817 \n",
      "epoch: 37 dis loss: 0.694296 dis accu: 0.554287 difference from random loss: 0.001148 \n",
      "epoch: 38 dis loss: 0.695731 dis accu: 0.553685 difference from random loss: 0.002584 \n",
      "epoch: 39 dis loss: 0.693996 dis accu: 0.557089 difference from random loss: 0.000849 \n",
      "epoch: 40 dis loss: 0.696336 dis accu: 0.545647 difference from random loss: 0.003189 \n",
      "epoch: 41 dis loss: 0.694402 dis accu: 0.55447 difference from random loss: 0.001255 \n",
      "epoch: 42 dis loss: 0.69448 dis accu: 0.546773 difference from random loss: 0.001332 \n",
      "epoch: 43 dis loss: 0.694636 dis accu: 0.549182 difference from random loss: 0.001489 \n",
      "epoch: 44 dis loss: 0.696568 dis accu: 0.537034 difference from random loss: 0.003421 \n",
      "epoch: 45 dis loss: 0.695271 dis accu: 0.545098 difference from random loss: 0.002124 \n",
      "epoch: 46 dis loss: 0.696154 dis accu: 0.539783 difference from random loss: 0.003007 \n",
      "epoch: 47 dis loss: 0.694254 dis accu: 0.542898 difference from random loss: 0.001106 \n",
      "epoch: 48 dis loss: 0.695304 dis accu: 0.541694 difference from random loss: 0.002157 \n",
      "epoch: 49 dis loss: 0.694377 dis accu: 0.54261 difference from random loss: 0.00123 \n",
      "Adversarial training for ST slide 151672: \n",
      "Start adversarial training...\n",
      "Discriminator target loss: 0.6931471805599453\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9046b1d9c5d34d2e8d0a2732f3773fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbcfc9e19efd4438822ab67d08c53ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Discriminator):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d931bcbf5942508f479104f9793a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Encoder):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 dis loss: 0.858168 dis accu: 0.338066 difference from random loss: 0.16502 <-- new best difference from random loss\n",
      "epoch: 1 dis loss: 0.794227 dis accu: 0.489332 difference from random loss: 0.10108 <-- new best difference from random loss\n",
      "epoch: 2 dis loss: 0.809445 dis accu: 0.469844 difference from random loss: 0.116298 \n",
      "epoch: 3 dis loss: 0.819991 dis accu: 0.458316 difference from random loss: 0.126844 \n",
      "epoch: 4 dis loss: 0.824377 dis accu: 0.453924 difference from random loss: 0.13123 \n",
      "epoch: 5 dis loss: 0.794398 dis accu: 0.48549 difference from random loss: 0.101251 \n",
      "epoch: 6 dis loss: 0.742694 dis accu: 0.553238 difference from random loss: 0.049547 <-- new best difference from random loss\n",
      "epoch: 7 dis loss: 0.705102 dis accu: 0.596831 difference from random loss: 0.011955 <-- new best difference from random loss\n",
      "epoch: 8 dis loss: 0.665695 dis accu: 0.649058 difference from random loss: 0.027452 \n",
      "epoch: 9 dis loss: 0.624764 dis accu: 0.695596 difference from random loss: 0.068383 \n",
      "epoch: 10 dis loss: 0.591075 dis accu: 0.731454 difference from random loss: 0.102072 \n",
      "epoch: 11 dis loss: 0.50936 dis accu: 0.804092 difference from random loss: 0.183787 \n",
      "epoch: 12 dis loss: 0.537321 dis accu: 0.783955 difference from random loss: 0.155826 \n",
      "epoch: 13 dis loss: 0.519477 dis accu: 0.797779 difference from random loss: 0.17367 \n",
      "epoch: 14 dis loss: 0.447794 dis accu: 0.841822 difference from random loss: 0.245353 \n",
      "epoch: 15 dis loss: 0.748748 dis accu: 0.620686 difference from random loss: 0.055601 \n",
      "epoch: 16 dis loss: 0.631015 dis accu: 0.701884 difference from random loss: 0.062132 \n",
      "epoch: 17 dis loss: 0.626403 dis accu: 0.710443 difference from random loss: 0.066745 \n",
      "epoch: 18 dis loss: 0.656404 dis accu: 0.679002 difference from random loss: 0.036743 \n",
      "epoch: 19 dis loss: 0.790698 dis accu: 0.516232 difference from random loss: 0.097551 \n",
      "epoch: 20 dis loss: 0.696723 dis accu: 0.595783 difference from random loss: 0.003575 <-- new best difference from random loss\n",
      "epoch: 21 dis loss: 0.748327 dis accu: 0.512339 difference from random loss: 0.05518 \n",
      "epoch: 22 dis loss: 0.74944 dis accu: 0.48262 difference from random loss: 0.056293 \n",
      "epoch: 23 dis loss: 0.761381 dis accu: 0.438253 difference from random loss: 0.068234 \n",
      "epoch: 24 dis loss: 0.732261 dis accu: 0.456494 difference from random loss: 0.039114 \n",
      "epoch: 25 dis loss: 0.724029 dis accu: 0.46927 difference from random loss: 0.030881 \n",
      "epoch: 26 dis loss: 0.731942 dis accu: 0.438802 difference from random loss: 0.038794 \n",
      "epoch: 27 dis loss: 0.714475 dis accu: 0.465777 difference from random loss: 0.021328 \n",
      "epoch: 28 dis loss: 0.710192 dis accu: 0.469669 difference from random loss: 0.017044 \n",
      "epoch: 29 dis loss: 0.709584 dis accu: 0.458391 difference from random loss: 0.016437 \n",
      "epoch: 30 dis loss: 0.702234 dis accu: 0.481273 difference from random loss: 0.009087 \n",
      "epoch: 31 dis loss: 0.696189 dis accu: 0.508771 difference from random loss: 0.003041 <-- new best difference from random loss\n",
      "epoch: 32 dis loss: 0.692867 dis accu: 0.52549 difference from random loss: 0.00028 <-- new best difference from random loss\n",
      "epoch: 33 dis loss: 0.690148 dis accu: 0.53864 difference from random loss: 0.003 \n",
      "epoch: 34 dis loss: 0.691344 dis accu: 0.533026 difference from random loss: 0.001803 \n",
      "epoch: 35 dis loss: 0.694491 dis accu: 0.527137 difference from random loss: 0.001344 \n",
      "epoch: 36 dis loss: 0.697592 dis accu: 0.517854 difference from random loss: 0.004445 \n",
      "epoch: 37 dis loss: 0.698118 dis accu: 0.51708 difference from random loss: 0.004971 \n",
      "epoch: 38 dis loss: 0.698489 dis accu: 0.521722 difference from random loss: 0.005342 \n",
      "epoch: 39 dis loss: 0.700097 dis accu: 0.51451 difference from random loss: 0.00695 \n",
      "epoch: 40 dis loss: 0.700392 dis accu: 0.509919 difference from random loss: 0.007245 \n",
      "epoch: 41 dis loss: 0.697962 dis accu: 0.51995 difference from random loss: 0.004815 \n",
      "epoch: 42 dis loss: 0.702325 dis accu: 0.500037 difference from random loss: 0.009178 \n",
      "epoch: 43 dis loss: 0.698605 dis accu: 0.512689 difference from random loss: 0.005458 \n",
      "epoch: 44 dis loss: 0.700579 dis accu: 0.503955 difference from random loss: 0.007431 \n",
      "epoch: 45 dis loss: 0.699512 dis accu: 0.507224 difference from random loss: 0.006365 \n",
      "epoch: 46 dis loss: 0.701148 dis accu: 0.499414 difference from random loss: 0.008001 \n",
      "epoch: 47 dis loss: 0.69739 dis accu: 0.51758 difference from random loss: 0.004243 \n",
      "epoch: 48 dis loss: 0.70113 dis accu: 0.497492 difference from random loss: 0.007983 \n",
      "epoch: 49 dis loss: 0.699509 dis accu: 0.503556 difference from random loss: 0.006362 \n",
      "Adversarial training for ST slide 151669: \n",
      "Start adversarial training...\n",
      "Discriminator target loss: 0.6931471805599453\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33c96fe32d4421a8ec9912a415199d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d921aa71f49e45fab9ec5e8291efe3c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Discriminator):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d4ecfe4cd448f0befe2ebb91390416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch (Encoder):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 dis loss: 0.853544 dis accu: 0.351077 difference from random loss: 0.160397 <-- new best difference from random loss\n",
      "epoch: 1 dis loss: 0.788876 dis accu: 0.494896 difference from random loss: 0.095729 <-- new best difference from random loss\n",
      "epoch: 2 dis loss: 0.812301 dis accu: 0.46926 difference from random loss: 0.119154 \n",
      "epoch: 3 dis loss: 0.825856 dis accu: 0.456076 difference from random loss: 0.132709 \n",
      "epoch: 4 dis loss: 0.828765 dis accu: 0.455006 difference from random loss: 0.135618 \n",
      "epoch: 5 dis loss: 0.794152 dis accu: 0.48743 difference from random loss: 0.101005 \n",
      "epoch: 6 dis loss: 0.746999 dis accu: 0.538076 difference from random loss: 0.053852 <-- new best difference from random loss\n",
      "epoch: 7 dis loss: 0.700995 dis accu: 0.596893 difference from random loss: 0.007848 <-- new best difference from random loss\n",
      "epoch: 8 dis loss: 0.656402 dis accu: 0.648871 difference from random loss: 0.036745 \n",
      "epoch: 9 dis loss: 0.63557 dis accu: 0.673463 difference from random loss: 0.057577 \n",
      "epoch: 10 dis loss: 0.604014 dis accu: 0.708237 difference from random loss: 0.089134 \n",
      "epoch: 11 dis loss: 0.520299 dis accu: 0.786921 difference from random loss: 0.172848 \n",
      "epoch: 12 dis loss: 0.726332 dis accu: 0.591098 difference from random loss: 0.033185 \n",
      "epoch: 13 dis loss: 0.716069 dis accu: 0.589035 difference from random loss: 0.022921 \n",
      "epoch: 14 dis loss: 0.585051 dis accu: 0.715755 difference from random loss: 0.108096 \n",
      "epoch: 15 dis loss: 0.567342 dis accu: 0.743428 difference from random loss: 0.125806 \n",
      "epoch: 16 dis loss: 0.602458 dis accu: 0.708289 difference from random loss: 0.090689 \n",
      "epoch: 17 dis loss: 0.644595 dis accu: 0.664665 difference from random loss: 0.048552 \n",
      "epoch: 18 dis loss: 0.772698 dis accu: 0.516042 difference from random loss: 0.079551 \n",
      "epoch: 19 dis loss: 0.667671 dis accu: 0.611696 difference from random loss: 0.025476 \n",
      "epoch: 20 dis loss: 0.68355 dis accu: 0.576687 difference from random loss: 0.009597 \n",
      "epoch: 21 dis loss: 0.69925 dis accu: 0.544446 difference from random loss: 0.006103 <-- new best difference from random loss\n",
      "epoch: 22 dis loss: 0.789072 dis accu: 0.419031 difference from random loss: 0.095925 \n",
      "epoch: 23 dis loss: 0.73214 dis accu: 0.462002 difference from random loss: 0.038993 \n",
      "epoch: 24 dis loss: 0.738397 dis accu: 0.440204 difference from random loss: 0.04525 \n",
      "epoch: 25 dis loss: 0.714682 dis accu: 0.475708 difference from random loss: 0.021535 \n",
      "epoch: 26 dis loss: 0.720105 dis accu: 0.464352 difference from random loss: 0.026958 \n",
      "epoch: 27 dis loss: 0.707726 dis accu: 0.498838 difference from random loss: 0.014579 \n",
      "epoch: 28 dis loss: 0.711102 dis accu: 0.48696 difference from random loss: 0.017954 \n",
      "epoch: 29 dis loss: 0.703355 dis accu: 0.505913 difference from random loss: 0.010208 \n",
      "epoch: 30 dis loss: 0.701008 dis accu: 0.517661 difference from random loss: 0.007861 \n",
      "epoch: 31 dis loss: 0.69757 dis accu: 0.522725 difference from random loss: 0.004423 <-- new best difference from random loss\n",
      "epoch: 32 dis loss: 0.696274 dis accu: 0.524892 difference from random loss: 0.003127 <-- new best difference from random loss\n",
      "epoch: 33 dis loss: 0.694794 dis accu: 0.525937 difference from random loss: 0.001647 <-- new best difference from random loss\n",
      "epoch: 34 dis loss: 0.695972 dis accu: 0.520506 difference from random loss: 0.002825 \n",
      "epoch: 35 dis loss: 0.694764 dis accu: 0.518314 difference from random loss: 0.001617 <-- new best difference from random loss\n",
      "epoch: 36 dis loss: 0.697091 dis accu: 0.502545 difference from random loss: 0.003944 \n",
      "epoch: 37 dis loss: 0.697004 dis accu: 0.505835 difference from random loss: 0.003857 \n",
      "epoch: 38 dis loss: 0.695753 dis accu: 0.509803 difference from random loss: 0.002606 \n",
      "epoch: 39 dis loss: 0.694999 dis accu: 0.511787 difference from random loss: 0.001852 \n",
      "epoch: 40 dis loss: 0.694856 dis accu: 0.517008 difference from random loss: 0.001709 \n",
      "epoch: 41 dis loss: 0.694325 dis accu: 0.519932 difference from random loss: 0.001178 <-- new best difference from random loss\n",
      "epoch: 42 dis loss: 0.695168 dis accu: 0.518653 difference from random loss: 0.002021 \n",
      "epoch: 43 dis loss: 0.696796 dis accu: 0.516721 difference from random loss: 0.003649 \n",
      "epoch: 44 dis loss: 0.695131 dis accu: 0.5239 difference from random loss: 0.001984 \n",
      "epoch: 45 dis loss: 0.698899 dis accu: 0.513197 difference from random loss: 0.005752 \n",
      "epoch: 46 dis loss: 0.695929 dis accu: 0.525571 difference from random loss: 0.002782 \n",
      "epoch: 47 dis loss: 0.695677 dis accu: 0.524918 difference from random loss: 0.00253 \n",
      "epoch: 48 dis loss: 0.695426 dis accu: 0.527581 difference from random loss: 0.002279 \n",
      "epoch: 49 dis loss: 0.69699 dis accu: 0.523639 difference from random loss: 0.003842 \n"
     ]
    }
   ],
   "source": [
    "if TRAIN_USING_ALL_ST_SAMPLES:\n",
    "    print(f\"Adversarial training for all ST slides\")\n",
    "    save_folder = advtrain_folder\n",
    "\n",
    "    best_checkpoint = torch.load(os.path.join(pretrain_folder, f\"final_model.pth\"))\n",
    "    model = best_checkpoint[\"model\"]\n",
    "    model.to(device)\n",
    "    model.advtraining()\n",
    "\n",
    "    train_adversarial(\n",
    "        model,\n",
    "        save_folder,\n",
    "        dataloader_source_train,\n",
    "        dataloader_source_val,\n",
    "        dataloader_target_train,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    for sample_id in st_sample_id_l:\n",
    "        print(f\"Adversarial training for ST slide {sample_id}: \")\n",
    "\n",
    "        save_folder = os.path.join(advtrain_folder, sample_id)\n",
    "        if not os.path.isdir(save_folder):\n",
    "            os.makedirs(save_folder)\n",
    "\n",
    "        best_checkpoint = torch.load(os.path.join(pretrain_folder, f\"final_model.pth\"))\n",
    "        model = best_checkpoint[\"model\"]\n",
    "        model.to(device)\n",
    "        model.advtraining()\n",
    "\n",
    "        train_adversarial(\n",
    "            model,\n",
    "            save_folder,\n",
    "            dataloader_source_train,\n",
    "            dataloader_source_val,\n",
    "            dataloader_target_train_d[sample_id],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn import model_selection\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# for sample_id in st_sample_id_l:\n",
    "#     best_checkpoint = torch.load(\n",
    "#         os.path.join(advtrain_folder, sample_id, f\"final_model.pth\")\n",
    "#     )\n",
    "#     model = best_checkpoint[\"model\"]\n",
    "#     model.to(device)\n",
    "\n",
    "#     model.eval()\n",
    "#     model.target_inference()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         source_emb = model.source_encoder(torch.Tensor(sc_mix_train_s).to(device))\n",
    "#         target_emb = model.target_encoder(\n",
    "#             torch.Tensor(mat_sp_test_s_d[sample_id]).to(device)\n",
    "#         )\n",
    "\n",
    "#         y_dis = torch.cat(\n",
    "#             [\n",
    "#                 torch.zeros(source_emb.shape[0], device=device, dtype=torch.long),\n",
    "#                 torch.ones(target_emb.shape[0], device=device, dtype=torch.long),\n",
    "#             ]\n",
    "#         )\n",
    "\n",
    "#         emb = torch.cat([source_emb, target_emb])\n",
    "\n",
    "#         emb = emb.detach().cpu().numpy()\n",
    "#         y_dis = y_dis.detach().cpu().numpy()\n",
    "\n",
    "#     (emb_train, emb_test, y_dis_train, y_dis_test,) = model_selection.train_test_split(\n",
    "#         emb,\n",
    "#         y_dis,\n",
    "#         test_size=0.2,\n",
    "#         random_state=225,\n",
    "#         stratify=y_dis,\n",
    "#     )\n",
    "\n",
    "#     pca = PCA(n_components=50)\n",
    "#     pca.fit(emb_train)\n",
    "\n",
    "#     emb_train_50 = pca.transform(emb_train)\n",
    "#     emb_test_50 = pca.transform(emb_test)\n",
    "\n",
    "#     clf = RandomForestClassifier(random_state=145, n_jobs=-1)\n",
    "#     clf.fit(emb_train_50, y_dis_train)\n",
    "#     accu_train = clf.score(emb_train_50, y_dis_train)\n",
    "#     accu_test = clf.score(emb_test_50, y_dis_test)\n",
    "#     class_proportions = np.mean(y_dis)\n",
    "\n",
    "#     print(\n",
    "#         \"Training accuracy: {}, Test accuracy: {}, Class proportions: {}\".format(\n",
    "#             accu_train, accu_test, class_proportions\n",
    "#         )\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 4. Predict cell fraction of spots and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_sp_d, pred_sp_noda_d = {}, {}\n",
    "# if TRAIN_USING_ALL_ST_SAMPLES:\n",
    "#     best_checkpoint = torch.load(os.path.join(advtrain_folder, f\"final_model.pth\"))\n",
    "#     model = best_checkpoint[\"model\"]\n",
    "#     model.to(device)\n",
    "\n",
    "#     model.eval()\n",
    "#     model.target_inference()\n",
    "#     with torch.no_grad():\n",
    "#         for sample_id in st_sample_id_l:\n",
    "#             pred_sp_d[sample_id] = (\n",
    "#                 torch.exp(\n",
    "#                     model(torch.Tensor(mat_sp_test_s_d[sample_id]).to(device))\n",
    "#                 )\n",
    "#                 .detach()\n",
    "#                 .cpu()\n",
    "#                 .numpy()\n",
    "#             )\n",
    "\n",
    "# else:\n",
    "#     for sample_id in st_sample_id_l:\n",
    "#         best_checkpoint = torch.load(\n",
    "#             os.path.join(advtrain_folder, sample_id, f\"final_model.pth\")\n",
    "#         )\n",
    "#         model = best_checkpoint[\"model\"]\n",
    "#         model.to(device)\n",
    "\n",
    "#         model.eval()\n",
    "#         model.target_inference()\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             pred_sp_d[sample_id] = (\n",
    "#                 torch.exp(\n",
    "#                     model(torch.Tensor(mat_sp_test_s_d[sample_id]).to(device))\n",
    "#                 )\n",
    "#                 .detach()\n",
    "#                 .cpu()\n",
    "#                 .numpy()\n",
    "#             )\n",
    "\n",
    "\n",
    "# best_checkpoint = torch.load(os.path.join(pretrain_folder, f\"best_model.pth\"))\n",
    "# model = best_checkpoint[\"model\"]\n",
    "# model.to(device)\n",
    "\n",
    "# model.eval()\n",
    "# model.set_encoder(\"source\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for sample_id in st_sample_id_l:\n",
    "#         pred_sp_noda_d[sample_id] = (\n",
    "#             torch.exp(model(torch.Tensor(mat_sp_test_s_d[sample_id]).to(device)))\n",
    "#             .detach()\n",
    "#             .cpu()\n",
    "#             .numpy()\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata_spatialLIBD = sc.read_h5ad(\n",
    "#     os.path.join(PROCESSED_DATA_DIR, \"adata_spatialLIBD.h5ad\")\n",
    "# )\n",
    "\n",
    "# adata_spatialLIBD_d = {}\n",
    "# for sample_id in st_sample_id_l:\n",
    "#     adata_spatialLIBD_d[sample_id] = adata_spatialLIBD[\n",
    "#         adata_spatialLIBD.obs.sample_id == sample_id\n",
    "#     ]\n",
    "#     adata_spatialLIBD_d[sample_id].obsm[\"spatial\"] = (\n",
    "#         adata_spatialLIBD_d[sample_id].obs[[\"X\", \"Y\"]].values\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_name_exN_l = []\n",
    "# for k, v in sc_sub_dict.items():\n",
    "#     if \"Ex\" in v:\n",
    "#         num_name_exN_l.append((k, v, int(v.split(\"_\")[1])))\n",
    "# num_name_exN_l.sort(key=lambda a: a[2])\n",
    "# num_name_exN_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex_to_L_d = {\n",
    "#     1: {5, 6},\n",
    "#     2: {5},\n",
    "#     3: {4, 5},\n",
    "#     4: {6},\n",
    "#     5: {5},\n",
    "#     6: {4, 5, 6},\n",
    "#     7: {4, 5, 6},\n",
    "#     8: {5, 6},\n",
    "#     9: {5, 6},\n",
    "#     10: {2, 3, 4},\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numlist = [t[0] for t in num_name_exN_l]\n",
    "# Ex_l = [t[2] for t in num_name_exN_l]\n",
    "# num_to_ex_d = dict(zip(numlist, Ex_l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_cellfraction(visnum, adata, pred_sp, ax=None):\n",
    "#     \"\"\"Plot predicted cell fraction for a given visnum\"\"\"\n",
    "#     adata.obs[\"Pred_label\"] = pred_sp[:, visnum]\n",
    "#     # vmin = 0\n",
    "#     # vmax = np.amax(pred_sp)\n",
    "\n",
    "#     sc.pl.spatial(\n",
    "#         adata,\n",
    "#         img_key=\"hires\",\n",
    "#         color=\"Pred_label\",\n",
    "#         palette=\"Set1\",\n",
    "#         size=1.5,\n",
    "#         legend_loc=None,\n",
    "#         title=f\"{sc_sub_dict[visnum]}\",\n",
    "#         spot_size=100,\n",
    "#         show=False,\n",
    "#         # vmin=vmin,\n",
    "#         # vmax=vmax,\n",
    "#         ax=ax,\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_roc(visnum, adata, pred_sp, name, ax=None):\n",
    "#     \"\"\"Plot ROC for a given visnum\"\"\"\n",
    "\n",
    "#     def layer_to_layer_number(x):\n",
    "#         for char in x:\n",
    "#             if char.isdigit():\n",
    "#                 if int(char) in Ex_to_L_d[num_to_ex_d[visnum]]:\n",
    "#                     return 1\n",
    "#         return 0\n",
    "\n",
    "#     y_pred = pred_sp[:, visnum]\n",
    "#     y_true = adata.obs[\"spatialLIBD\"].map(layer_to_layer_number).fillna(0)\n",
    "#     # print(y_true)\n",
    "#     # print(y_true.isna().sum())\n",
    "#     RocCurveDisplay.from_predictions(y_true=y_true, y_pred=y_pred, name=name, ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 5), constrained_layout=True)\n",
    "\n",
    "# sc.pl.spatial(\n",
    "#     adata_spatialLIBD_d[SAMPLE_ID_N],\n",
    "#     img_key=None,\n",
    "#     color=\"spatialLIBD\",\n",
    "#     palette=\"Accent_r\",\n",
    "#     size=1.5,\n",
    "#     title=SAMPLE_ID_N,\n",
    "#     # legend_loc = 4,\n",
    "#     spot_size=100,\n",
    "#     show=False,\n",
    "#     ax=ax,\n",
    "# )\n",
    "\n",
    "# ax.axis(\"equal\")\n",
    "# ax.set_xlabel(\"\")\n",
    "# ax.set_ylabel(\"\")\n",
    "\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(2, 5, figsize=(20, 8), constrained_layout=True)\n",
    "\n",
    "# for i, num in enumerate(numlist):\n",
    "#     plot_cellfraction(\n",
    "#         num, adata_spatialLIBD_d[SAMPLE_ID_N], pred_sp_d[SAMPLE_ID_N], ax.flat[i]\n",
    "#     )\n",
    "#     ax.flat[i].axis(\"equal\")\n",
    "#     ax.flat[i].set_xlabel(\"\")\n",
    "#     ax.flat[i].set_ylabel(\"\")\n",
    "\n",
    "# fig.show()\n",
    "\n",
    "# fig, ax = plt.subplots(\n",
    "#     2, 5, figsize=(20, 8), constrained_layout=True, sharex=True, sharey=True\n",
    "# )\n",
    "\n",
    "# for i, num in enumerate(numlist):\n",
    "#     plot_roc(\n",
    "#         num,\n",
    "#         adata_spatialLIBD_d[SAMPLE_ID_N],\n",
    "#         pred_sp_d[SAMPLE_ID_N],\n",
    "#         \"ADDA\",\n",
    "#         ax.flat[i],\n",
    "#     )\n",
    "#     plot_roc(\n",
    "#         num,\n",
    "#         adata_spatialLIBD_d[SAMPLE_ID_N],\n",
    "#         pred_sp_noda_d[SAMPLE_ID_N],\n",
    "#         \"NN_wo_da\",\n",
    "#         ax.flat[i],\n",
    "#     )\n",
    "#     ax.flat[i].plot([0, 1], [0, 1], transform=ax.flat[i].transAxes, ls=\"--\", color=\"k\")\n",
    "#     ax.flat[i].set_aspect(\"equal\")\n",
    "#     ax.flat[i].set_xlim([0, 1])\n",
    "#     ax.flat[i].set_ylim([0, 1])\n",
    "\n",
    "#     ax.flat[i].set_title(f\"{sc_sub_dict[num]}\")\n",
    "\n",
    "#     if i >= len(numlist) - 5:\n",
    "#         ax.flat[i].set_xlabel(\"FPR\")\n",
    "#     else:\n",
    "#         ax.flat[i].set_xlabel(\"\")\n",
    "#     if i % 5 == 0:\n",
    "#         ax.flat[i].set_ylabel(\"TPR\")\n",
    "#     else:\n",
    "#         ax.flat[i].set_ylabel(\"\")\n",
    "\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if TRAIN_USING_ALL_ST_SAMPLES:\n",
    "#     best_checkpoint = torch.load(os.path.join(advtrain_folder, f\"final_model.pth\"))\n",
    "# else:\n",
    "#     best_checkpoint = torch.load(\n",
    "#         os.path.join(advtrain_folder, SAMPLE_ID_N, f\"final_model.pth\")\n",
    "#     )\n",
    "\n",
    "# model = best_checkpoint[\"model\"]\n",
    "# model.to(device)\n",
    "\n",
    "# model.eval()\n",
    "# model.set_encoder(\"source\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     pred_mix = (\n",
    "#         torch.exp(model(torch.Tensor(sc_mix_test_s).to(device)))\n",
    "#         .detach()\n",
    "#         .cpu()\n",
    "#         .numpy()\n",
    "#     )\n",
    "\n",
    "# cell_type_nums = sc_sub_dict.keys()\n",
    "# nrows = ceil(len(cell_type_nums) / 5)\n",
    "\n",
    "# line_kws = {\"color\": \"tab:orange\"}\n",
    "# scatter_kws = {\"s\": 5}\n",
    "\n",
    "# props = dict(facecolor=\"w\", alpha=0.5)\n",
    "\n",
    "# fig, ax = plt.subplots(\n",
    "#     nrows,\n",
    "#     5,\n",
    "#     figsize=(25, 5 * nrows),\n",
    "#     constrained_layout=True,\n",
    "#     sharex=False,\n",
    "#     sharey=True,\n",
    "# )\n",
    "# for i, visnum in enumerate(cell_type_nums):\n",
    "#     sns.regplot(\n",
    "#         x=pred_mix[:, visnum],\n",
    "#         y=lab_mix_test[:, visnum],\n",
    "#         line_kws=line_kws,\n",
    "#         scatter_kws=scatter_kws,\n",
    "#         ax=ax.flat[i],\n",
    "#     ).set_title(sc_sub_dict[visnum])\n",
    "\n",
    "#     ax.flat[i].set_aspect(\"equal\")\n",
    "#     ax.flat[i].set_xlabel(\"Predicted Proportion\")\n",
    "\n",
    "#     if i % 5 == 0:\n",
    "#         ax.flat[i].set_ylabel(\"True Proportion\")\n",
    "#     else:\n",
    "#         ax.flat[i].set_ylabel(\"\")\n",
    "#     ax.flat[i].set_xlim([0, 1])\n",
    "#     ax.flat[i].set_ylim([0, 1])\n",
    "\n",
    "#     textstr = (\n",
    "#         f\"MSE: {mean_squared_error(pred_mix[:,visnum], lab_mix_test[:,visnum]):.5f}\"\n",
    "#     )\n",
    "\n",
    "#     # place a text box in upper left in axes coords\n",
    "#     ax.flat[i].text(\n",
    "#         0.95,\n",
    "#         0.05,\n",
    "#         textstr,\n",
    "#         transform=ax.flat[i].transAxes,\n",
    "#         verticalalignment=\"bottom\",\n",
    "#         horizontalalignment=\"right\",\n",
    "#         bbox=props,\n",
    "#     )\n",
    "\n",
    "# for i in range(len(cell_type_nums), nrows * 5):\n",
    "#     ax.flat[i].axis(\"off\")\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('agreda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c8a91b640e5c43bacdcbf87782ad770b561ed71a46153862bbc20bda0bebf44f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
