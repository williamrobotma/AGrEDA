{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # DANN for ST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Creating something like CellDART but it actually follows Adda in PyTorch as a first step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T01:29:35.617262Z",
     "iopub.status.busy": "2022-12-14T01:29:35.616778Z",
     "iopub.status.idle": "2022-12-14T01:29:36.647879Z",
     "shell.execute_reply": "2022-12-14T01:29:36.646912Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "from itertools import count\n",
    "import warnings\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch import autocast\n",
    "\n",
    "from src.da_models.autoencoder import AutoEncoder\n",
    "from src.da_models.datasets import SpotDataset\n",
    "from src.da_models.utils import set_requires_grad\n",
    "from src.da_models.utils import initialize_weights\n",
    "from src.utils.data_loading import load_sc, get_selected_dir\n",
    "from src.utils.dupstdout import DupStdout\n",
    "from src.utils.evaluation import format_iters\n",
    "\n",
    "\n",
    "# datetime object containing current date and time\n",
    "script_start_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%Hh%Mm%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T01:29:36.652958Z",
     "iopub.status.busy": "2022-12-14T01:29:36.652813Z",
     "iopub.status.idle": "2022-12-14T01:29:36.667393Z",
     "shell.execute_reply": "2022-12-14T01:29:36.666759Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cpu\":\n",
    "    warnings.warn(\"Using CPU\", stacklevel=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T01:29:36.672547Z",
     "iopub.status.busy": "2022-12-14T01:29:36.672427Z",
     "iopub.status.idle": "2022-12-14T01:29:36.676887Z",
     "shell.execute_reply": "2022-12-14T01:29:36.676216Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DATA_DIR = \"./data\"\n",
    "\n",
    "TRAIN_USING_ALL_ST_SAMPLES = False\n",
    "N_MARKERS = 20\n",
    "ALL_GENES = False\n",
    "N_SPOTS = 100000\n",
    "N_MIX = 8\n",
    "SCALER_NAME = \"standard\"\n",
    "\n",
    "SAMPLE_ID_N = \"151673\"\n",
    "\n",
    "MODEL_NAME = \"Autoencoder\"\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "INITIAL_TRAIN_LR = 0.001\n",
    "NUM_WORKERS = 4\n",
    "INITIAL_TRAIN_EPOCHS = 500\n",
    "\n",
    "\n",
    "MIN_EPOCHS = 0.4 * INITIAL_TRAIN_EPOCHS\n",
    "EARLY_STOP_CRIT = INITIAL_TRAIN_EPOCHS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "autoencoder_kwargs = {\n",
    "    \"emb_dim\": 64,\n",
    "    \"dropout\": 0.05,\n",
    "    \"enc_out_act\": None,\n",
    "    \"dec_out_act\": None,\n",
    "    \"bn_momentum\": 0.1,\n",
    "}\n",
    "adamw_kwargs = {\"lr\": INITIAL_TRAIN_LR, \"weight_decay\": 0.2}\n",
    "\n",
    "# Validation using MSE Loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "# def log_target_pnll_loss(input, target, **kwargs):\n",
    "#     return F.poisson_nll_loss(input, torch.exp(target), log_input=True, full=True, **kwargs)\n",
    "# criterion = log_target_pnll_loss\n",
    "# criterion = torch.nn.PoissonNLLLoss(log_input=True, full=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T01:29:36.679471Z",
     "iopub.status.busy": "2022-12-14T01:29:36.679359Z",
     "iopub.status.idle": "2022-12-14T01:29:36.682973Z",
     "shell.execute_reply": "2022-12-14T01:29:36.682342Z"
    }
   },
   "outputs": [],
   "source": [
    "model_folder = os.path.join(\"model\", MODEL_NAME, script_start_time)\n",
    "\n",
    "model_folder = os.path.join(\"model\", MODEL_NAME, \"V1\")\n",
    "\n",
    "if not os.path.isdir(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "    print(model_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T01:29:36.684812Z",
     "iopub.status.busy": "2022-12-14T01:29:36.684695Z",
     "iopub.status.idle": "2022-12-14T01:29:36.728758Z",
     "shell.execute_reply": "2022-12-14T01:29:36.727998Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load sc data\n",
    "sc_mix_d, lab_mix_d, sc_sub_dict, sc_sub_dict2 = load_sc(\n",
    "    get_selected_dir(DATA_DIR, N_MARKERS, ALL_GENES),\n",
    "    SCALER_NAME,\n",
    "    n_mix=N_MIX,\n",
    "    n_spots=N_SPOTS,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Training: Adversarial domain adaptation for cell fraction estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T01:29:36.732207Z",
     "iopub.status.busy": "2022-12-14T01:29:36.732067Z",
     "iopub.status.idle": "2022-12-14T01:29:36.757716Z",
     "shell.execute_reply": "2022-12-14T01:29:36.756892Z"
    }
   },
   "outputs": [],
   "source": [
    "### source dataloaders\n",
    "source_train_set = SpotDataset(sc_mix_d[\"train\"], lab_mix_d[\"train\"])\n",
    "source_val_set = SpotDataset(sc_mix_d[\"val\"], lab_mix_d[\"val\"])\n",
    "source_test_set = SpotDataset(sc_mix_d[\"test\"], lab_mix_d[\"test\"])\n",
    "\n",
    "dataloader_source_train = torch.utils.data.DataLoader(\n",
    "    source_train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False,\n",
    ")\n",
    "dataloader_source_val = torch.utils.data.DataLoader(\n",
    "    source_val_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False,\n",
    ")\n",
    "dataloader_source_test = torch.utils.data.DataLoader(\n",
    "    source_test_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "### target dataloaders\n",
    "# target_test_set_d = {}\n",
    "# for sample_id in st_sample_id_l:\n",
    "#     target_test_set_d[sample_id] = SpotDataset(mat_sp_test_s_d[sample_id])\n",
    "\n",
    "# dataloader_target_test_d = {}\n",
    "# for sample_id in st_sample_id_l:\n",
    "#     dataloader_target_test_d[sample_id] = torch.utils.data.DataLoader(\n",
    "#         target_test_set_d[sample_id],\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         shuffle=False,\n",
    "#         num_workers=NUM_WORKERS,\n",
    "#         pin_memory=False,\n",
    "#     )\n",
    "\n",
    "# if TRAIN_USING_ALL_ST_SAMPLES:\n",
    "#     target_train_set = SpotDataset(mat_sp_train_s)\n",
    "#     dataloader_target_train = torch.utils.data.DataLoader(\n",
    "#         target_train_set,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         shuffle=True,\n",
    "#         num_workers=NUM_WORKERS,\n",
    "#         pin_memory=False,\n",
    "#     )\n",
    "#     target_train_set_dis = SpotDataset(deepcopy(mat_sp_train_s))\n",
    "#     dataloader_target_train_dis = torch.utils.data.DataLoader(\n",
    "#         target_train_set_dis,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         shuffle=True,\n",
    "#         num_workers=NUM_WORKERS,\n",
    "#         pin_memory=False,\n",
    "#     )\n",
    "# else:\n",
    "#     target_train_set_d = {}\n",
    "#     dataloader_target_train_d = {}\n",
    "\n",
    "#     target_train_set_dis_d = {}\n",
    "#     dataloader_target_train_dis_d = {}\n",
    "#     for sample_id in st_sample_id_l:\n",
    "#         target_train_set_d[sample_id] = SpotDataset(\n",
    "#             deepcopy(mat_sp_test_s_d[sample_id])\n",
    "#         )\n",
    "#         dataloader_target_train_d[sample_id] = torch.utils.data.DataLoader(\n",
    "#             target_train_set_d[sample_id],\n",
    "#             batch_size=BATCH_SIZE,\n",
    "#             shuffle=True,\n",
    "#             num_workers=NUM_WORKERS,\n",
    "#             pin_memory=False,\n",
    "#         )\n",
    "\n",
    "#         target_train_set_dis_d[sample_id] = SpotDataset(\n",
    "#             deepcopy(mat_sp_test_s_d[sample_id])\n",
    "#         )\n",
    "#         dataloader_target_train_dis_d[sample_id] = torch.utils.data.DataLoader(\n",
    "#             target_train_set_dis_d[sample_id],\n",
    "#             batch_size=BATCH_SIZE,\n",
    "#             shuffle=True,\n",
    "#             num_workers=NUM_WORKERS,\n",
    "#             pin_memory=False,\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T01:29:36.761501Z",
     "iopub.status.busy": "2022-12-14T01:29:36.761362Z",
     "iopub.status.idle": "2022-12-14T01:29:36.768667Z",
     "shell.execute_reply": "2022-12-14T01:29:36.768020Z"
    }
   },
   "outputs": [],
   "source": [
    "# import itertools\n",
    "\n",
    "\n",
    "# def plot_iters(nested_list, startpoint=False, endpoint=True, ax=None, **kwargs):\n",
    "#     if ax is None:\n",
    "#         ax = plt\n",
    "\n",
    "#     x = []\n",
    "#     if startpoint:\n",
    "#         for i, l in enumerate(nested_list):\n",
    "#             if endpoint and i == len(nested_list) - 1:\n",
    "#                 x_i = np.linspace(i - 1, i, len(l), endpoint=True, dtype=np.float32)\n",
    "#             else:\n",
    "#                 x_i = np.linspace(i - 1, i, len(l), endpoint=False, dtype=np.float32)\n",
    "#             x.append(x_i)\n",
    "#     else:\n",
    "#         for i, l in enumerate(nested_list):\n",
    "#             if not endpoint and i == len(nested_list) - 1:\n",
    "#                 x_i = np.linspace(\n",
    "#                     i, i - 1, len(l + 1), endpoint=False, dtype=np.float32\n",
    "#                 )\n",
    "#                 x_i = x_i[1:]\n",
    "#             else:\n",
    "#                 x_i = np.linspace(i, i - 1, len(l), endpoint=False, dtype=np.float32)\n",
    "\n",
    "#             # Flip to not include startpoint i.e. shift to end of iteration\n",
    "#             x_i = np.flip(x_i)\n",
    "#             x.append(x_i)\n",
    "\n",
    "#     x = np.asarray(list(itertools.chain(*x)))\n",
    "#     y = np.asarray(list(itertools.chain(*nested_list)))\n",
    "#     ax.plot(x, y, **kwargs)\n",
    "\n",
    "#     return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T01:29:36.771257Z",
     "iopub.status.busy": "2022-12-14T01:29:36.771138Z",
     "iopub.status.idle": "2022-12-14T01:29:37.917079Z",
     "shell.execute_reply": "2022-12-14T01:29:37.915995Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Using an Adam Optimizer with lr = 0.1\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), **adamw_kwargs)\n",
    "\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "#     optimizer,\n",
    "#     max_lr=INITIAL_TRAIN_LR,\n",
    "#     steps_per_epoch=len(dataloader_source_train),\n",
    "#     epochs=INITIAL_TRAIN_EPOCHS,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T01:29:37.921961Z",
     "iopub.status.busy": "2022-12-14T01:29:37.921842Z",
     "iopub.status.idle": "2022-12-14T01:29:37.935458Z",
     "shell.execute_reply": "2022-12-14T01:29:37.934314Z"
    }
   },
   "outputs": [],
   "source": [
    "def pred_loss(model, x):\n",
    "    x_pred = model(x)\n",
    "    loss = criterion(x_pred, x)\n",
    "    if not isinstance(criterion, nn.MSELoss):\n",
    "        mse = F.mse_loss(x_pred.detach(), x)\n",
    "    else:\n",
    "        mse = loss\n",
    "    return x_pred, loss, mse\n",
    "\n",
    "\n",
    "def run_epoch(\n",
    "    dataloader,\n",
    "    model,\n",
    "    optimizer=None,\n",
    "    scheduler=None,\n",
    "    scaler=None,\n",
    "    inner=None,\n",
    "    predict=False,\n",
    "):\n",
    "    \"\"\"Runs a single epoch of training or validation.\"\"\"\n",
    "    running_results = {}\n",
    "    running_results[\"loss\"] = []\n",
    "    running_results[\"mse\"] = []\n",
    "    running_results[\"bs\"] = []\n",
    "    if scheduler is not None:\n",
    "        running_results[\"lr\"] = []\n",
    "    if predict:\n",
    "        running_results[\"pred\"] = []\n",
    "        running_results[\"true\"] = []\n",
    "    for _, (x, _) in enumerate(dataloader):\n",
    "        if scheduler is not None:\n",
    "            running_results[\"lr\"].append(scheduler.get_last_lr()[-1])\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        x = x.to(torch.float32).to(device)\n",
    "\n",
    "        if scaler is not None:\n",
    "            with autocast(device_type=device.type, dtype=torch.float16):\n",
    "                x_pred, loss, mse = pred_loss(model, x)\n",
    "        else:\n",
    "            x_pred, loss, mse = pred_loss(model, x)\n",
    "                \n",
    "\n",
    "        running_results[\"loss\"].append(loss.item())\n",
    "        running_results[\"mse\"].append(mse.item())\n",
    "        # we will weight average by batch size later\n",
    "        running_results[\"bs\"].append(len(x))\n",
    "        if predict:\n",
    "            running_results[\"pred\"].append(x_pred.detach().cpu().numpy())\n",
    "            running_results[\"true\"].append(x.detach().cpu().numpy())\n",
    "\n",
    "        if scaler is not None:\n",
    "            if optimizer is not None:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "\n",
    "            scale = scaler.get_scale()\n",
    "            scaler.update()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                skip_lr_sched = scale > scaler.get_scale()\n",
    "                if not skip_lr_sched:\n",
    "                    scheduler.step()\n",
    "        else:\n",
    "            if optimizer is not None:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "                \n",
    "        if inner is not None:\n",
    "            inner.update(1)\n",
    "\n",
    "    # Weight average the loss by batch size\n",
    "    avg_results = {}\n",
    "    for results in running_results:\n",
    "        avg_results[results] = np.average(\n",
    "            running_results[results], weights=running_results[\"bs\"]\n",
    "        )\n",
    "\n",
    "    return avg_results, running_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T01:29:37.939189Z",
     "iopub.status.busy": "2022-12-14T01:29:37.939076Z",
     "iopub.status.idle": "2022-12-14T01:32:30.764975Z",
     "shell.execute_reply": "2022-12-14T01:32:30.763464Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    save_folder,\n",
    "    dataloader_train,\n",
    "    dataloader_val,\n",
    "    epochs=10,\n",
    "    **adamw_kwargs,\n",
    "):\n",
    "    model.to(device)\n",
    "    \n",
    "    # AMP Scaler\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), **adamw_kwargs)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=optimizer.state_dict()[\"param_groups\"][0][\"lr\"],\n",
    "        steps_per_epoch=len(dataloader_train),\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    # Initialize lists to store loss and accuracy values\n",
    "    results_history_running = {}\n",
    "    results_history = {}\n",
    "    results_history_val = {}\n",
    "\n",
    "    # Early Stopping\n",
    "    best_loss_val = np.inf\n",
    "    early_stop_count = 0\n",
    "\n",
    "    # Train\n",
    "    with DupStdout().dup_to_file(os.path.join(save_folder, \"log.txt\"), \"w\") as f_log:\n",
    "        print(\"Start train...\")\n",
    "        outer = tqdm(total=INITIAL_TRAIN_EPOCHS, desc=\"Epochs\", position=0)\n",
    "        inner = tqdm(total=len(dataloader_train), desc=f\"Batch\", position=1)\n",
    "\n",
    "        print(\n",
    "            \" Epoch | Train Loss | Train MSE  | Val Loss   | Val MSE    | Next LR    \"\n",
    "        )\n",
    "        print(\n",
    "            \"------------------------------------------------------------------------\"\n",
    "        )\n",
    "        checkpoint = {\n",
    "            \"epoch\": -1,\n",
    "            \"model\": model,\n",
    "            \"optimizer\": optimizer,\n",
    "            \"scheduler\": scheduler,\n",
    "            \"scaler\": scaler,\n",
    "        }\n",
    "        for epoch in range(INITIAL_TRAIN_EPOCHS):\n",
    "            checkpoint[\"epoch\"] = epoch\n",
    "\n",
    "            # Train mode\n",
    "            model.train()\n",
    "            inner.refresh()  # force print final state\n",
    "            inner.reset()  # reuse bar\n",
    "\n",
    "            avg_results, running_results = run_epoch(\n",
    "                dataloader_train,\n",
    "                model,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                scaler=scaler,\n",
    "                inner=inner,\n",
    "            )\n",
    "\n",
    "            for k in avg_results:\n",
    "                try:\n",
    "                    results_history[k].append(avg_results[k])\n",
    "                except KeyError:\n",
    "                    results_history[k] = [avg_results[k]]\n",
    "            for k in running_results:\n",
    "                try:\n",
    "                    results_history_running[k].append(running_results[k])\n",
    "                except KeyError:\n",
    "                    results_history_running[k] = [running_results[k]]\n",
    "\n",
    "            # Evaluate mode\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                avg_results_val, running_results_val = run_epoch(\n",
    "                    dataloader_val, model\n",
    "                )\n",
    "            for k in avg_results_val:\n",
    "                try:\n",
    "                    results_history_val[k].append(avg_results_val[k])\n",
    "                except KeyError:\n",
    "                    results_history_val[k] = [avg_results_val[k]]\n",
    "\n",
    "            # Print the results\n",
    "            outer.update(1)\n",
    "            print(\n",
    "                f\" {epoch:5d}\",\n",
    "                f\"| {avg_results['loss']:<10.8f}\",\n",
    "                f\"| {avg_results['mse']:<10.8f}\",\n",
    "                f\"| {avg_results_val['loss']:<10.8f}\",\n",
    "                f\"| {avg_results_val['mse']:<10.8f}\",\n",
    "                f\"| {scheduler.get_last_lr()[-1]:<10.5}\",\n",
    "                end=\" \",\n",
    "            )\n",
    "\n",
    "            # Save the best weights\n",
    "            if avg_results_val[\"loss\"] < best_loss_val:\n",
    "                best_loss_val = avg_results_val[\"loss\"]\n",
    "                torch.save(checkpoint, os.path.join(save_folder, f\"best_model.pth\"))\n",
    "                early_stop_count = 0\n",
    "\n",
    "                print(\"<-- new best val loss\")\n",
    "            else:\n",
    "                print(\"\")\n",
    "\n",
    "            # Save checkpoint every 10\n",
    "            if epoch % 10 == 0 or epoch >= INITIAL_TRAIN_EPOCHS - 1:\n",
    "                torch.save(\n",
    "                    checkpoint, os.path.join(save_folder, f\"checkpt{epoch}.pth\")\n",
    "                )\n",
    "\n",
    "            # check to see if validation loss has plateau'd\n",
    "            if early_stop_count >= EARLY_STOP_CRIT and epoch >= MIN_EPOCHS - 1:\n",
    "                print(\n",
    "                    f\"Validation loss plateaued after {early_stop_count} at epoch {epoch}\"\n",
    "                )\n",
    "                torch.save(\n",
    "                    checkpoint, os.path.join(save_folder, f\"earlystop{epoch}.pth\")\n",
    "                )\n",
    "                break\n",
    "\n",
    "            early_stop_count += 1\n",
    "\n",
    "    results_history_running[\"lr\"][-1].append(scheduler.get_last_lr()[-1])\n",
    "\n",
    "    # Save final model\n",
    "    best_checkpoint = torch.load(os.path.join(save_folder, f\"best_model.pth\"))\n",
    "    torch.save(best_checkpoint, os.path.join(save_folder, f\"final_model.pth\"))\n",
    "\n",
    "    results_history_out = {\n",
    "        \"train\": {\"avg\": results_history, \"running\": results_history_running},\n",
    "        \"val\": {\"avg\": results_history_val},\n",
    "    }\n",
    "    # Save results\n",
    "    with open(os.path.join(save_folder, f\"results_history_out.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(results_history_out, f)\n",
    "\n",
    "    return results_history_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T01:32:30.771838Z",
     "iopub.status.busy": "2022-12-14T01:32:30.771630Z",
     "iopub.status.idle": "2022-12-14T01:32:31.867113Z",
     "shell.execute_reply": "2022-12-14T01:32:31.865865Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_results(train_history_running, val_history, best_epoch, save_folder=None):\n",
    "    best_loss_val = val_history[\"loss\"][best_epoch]\n",
    "    best_mse_val = val_history[\"mse\"][best_epoch]\n",
    "\n",
    "    fig, axs = plt.subplots(3, 1, sharex=True, figsize=(12, 8), layout=\"constrained\")\n",
    "\n",
    "    x_text = best_epoch + (2 if best_epoch < len(val_history[\"loss\"]) * 0.75 else -2)\n",
    "    ha_text = \"left\" if best_epoch < len(val_history[\"loss\"]) * 0.75 else \"right\"\n",
    "\n",
    "    axs[0].plot(\n",
    "        *(x_y_loss_iters := format_iters(train_history_running[\"loss\"])),\n",
    "        label=\"Training\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    axs[0].plot(val_history[\"loss\"], label=\"Validation\")\n",
    "    axs[0].axvline(best_epoch, color=\"tab:green\")\n",
    "\n",
    "    axs[0].set_ylim(bottom=0)\n",
    "    axs[0].grid(which=\"major\")\n",
    "    axs[0].minorticks_on()\n",
    "    axs[0].grid(which=\"minor\", alpha=0.2)\n",
    "\n",
    "    axs[0].text(\n",
    "        x=x_text,\n",
    "        y=max(*x_y_loss_iters[1], *val_history[\"loss\"]) * 0.5,\n",
    "        s=f\"Best val. loss:\\n{best_loss_val:.4f} at epoch {best_epoch}\",\n",
    "        ha=ha_text,\n",
    "        size=\"x-small\",\n",
    "    )\n",
    "\n",
    "    axs[0].set_title(\"Loss\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    # MSE loss\n",
    "    axs[1].plot(\n",
    "        *(x_y_mse_iters := format_iters(train_history_running[\"mse\"])),\n",
    "        label=\"Training\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    axs[1].plot(val_history[\"mse\"], label=\"Validation\")\n",
    "    axs[1].axvline(best_epoch, ymax=2, clip_on=False, color=\"tab:green\")\n",
    "\n",
    "    axs[1].set_ylim(bottom=0)\n",
    "    axs[1].grid(which=\"major\")\n",
    "    axs[1].minorticks_on()\n",
    "    axs[1].grid(which=\"minor\", alpha=0.2)\n",
    "\n",
    "    axs[1].text(\n",
    "        x=x_text,\n",
    "        y=max(*x_y_mse_iters[1], *val_history[\"mse\"]) * 0.5,\n",
    "        s=f\"Val. mse:\\n{best_mse_val:.4f} at epoch {best_epoch}\",\n",
    "        ha=ha_text,\n",
    "        size=\"x-small\",\n",
    "    )\n",
    "\n",
    "    axs[1].set_title(\"MSE\")\n",
    "    axs[1].legend()\n",
    "\n",
    "    # lr history\n",
    "    iters_by_epoch, lr_history_running_flat = format_iters(\n",
    "        train_history_running[\"lr\"], startpoint=True\n",
    "    )\n",
    "    axs[2].plot(iters_by_epoch, lr_history_running_flat)\n",
    "    axs[2].axvline(best_epoch, ymax=2, clip_on=False, color=\"tab:green\")\n",
    "\n",
    "    axs[2].set_ylim(bottom=0)\n",
    "    axs[2].grid(which=\"major\")\n",
    "    axs[2].minorticks_on()\n",
    "    axs[2].grid(which=\"minor\", alpha=0.2)\n",
    "\n",
    "    best_epoch_idx = np.where(iters_by_epoch == best_epoch)[0][0]\n",
    "    axs[2].text(\n",
    "        x=x_text,\n",
    "        y=np.median(lr_history_running_flat),\n",
    "        s=f\"LR:\\n{lr_history_running_flat[best_epoch_idx]:.4} at epoch {best_epoch}\",\n",
    "        ha=ha_text,\n",
    "        size=\"x-small\",\n",
    "    )\n",
    "\n",
    "    axs[2].set_xlabel(\"Epoch\")\n",
    "    axs[2].set_title(\"Learning Rate\")\n",
    "\n",
    "    if save_folder is not None:\n",
    "        if not os.path.exists(save_folder):\n",
    "            os.makedirs(save_folder)\n",
    "        plt.savefig(os.path.join(save_folder, \"train_plots.png\"), bbox_inches=\"tight\")\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder(\n",
      "  (encoder): ADDAMLPEncoder(\n",
      "    (encoder): Sequential(\n",
      "      (0): Linear(in_features=360, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Dropout(p=0.05, inplace=False)\n",
      "      (4): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (5): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): LeakyReLU(negative_slope=0.01)\n",
      "      (7): Dropout(p=0.05, inplace=False)\n",
      "      (8): Linear(in_features=512, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): ADDAMLPDecoder(\n",
      "    (decoder): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Dropout(p=0.05, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (5): BatchNorm1d(1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): LeakyReLU(negative_slope=0.01)\n",
      "      (7): Dropout(p=0.05, inplace=False)\n",
      "      (8): Linear(in_features=1024, out_features=360, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Start train...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99586226058480ebd824bf46853027e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1b063455ad478a91f7301873931361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch | Train Loss | Train MSE  | Val Loss   | Val MSE    | Next LR    \n",
      "------------------------------------------------------------------------\n",
      "     0 | 1.35913212 | 1.35913212 | 0.95163492 | 0.95163492 | 4.0105e-05 <-- new best val loss\n",
      "     1 | 0.94403910 | 0.94403910 | 0.81669880 | 0.81669880 | 4.0421e-05 <-- new best val loss\n",
      "     2 | 0.85760308 | 0.85760308 | 0.77535217 | 0.77535217 | 4.0947e-05 <-- new best val loss\n",
      "     3 | 0.81931361 | 0.81931361 | 0.74987401 | 0.74987401 | 4.1684e-05 <-- new best val loss\n",
      "     4 | 0.79356262 | 0.79356262 | 0.73003895 | 0.73003895 | 4.263e-05  <-- new best val loss\n",
      "     5 | 0.77332181 | 0.77332181 | 0.71434434 | 0.71434434 | 4.3785e-05 <-- new best val loss\n",
      "     6 | 0.75701771 | 0.75701771 | 0.70149923 | 0.70149923 | 4.515e-05  <-- new best val loss\n",
      "     7 | 0.74322617 | 0.74322617 | 0.69008671 | 0.69008671 | 4.6722e-05 <-- new best val loss\n",
      "     8 | 0.73074248 | 0.73074248 | 0.67984910 | 0.67984910 | 4.8503e-05 <-- new best val loss\n",
      "     9 | 0.71888615 | 0.71888615 | 0.66960066 | 0.66960066 | 5.049e-05  <-- new best val loss\n",
      "    10 | 0.70734349 | 0.70734349 | 0.65979641 | 0.65979641 | 5.2683e-05 <-- new best val loss\n",
      "    11 | 0.69640379 | 0.69640379 | 0.65008518 | 0.65008518 | 5.5081e-05 <-- new best val loss\n",
      "    12 | 0.68563912 | 0.68563912 | 0.64109212 | 0.64109212 | 5.7683e-05 <-- new best val loss\n",
      "    13 | 0.67518893 | 0.67518893 | 0.63251315 | 0.63251315 | 6.0488e-05 <-- new best val loss\n",
      "    14 | 0.66504557 | 0.66504557 | 0.62432175 | 0.62432175 | 6.3494e-05 <-- new best val loss\n",
      "    15 | 0.65552699 | 0.65552699 | 0.61655224 | 0.61655224 | 6.6701e-05 <-- new best val loss\n",
      "    16 | 0.64628642 | 0.64628642 | 0.60945031 | 0.60945031 | 7.0107e-05 <-- new best val loss\n",
      "    17 | 0.63774707 | 0.63774707 | 0.60334074 | 0.60334074 | 7.371e-05  <-- new best val loss\n",
      "    18 | 0.62970621 | 0.62970621 | 0.59732203 | 0.59732203 | 7.7508e-05 <-- new best val loss\n",
      "    19 | 0.62219672 | 0.62219672 | 0.59229558 | 0.59229558 | 8.1501e-05 <-- new best val loss\n",
      "    20 | 0.61527138 | 0.61527138 | 0.58784689 | 0.58784689 | 8.5686e-05 <-- new best val loss\n",
      "    21 | 0.60870664 | 0.60870664 | 0.58378291 | 0.58378291 | 9.0062e-05 <-- new best val loss\n",
      "    22 | 0.60276227 | 0.60276227 | 0.57999106 | 0.57999106 | 9.4626e-05 <-- new best val loss\n",
      "    23 | 0.59740772 | 0.59740772 | 0.57595357 | 0.57595357 | 9.9377e-05 <-- new best val loss\n",
      "    24 | 0.59249009 | 0.59249009 | 0.57258228 | 0.57258228 | 0.00010431 <-- new best val loss\n",
      "    25 | 0.58778656 | 0.58778656 | 0.56960289 | 0.56960289 | 0.00010943 <-- new best val loss\n",
      "    26 | 0.58357597 | 0.58357597 | 0.56665764 | 0.56665764 | 0.00011473 <-- new best val loss\n",
      "    27 | 0.57916150 | 0.57916150 | 0.56344995 | 0.56344995 | 0.0001202  <-- new best val loss\n",
      "    28 | 0.57520661 | 0.57520661 | 0.56052318 | 0.56052318 | 0.00012585 <-- new best val loss\n",
      "    29 | 0.57152357 | 0.57152357 | 0.55794461 | 0.55794461 | 0.00013168 <-- new best val loss\n",
      "    30 | 0.56789939 | 0.56789939 | 0.55488831 | 0.55488831 | 0.00013767 <-- new best val loss\n",
      "    31 | 0.56436788 | 0.56436788 | 0.55258339 | 0.55258339 | 0.00014383 <-- new best val loss\n",
      "    32 | 0.56108705 | 0.56108705 | 0.54991721 | 0.54991721 | 0.00015016 <-- new best val loss\n",
      "    33 | 0.55782968 | 0.55782968 | 0.54811319 | 0.54811319 | 0.00015665 <-- new best val loss\n",
      "    34 | 0.55468944 | 0.55468944 | 0.54542747 | 0.54542747 | 0.0001633  <-- new best val loss\n",
      "    35 | 0.55172202 | 0.55172202 | 0.54323722 | 0.54323722 | 0.0001701  <-- new best val loss\n",
      "    36 | 0.54866264 | 0.54866264 | 0.54134986 | 0.54134986 | 0.00017706 <-- new best val loss\n",
      "    37 | 0.54569160 | 0.54569160 | 0.53922424 | 0.53922424 | 0.00018417 <-- new best val loss\n",
      "    38 | 0.54281812 | 0.54281812 | 0.53674433 | 0.53674433 | 0.00019143 <-- new best val loss\n",
      "    39 | 0.54004084 | 0.54004084 | 0.53462202 | 0.53462202 | 0.00019883 <-- new best val loss\n",
      "    40 | 0.53732337 | 0.53732337 | 0.53300143 | 0.53300143 | 0.00020637 <-- new best val loss\n",
      "    41 | 0.53461708 | 0.53461708 | 0.53071063 | 0.53071063 | 0.00021405 <-- new best val loss\n",
      "    42 | 0.53211755 | 0.53211755 | 0.52941917 | 0.52941917 | 0.00022186 <-- new best val loss\n",
      "    43 | 0.52966330 | 0.52966330 | 0.52728576 | 0.52728576 | 0.0002298  <-- new best val loss\n",
      "    44 | 0.52699299 | 0.52699299 | 0.52573787 | 0.52573787 | 0.00023788 <-- new best val loss\n",
      "    45 | 0.52472605 | 0.52472605 | 0.52408443 | 0.52408443 | 0.00024607 <-- new best val loss\n",
      "    46 | 0.52247198 | 0.52247198 | 0.52232314 | 0.52232314 | 0.00025439 <-- new best val loss\n",
      "    47 | 0.52028686 | 0.52028686 | 0.52062351 | 0.52062351 | 0.00026282 <-- new best val loss\n",
      "    48 | 0.51825490 | 0.51825490 | 0.51908122 | 0.51908122 | 0.00027136 <-- new best val loss\n",
      "    49 | 0.51638634 | 0.51638634 | 0.51758596 | 0.51758596 | 0.00028001 <-- new best val loss\n",
      "    50 | 0.51452001 | 0.51452001 | 0.51623950 | 0.51623950 | 0.00028877 <-- new best val loss\n",
      "    51 | 0.51283959 | 0.51283959 | 0.51527799 | 0.51527799 | 0.00029763 <-- new best val loss\n",
      "    52 | 0.51123295 | 0.51123295 | 0.51391280 | 0.51391280 | 0.00030659 <-- new best val loss\n",
      "    53 | 0.50981889 | 0.50981889 | 0.51290727 | 0.51290727 | 0.00031564 <-- new best val loss\n",
      "    54 | 0.50824306 | 0.50824306 | 0.51169304 | 0.51169304 | 0.00032478 <-- new best val loss\n",
      "    55 | 0.50695020 | 0.50695020 | 0.51102930 | 0.51102930 | 0.00033401 <-- new best val loss\n",
      "    56 | 0.50553303 | 0.50553303 | 0.50960214 | 0.50960214 | 0.00034332 <-- new best val loss\n",
      "    57 | 0.50428307 | 0.50428307 | 0.50855208 | 0.50855208 | 0.0003527  <-- new best val loss\n",
      "    58 | 0.50314903 | 0.50314903 | 0.50749356 | 0.50749356 | 0.00036216 <-- new best val loss\n",
      "    59 | 0.50208070 | 0.50208070 | 0.50720991 | 0.50720991 | 0.00037169 <-- new best val loss\n",
      "    60 | 0.50080920 | 0.50080920 | 0.50597004 | 0.50597004 | 0.00038128 <-- new best val loss\n",
      "    61 | 0.49977516 | 0.49977516 | 0.50536334 | 0.50536334 | 0.00039094 <-- new best val loss\n",
      "    62 | 0.49867732 | 0.49867732 | 0.50459919 | 0.50459919 | 0.00040065 <-- new best val loss\n",
      "    63 | 0.49767895 | 0.49767895 | 0.50355076 | 0.50355076 | 0.00041041 <-- new best val loss\n",
      "    64 | 0.49673102 | 0.49673102 | 0.50266269 | 0.50266269 | 0.00042022 <-- new best val loss\n",
      "    65 | 0.49577062 | 0.49577062 | 0.50254007 | 0.50254007 | 0.00043008 <-- new best val loss\n",
      "    66 | 0.49495019 | 0.49495019 | 0.50168631 | 0.50168631 | 0.00043997 <-- new best val loss\n",
      "    67 | 0.49421283 | 0.49421283 | 0.50126957 | 0.50126957 | 0.0004499  <-- new best val loss\n",
      "    68 | 0.49327458 | 0.49327458 | 0.50046384 | 0.50046384 | 0.00045986 <-- new best val loss\n",
      "    69 | 0.49257189 | 0.49257189 | 0.50001692 | 0.50001692 | 0.00046985 <-- new best val loss\n",
      "    70 | 0.49187461 | 0.49187461 | 0.49955628 | 0.49955628 | 0.00047986 <-- new best val loss\n",
      "    71 | 0.49111622 | 0.49111622 | 0.49902583 | 0.49902583 | 0.00048989 <-- new best val loss\n",
      "    72 | 0.49039376 | 0.49039376 | 0.49801943 | 0.49801943 | 0.00049992 <-- new best val loss\n",
      "    73 | 0.48959036 | 0.48959036 | 0.49772163 | 0.49772163 | 0.00050997 <-- new best val loss\n",
      "    74 | 0.48919121 | 0.48919121 | 0.49694314 | 0.49694314 | 0.00052003 <-- new best val loss\n",
      "    75 | 0.48861306 | 0.48861306 | 0.49758641 | 0.49758641 | 0.00053008 \n",
      "    76 | 0.48803225 | 0.48803225 | 0.49623784 | 0.49623784 | 0.00054013 <-- new best val loss\n",
      "    77 | 0.48749175 | 0.48749175 | 0.49552865 | 0.49552865 | 0.00055017 <-- new best val loss\n",
      "    78 | 0.48691526 | 0.48691526 | 0.49521117 | 0.49521117 | 0.00056019 <-- new best val loss\n",
      "    79 | 0.48652171 | 0.48652171 | 0.49487631 | 0.49487631 | 0.0005702  <-- new best val loss\n",
      "    80 | 0.48589972 | 0.48589972 | 0.49457861 | 0.49457861 | 0.00058019 <-- new best val loss\n",
      "    81 | 0.48548792 | 0.48548792 | 0.49397449 | 0.49397449 | 0.00059015 <-- new best val loss\n",
      "    82 | 0.48509666 | 0.48509666 | 0.49399209 | 0.49399209 | 0.00060008 \n",
      "    83 | 0.48465153 | 0.48465153 | 0.49398009 | 0.49398009 | 0.00060997 \n",
      "    84 | 0.48423496 | 0.48423496 | 0.49287848 | 0.49287848 | 0.00061983 <-- new best val loss\n",
      "    85 | 0.48367251 | 0.48367251 | 0.49287906 | 0.49287906 | 0.00062964 \n",
      "    86 | 0.48331406 | 0.48331406 | 0.49223624 | 0.49223624 | 0.0006394  <-- new best val loss\n",
      "    87 | 0.48306554 | 0.48306554 | 0.49199115 | 0.49199115 | 0.00064911 <-- new best val loss\n",
      "    88 | 0.48253022 | 0.48253022 | 0.49120873 | 0.49120873 | 0.00065876 <-- new best val loss\n",
      "    89 | 0.48235591 | 0.48235591 | 0.49094951 | 0.49094951 | 0.00066836 <-- new best val loss\n",
      "    90 | 0.48179918 | 0.48179918 | 0.49240099 | 0.49240099 | 0.00067789 \n",
      "    91 | 0.48169660 | 0.48169660 | 0.49080221 | 0.49080221 | 0.00068734 <-- new best val loss\n",
      "    92 | 0.48120450 | 0.48120450 | 0.49023770 | 0.49023770 | 0.00069673 <-- new best val loss\n",
      "    93 | 0.48092773 | 0.48092773 | 0.48965733 | 0.48965733 | 0.00070604 <-- new best val loss\n",
      "    94 | 0.48044710 | 0.48044710 | 0.48899840 | 0.48899840 | 0.00071526 <-- new best val loss\n",
      "    95 | 0.48020500 | 0.48020500 | 0.48898794 | 0.48898794 | 0.0007244  <-- new best val loss\n",
      "    96 | 0.47979384 | 0.47979384 | 0.48867185 | 0.48867185 | 0.00073345 <-- new best val loss\n",
      "    97 | 0.47957707 | 0.47957707 | 0.48860490 | 0.48860490 | 0.00074241 <-- new best val loss\n",
      "    98 | 0.47916798 | 0.47916798 | 0.48800526 | 0.48800526 | 0.00075123 <-- new best val loss\n",
      "    99 | 0.47889559 | 0.47889559 | 0.48806073 | 0.48806073 | 0.00075999 \n",
      "   100 | 0.47855887 | 0.47855887 | 0.48713939 | 0.48713939 | 0.00076864 <-- new best val loss\n",
      "   101 | 0.47826535 | 0.47826535 | 0.48693072 | 0.48693072 | 0.00077718 <-- new best val loss\n",
      "   102 | 0.47799589 | 0.47799589 | 0.48644337 | 0.48644337 | 0.00078561 <-- new best val loss\n",
      "   103 | 0.47786965 | 0.47786965 | 0.48603138 | 0.48603138 | 0.00079393 <-- new best val loss\n",
      "   104 | 0.47747167 | 0.47747167 | 0.48563246 | 0.48563246 | 0.00080212 <-- new best val loss\n",
      "   105 | 0.47703587 | 0.47703587 | 0.48535498 | 0.48535498 | 0.0008102  <-- new best val loss\n",
      "   106 | 0.47679910 | 0.47679910 | 0.48452676 | 0.48452676 | 0.00081814 <-- new best val loss\n",
      "   107 | 0.47651442 | 0.47651442 | 0.48539674 | 0.48539674 | 0.00082595 \n",
      "   108 | 0.47630174 | 0.47630174 | 0.48452028 | 0.48452028 | 0.00083363 <-- new best val loss\n",
      "   109 | 0.47596705 | 0.47596705 | 0.48374524 | 0.48374524 | 0.00084113 <-- new best val loss\n",
      "   110 | 0.47559792 | 0.47559792 | 0.48499332 | 0.48499332 | 0.00084854 \n",
      "   111 | 0.47547031 | 0.47547031 | 0.48313861 | 0.48313861 | 0.00085579 <-- new best val loss\n",
      "   112 | 0.47508058 | 0.47508058 | 0.48303882 | 0.48303882 | 0.0008629  <-- new best val loss\n",
      "   113 | 0.47492937 | 0.47492937 | 0.48148847 | 0.48148847 | 0.00086986 <-- new best val loss\n",
      "   114 | 0.47465464 | 0.47465464 | 0.48298540 | 0.48298540 | 0.00087667 \n",
      "   115 | 0.47431578 | 0.47431578 | 0.48164333 | 0.48164333 | 0.00088332 \n",
      "   116 | 0.47402248 | 0.47402248 | 0.48140872 | 0.48140872 | 0.00088981 <-- new best val loss\n",
      "   117 | 0.47376258 | 0.47376258 | 0.48113259 | 0.48113259 | 0.00089613 <-- new best val loss\n",
      "   118 | 0.47344169 | 0.47344169 | 0.48154519 | 0.48154519 | 0.0009023  \n",
      "   119 | 0.47328977 | 0.47328977 | 0.48048482 | 0.48048482 | 0.00090826 <-- new best val loss\n",
      "   120 | 0.47299749 | 0.47299749 | 0.47982883 | 0.47982883 | 0.00091409 <-- new best val loss\n",
      "   121 | 0.47269503 | 0.47269503 | 0.48036225 | 0.48036225 | 0.00091974 \n",
      "   122 | 0.47252123 | 0.47252123 | 0.48047246 | 0.48047246 | 0.00092522 \n",
      "   123 | 0.47223030 | 0.47223030 | 0.47881109 | 0.47881109 | 0.00093052 <-- new best val loss\n",
      "   124 | 0.47209456 | 0.47209456 | 0.47796675 | 0.47796675 | 0.00093561 <-- new best val loss\n",
      "   125 | 0.47167221 | 0.47167221 | 0.47902819 | 0.47902819 | 0.00094055 \n",
      "   126 | 0.47141280 | 0.47141280 | 0.47894082 | 0.47894082 | 0.0009453  \n",
      "   127 | 0.47111473 | 0.47111473 | 0.47814204 | 0.47814204 | 0.00094987 \n",
      "   128 | 0.47104598 | 0.47104598 | 0.47891557 | 0.47891557 | 0.00095423 \n",
      "   129 | 0.47056955 | 0.47056955 | 0.47768328 | 0.47768328 | 0.00095842 <-- new best val loss\n",
      "   130 | 0.47053961 | 0.47053961 | 0.47806316 | 0.47806316 | 0.00096241 \n",
      "   131 | 0.47018738 | 0.47018738 | 0.47654803 | 0.47654803 | 0.00096621 <-- new best val loss\n",
      "   132 | 0.47027348 | 0.47027348 | 0.47736737 | 0.47736737 | 0.00096982 \n",
      "   133 | 0.46994343 | 0.46994343 | 0.47701713 | 0.47701713 | 0.00097323 \n",
      "   134 | 0.46951188 | 0.46951188 | 0.47835353 | 0.47835353 | 0.00097644 \n",
      "   135 | 0.46920890 | 0.46920890 | 0.47643295 | 0.47643295 | 0.00097945 <-- new best val loss\n",
      "   136 | 0.46922093 | 0.46922093 | 0.47774325 | 0.47774325 | 0.00098226 \n",
      "   137 | 0.46887802 | 0.46887802 | 0.47495247 | 0.47495247 | 0.00098487 <-- new best val loss\n",
      "   138 | 0.46870807 | 0.46870807 | 0.47482959 | 0.47482959 | 0.00098727 <-- new best val loss\n",
      "   139 | 0.46852809 | 0.46852809 | 0.47546124 | 0.47546124 | 0.00098947 \n",
      "   140 | 0.46824716 | 0.46824716 | 0.47551942 | 0.47551942 | 0.00099146 \n",
      "   141 | 0.46801586 | 0.46801586 | 0.47620863 | 0.47620863 | 0.00099324 \n",
      "   142 | 0.46801902 | 0.46801902 | 0.47489612 | 0.47489612 | 0.00099482 \n",
      "   143 | 0.46766994 | 0.46766994 | 0.47401119 | 0.47401119 | 0.00099619 <-- new best val loss\n",
      "   144 | 0.46759198 | 0.46759198 | 0.47506213 | 0.47506213 | 0.00099735 \n",
      "   145 | 0.46738868 | 0.46738868 | 0.47417574 | 0.47417574 | 0.0009983  \n",
      "   146 | 0.46705527 | 0.46705527 | 0.47451377 | 0.47451377 | 0.00099904 \n",
      "   147 | 0.46687799 | 0.46687799 | 0.47309026 | 0.47309026 | 0.00099957 <-- new best val loss\n",
      "   148 | 0.46685336 | 0.46685336 | 0.47306948 | 0.47306948 | 0.00099989 <-- new best val loss\n",
      "   149 | 0.46661561 | 0.46661561 | 0.47308135 | 0.47308135 | 0.001      \n",
      "   150 | 0.46612950 | 0.46612950 | 0.47398065 | 0.47398065 | 0.00099998 \n",
      "   151 | 0.46606039 | 0.46606039 | 0.47464891 | 0.47464891 | 0.00099992 \n",
      "   152 | 0.46595803 | 0.46595803 | 0.47295660 | 0.47295660 | 0.00099982 <-- new best val loss\n",
      "   153 | 0.46567609 | 0.46567609 | 0.47208328 | 0.47208328 | 0.00099968 <-- new best val loss\n",
      "   154 | 0.46561057 | 0.46561057 | 0.47356506 | 0.47356506 | 0.0009995  \n",
      "   155 | 0.46545070 | 0.46545070 | 0.47355237 | 0.47355237 | 0.00099928 \n",
      "   156 | 0.46515558 | 0.46515558 | 0.47414996 | 0.47414996 | 0.00099902 \n",
      "   157 | 0.46502311 | 0.46502311 | 0.47248378 | 0.47248378 | 0.00099872 \n",
      "   158 | 0.46473729 | 0.46473729 | 0.47328912 | 0.47328912 | 0.00099838 \n",
      "   159 | 0.46466050 | 0.46466050 | 0.47320777 | 0.47320777 | 0.000998   \n",
      "   160 | 0.46452070 | 0.46452070 | 0.47252730 | 0.47252730 | 0.00099758 \n",
      "   161 | 0.46426596 | 0.46426596 | 0.47181873 | 0.47181873 | 0.00099712 <-- new best val loss\n",
      "   162 | 0.46405749 | 0.46405749 | 0.46941449 | 0.46941449 | 0.00099662 <-- new best val loss\n",
      "   163 | 0.46365587 | 0.46365587 | 0.47171905 | 0.47171905 | 0.00099608 \n",
      "   164 | 0.46386288 | 0.46386288 | 0.47072863 | 0.47072863 | 0.0009955  \n",
      "   165 | 0.46360464 | 0.46360464 | 0.47072685 | 0.47072685 | 0.00099488 \n",
      "   166 | 0.46348491 | 0.46348491 | 0.46992535 | 0.46992535 | 0.00099421 \n",
      "   167 | 0.46315760 | 0.46315760 | 0.47151144 | 0.47151144 | 0.00099351 \n",
      "   168 | 0.46318786 | 0.46318786 | 0.47037384 | 0.47037384 | 0.00099277 \n",
      "   169 | 0.46306239 | 0.46306239 | 0.47255315 | 0.47255315 | 0.00099199 \n",
      "   170 | 0.46284050 | 0.46284050 | 0.46939388 | 0.46939388 | 0.00099117 <-- new best val loss\n",
      "   171 | 0.46265809 | 0.46265809 | 0.46962844 | 0.46962844 | 0.00099031 \n",
      "   172 | 0.46238685 | 0.46238685 | 0.47071185 | 0.47071185 | 0.00098942 \n",
      "   173 | 0.46224747 | 0.46224747 | 0.46834546 | 0.46834546 | 0.00098848 <-- new best val loss\n",
      "   174 | 0.46207834 | 0.46207834 | 0.46938429 | 0.46938429 | 0.0009875  \n",
      "   175 | 0.46192574 | 0.46192574 | 0.46903110 | 0.46903110 | 0.00098649 \n",
      "   176 | 0.46174064 | 0.46174064 | 0.47043017 | 0.47043017 | 0.00098543 \n",
      "   177 | 0.46171578 | 0.46171578 | 0.46794347 | 0.46794347 | 0.00098434 <-- new best val loss\n",
      "   178 | 0.46173151 | 0.46173151 | 0.46825102 | 0.46825102 | 0.0009832  \n",
      "   179 | 0.46164466 | 0.46164466 | 0.47096979 | 0.47096979 | 0.00098203 \n",
      "   180 | 0.46134080 | 0.46134080 | 0.46765548 | 0.46765548 | 0.00098082 <-- new best val loss\n",
      "   181 | 0.46130566 | 0.46130566 | 0.46901250 | 0.46901250 | 0.00097957 \n",
      "   182 | 0.46124303 | 0.46124303 | 0.47005439 | 0.47005439 | 0.00097828 \n",
      "   183 | 0.46101313 | 0.46101313 | 0.46837565 | 0.46837565 | 0.00097695 \n",
      "   184 | 0.46082900 | 0.46082900 | 0.46693760 | 0.46693760 | 0.00097559 <-- new best val loss\n",
      "   185 | 0.46041156 | 0.46041156 | 0.46886379 | 0.46886379 | 0.00097419 \n",
      "   186 | 0.46056731 | 0.46056731 | 0.46843245 | 0.46843245 | 0.00097275 \n",
      "   187 | 0.46043158 | 0.46043158 | 0.46804068 | 0.46804068 | 0.00097126 \n",
      "   188 | 0.46034757 | 0.46034757 | 0.46917484 | 0.46917484 | 0.00096975 \n",
      "   189 | 0.46020575 | 0.46020575 | 0.46700425 | 0.46700425 | 0.00096819 \n",
      "   190 | 0.45997614 | 0.45997614 | 0.46714485 | 0.46714485 | 0.0009666  \n",
      "   191 | 0.46002252 | 0.46002252 | 0.46932937 | 0.46932937 | 0.00096496 \n",
      "   192 | 0.45986117 | 0.45986117 | 0.46759975 | 0.46759975 | 0.0009633  \n",
      "   193 | 0.45968736 | 0.45968736 | 0.46676509 | 0.46676509 | 0.00096159 <-- new best val loss\n",
      "   194 | 0.45964763 | 0.45964763 | 0.46970216 | 0.46970216 | 0.00095985 \n",
      "   195 | 0.45939553 | 0.45939553 | 0.46666266 | 0.46666266 | 0.00095806 <-- new best val loss\n",
      "   196 | 0.45940227 | 0.45940227 | 0.46525664 | 0.46525664 | 0.00095626 <-- new best val loss\n",
      "   197 | 0.45914426 | 0.45914426 | 0.46697664 | 0.46697664 | 0.0009544  \n",
      "   198 | 0.45910430 | 0.45910430 | 0.46742691 | 0.46742691 | 0.00095251 \n",
      "   199 | 0.45901577 | 0.45901577 | 0.46632523 | 0.46632523 | 0.00095058 \n",
      "   200 | 0.45887782 | 0.45887782 | 0.46710156 | 0.46710156 | 0.00094862 \n",
      "   201 | 0.45877072 | 0.45877072 | 0.46680876 | 0.46680876 | 0.00094662 \n",
      "   202 | 0.45873232 | 0.45873232 | 0.46606352 | 0.46606352 | 0.00094459 \n",
      "   203 | 0.45866477 | 0.45866477 | 0.46610602 | 0.46610602 | 0.00094251 \n",
      "   204 | 0.45833787 | 0.45833787 | 0.46576947 | 0.46576947 | 0.00094041 \n",
      "   205 | 0.45818752 | 0.45818752 | 0.46659560 | 0.46659560 | 0.00093826 \n",
      "   206 | 0.45824488 | 0.45824488 | 0.46463846 | 0.46463846 | 0.00093609 <-- new best val loss\n",
      "   207 | 0.45798146 | 0.45798146 | 0.46543905 | 0.46543905 | 0.00093388 \n",
      "   208 | 0.45786333 | 0.45786333 | 0.46527027 | 0.46527027 | 0.00093164 \n",
      "   209 | 0.45788066 | 0.45788066 | 0.46590830 | 0.46590830 | 0.00092935 \n",
      "   210 | 0.45767716 | 0.45767716 | 0.46540626 | 0.46540626 | 0.00092704 \n",
      "   211 | 0.45748672 | 0.45748672 | 0.46416476 | 0.46416476 | 0.00092469 <-- new best val loss\n",
      "   212 | 0.45737843 | 0.45737843 | 0.46619198 | 0.46619198 | 0.0009223  \n",
      "   213 | 0.45741968 | 0.45741968 | 0.46541023 | 0.46541023 | 0.00091988 \n",
      "   214 | 0.45719350 | 0.45719350 | 0.46510104 | 0.46510104 | 0.00091743 \n",
      "   215 | 0.45712999 | 0.45712999 | 0.46697841 | 0.46697841 | 0.00091494 \n",
      "   216 | 0.45696329 | 0.45696329 | 0.46464246 | 0.46464246 | 0.00091242 \n",
      "   217 | 0.45681898 | 0.45681898 | 0.46517123 | 0.46517123 | 0.00090986 \n",
      "   218 | 0.45675059 | 0.45675059 | 0.46342155 | 0.46342155 | 0.00090728 <-- new best val loss\n",
      "   219 | 0.45670916 | 0.45670916 | 0.46509833 | 0.46509833 | 0.00090467 \n",
      "   220 | 0.45640552 | 0.45640552 | 0.46490768 | 0.46490768 | 0.00090202 \n",
      "   221 | 0.45657402 | 0.45657402 | 0.46448097 | 0.46448097 | 0.00089933 \n",
      "   222 | 0.45622623 | 0.45622623 | 0.46454214 | 0.46454214 | 0.00089662 \n",
      "   223 | 0.45635840 | 0.45635840 | 0.46334347 | 0.46334347 | 0.00089387 <-- new best val loss\n",
      "   224 | 0.45615234 | 0.45615234 | 0.46276649 | 0.46276649 | 0.0008911  <-- new best val loss\n",
      "   225 | 0.45574297 | 0.45574297 | 0.46266846 | 0.46266846 | 0.00088829 <-- new best val loss\n",
      "   226 | 0.45582202 | 0.45582202 | 0.46390737 | 0.46390737 | 0.00088545 \n",
      "   227 | 0.45589023 | 0.45589023 | 0.46361139 | 0.46361139 | 0.00088257 \n",
      "   228 | 0.45569517 | 0.45569517 | 0.46321485 | 0.46321485 | 0.00087967 \n",
      "   229 | 0.45548016 | 0.45548016 | 0.46232937 | 0.46232937 | 0.00087673 <-- new best val loss\n",
      "   230 | 0.45535155 | 0.45535155 | 0.46114646 | 0.46114646 | 0.00087377 <-- new best val loss\n",
      "   231 | 0.45535696 | 0.45535696 | 0.46322212 | 0.46322212 | 0.00087077 \n",
      "   232 | 0.45520758 | 0.45520758 | 0.46304016 | 0.46304016 | 0.00086774 \n",
      "   233 | 0.45500394 | 0.45500394 | 0.46201420 | 0.46201420 | 0.00086469 \n",
      "   234 | 0.45497451 | 0.45497451 | 0.46210085 | 0.46210085 | 0.0008616  \n",
      "   235 | 0.45486166 | 0.45486166 | 0.46285845 | 0.46285845 | 0.00085849 \n",
      "   236 | 0.45471804 | 0.45471804 | 0.46135670 | 0.46135670 | 0.00085535 \n",
      "   237 | 0.45449979 | 0.45449979 | 0.46290237 | 0.46290237 | 0.00085218 \n",
      "   238 | 0.45430097 | 0.45430097 | 0.46286773 | 0.46286773 | 0.00084898 \n",
      "   239 | 0.45422262 | 0.45422262 | 0.46146248 | 0.46146248 | 0.00084575 \n",
      "   240 | 0.45424749 | 0.45424749 | 0.46282630 | 0.46282630 | 0.00084249 \n",
      "   241 | 0.45407577 | 0.45407577 | 0.46208687 | 0.46208687 | 0.00083921 \n",
      "   242 | 0.45405226 | 0.45405226 | 0.46317901 | 0.46317901 | 0.0008359  \n",
      "   243 | 0.45374851 | 0.45374851 | 0.46032608 | 0.46032608 | 0.00083256 <-- new best val loss\n",
      "   244 | 0.45357046 | 0.45357046 | 0.46111318 | 0.46111318 | 0.00082919 \n",
      "   245 | 0.45354858 | 0.45354858 | 0.46072591 | 0.46072591 | 0.0008258  \n",
      "   246 | 0.45344048 | 0.45344048 | 0.46055604 | 0.46055604 | 0.0008224  \n",
      "   247 | 0.45319394 | 0.45319394 | 0.45941307 | 0.45941307 | 0.00081896 <-- new best val loss\n",
      "   248 | 0.45317672 | 0.45317672 | 0.46051040 | 0.46051040 | 0.00081549 \n",
      "   249 | 0.45304837 | 0.45304837 | 0.46242515 | 0.46242515 | 0.000812   \n",
      "   250 | 0.45290983 | 0.45290983 | 0.46295587 | 0.46295587 | 0.00080848 \n",
      "   251 | 0.45298487 | 0.45298487 | 0.46193735 | 0.46193735 | 0.00080493 \n",
      "   252 | 0.45254248 | 0.45254248 | 0.45901440 | 0.45901440 | 0.00080136 <-- new best val loss\n",
      "   253 | 0.45251469 | 0.45251469 | 0.46061123 | 0.46061123 | 0.00079777 \n",
      "   254 | 0.45231709 | 0.45231709 | 0.45994358 | 0.45994358 | 0.00079415 \n",
      "   255 | 0.45223863 | 0.45223863 | 0.46067958 | 0.46067958 | 0.00079051 \n",
      "   256 | 0.45217144 | 0.45217144 | 0.45942709 | 0.45942709 | 0.00078685 \n",
      "   257 | 0.45215620 | 0.45215620 | 0.45932432 | 0.45932432 | 0.00078318 \n",
      "   258 | 0.45155993 | 0.45155993 | 0.45871347 | 0.45871347 | 0.00077947 <-- new best val loss\n",
      "   259 | 0.45159413 | 0.45159413 | 0.45960054 | 0.45960054 | 0.00077574 \n",
      "   260 | 0.45152587 | 0.45152587 | 0.45952491 | 0.45952491 | 0.00077198 \n",
      "   261 | 0.45141302 | 0.45141302 | 0.45793365 | 0.45793365 | 0.0007682  <-- new best val loss\n",
      "   262 | 0.45119094 | 0.45119094 | 0.45837834 | 0.45837834 | 0.00076441 \n",
      "   263 | 0.45120346 | 0.45120346 | 0.45785910 | 0.45785910 | 0.00076059 <-- new best val loss\n",
      "   264 | 0.45093285 | 0.45093285 | 0.45876805 | 0.45876805 | 0.00075675 \n",
      "   265 | 0.45100095 | 0.45100095 | 0.45906761 | 0.45906761 | 0.00075288 \n",
      "   266 | 0.45076352 | 0.45076352 | 0.45775048 | 0.45775048 | 0.000749   <-- new best val loss\n",
      "   267 | 0.45064753 | 0.45064753 | 0.45813916 | 0.45813916 | 0.0007451  \n",
      "   268 | 0.45039323 | 0.45039323 | 0.45840223 | 0.45840223 | 0.0007412  \n",
      "   269 | 0.45029101 | 0.45029101 | 0.45707149 | 0.45707149 | 0.00073728 <-- new best val loss\n",
      "   270 | 0.45005329 | 0.45005329 | 0.45792314 | 0.45792314 | 0.00073332 \n",
      "   271 | 0.45006008 | 0.45006008 | 0.45644229 | 0.45644229 | 0.00072934 <-- new best val loss\n",
      "   272 | 0.44999382 | 0.44999382 | 0.45682164 | 0.45682164 | 0.00072534 \n",
      "   273 | 0.44983578 | 0.44983578 | 0.45736533 | 0.45736533 | 0.00072133 \n",
      "   274 | 0.44974569 | 0.44974569 | 0.45654272 | 0.45654272 | 0.00071729 \n",
      "   275 | 0.44960539 | 0.44960539 | 0.45868497 | 0.45868497 | 0.00071324 \n",
      "   276 | 0.44941644 | 0.44941644 | 0.45633973 | 0.45633973 | 0.00070918 <-- new best val loss\n",
      "   277 | 0.44918398 | 0.44918398 | 0.45615702 | 0.45615702 | 0.00070509 <-- new best val loss\n",
      "   278 | 0.44931249 | 0.44931249 | 0.45628155 | 0.45628155 | 0.00070099 \n",
      "   279 | 0.44885495 | 0.44885495 | 0.45563586 | 0.45563586 | 0.00069687 <-- new best val loss\n",
      "   280 | 0.44890441 | 0.44890441 | 0.45567620 | 0.45567620 | 0.00069274 \n",
      "   281 | 0.44885419 | 0.44885419 | 0.45742775 | 0.45742775 | 0.00068859 \n",
      "   282 | 0.44853575 | 0.44853575 | 0.45414715 | 0.45414715 | 0.00068445 <-- new best val loss\n",
      "   283 | 0.44841123 | 0.44841123 | 0.45543767 | 0.45543767 | 0.00068027 \n",
      "   284 | 0.44831924 | 0.44831924 | 0.45609830 | 0.45609830 | 0.00067607 \n",
      "   285 | 0.44825751 | 0.44825751 | 0.45669565 | 0.45669565 | 0.00067187 \n",
      "   286 | 0.44804315 | 0.44804315 | 0.45472701 | 0.45472701 | 0.00066765 \n",
      "   287 | 0.44799567 | 0.44799567 | 0.45490260 | 0.45490260 | 0.00066341 \n",
      "   288 | 0.44766393 | 0.44766393 | 0.45468357 | 0.45468357 | 0.00065916 \n",
      "   289 | 0.44773926 | 0.44773926 | 0.45403294 | 0.45403294 | 0.0006549  <-- new best val loss\n",
      "   290 | 0.44746136 | 0.44746136 | 0.45429060 | 0.45429060 | 0.00065063 \n",
      "   291 | 0.44750125 | 0.44750125 | 0.45355561 | 0.45355561 | 0.00064634 <-- new best val loss\n",
      "   292 | 0.44739486 | 0.44739486 | 0.45472172 | 0.45472172 | 0.00064205 \n",
      "   293 | 0.44708696 | 0.44708696 | 0.45353388 | 0.45353388 | 0.00063774 <-- new best val loss\n",
      "   294 | 0.44684225 | 0.44684225 | 0.45358564 | 0.45358564 | 0.00063342 \n",
      "   295 | 0.44688754 | 0.44688754 | 0.45182654 | 0.45182654 | 0.00062909 <-- new best val loss\n",
      "   296 | 0.44673537 | 0.44673537 | 0.45300429 | 0.45300429 | 0.00062475 \n",
      "   297 | 0.44645085 | 0.44645085 | 0.45284048 | 0.45284048 | 0.00062039 \n",
      "   298 | 0.44655681 | 0.44655681 | 0.45357126 | 0.45357126 | 0.00061603 \n",
      "   299 | 0.44636910 | 0.44636910 | 0.45127538 | 0.45127538 | 0.00061169 <-- new best val loss\n",
      "   300 | 0.44603390 | 0.44603390 | 0.45394458 | 0.45394458 | 0.00060731 \n",
      "   301 | 0.44617289 | 0.44617289 | 0.45205605 | 0.45205605 | 0.00060292 \n",
      "   302 | 0.44592917 | 0.44592917 | 0.45306321 | 0.45306321 | 0.00059852 \n",
      "   303 | 0.44580892 | 0.44580892 | 0.45170725 | 0.45170725 | 0.00059412 \n",
      "   304 | 0.44582983 | 0.44582983 | 0.45210436 | 0.45210436 | 0.00058971 \n",
      "   305 | 0.44532860 | 0.44532860 | 0.45230674 | 0.45230674 | 0.00058529 \n",
      "   306 | 0.44527224 | 0.44527224 | 0.45115178 | 0.45115178 | 0.00058086 <-- new best val loss\n",
      "   307 | 0.44544232 | 0.44544232 | 0.45163679 | 0.45163679 | 0.00057643 \n",
      "   308 | 0.44505364 | 0.44505364 | 0.45199169 | 0.45199169 | 0.00057199 \n",
      "   309 | 0.44497377 | 0.44497377 | 0.45182912 | 0.45182912 | 0.00056755 \n",
      "   310 | 0.44484988 | 0.44484988 | 0.45149122 | 0.45149122 | 0.0005631  \n",
      "   311 | 0.44454817 | 0.44454817 | 0.45054497 | 0.45054497 | 0.00055865 <-- new best val loss\n",
      "   312 | 0.44452398 | 0.44452398 | 0.45097946 | 0.45097946 | 0.00055419 \n",
      "   313 | 0.44452920 | 0.44452920 | 0.45144743 | 0.45144743 | 0.00054972 \n",
      "   314 | 0.44412746 | 0.44412746 | 0.45198951 | 0.45198951 | 0.00054525 \n",
      "   315 | 0.44396030 | 0.44396030 | 0.45001321 | 0.45001321 | 0.00054078 <-- new best val loss\n",
      "   316 | 0.44395760 | 0.44395760 | 0.44937083 | 0.44937083 | 0.00053633 <-- new best val loss\n",
      "   317 | 0.44387814 | 0.44387814 | 0.45046847 | 0.45046847 | 0.00053185 \n",
      "   318 | 0.44367621 | 0.44367621 | 0.45043189 | 0.45043189 | 0.00052737 \n",
      "   319 | 0.44359602 | 0.44359602 | 0.45038415 | 0.45038415 | 0.00052289 \n",
      "   320 | 0.44352084 | 0.44352084 | 0.44999410 | 0.44999410 | 0.00051841 \n",
      "   321 | 0.44324049 | 0.44324049 | 0.45017463 | 0.45017463 | 0.00051392 \n",
      "   322 | 0.44311398 | 0.44311398 | 0.44963588 | 0.44963588 | 0.00050944 \n",
      "   323 | 0.44295094 | 0.44295094 | 0.44905840 | 0.44905840 | 0.00050495 <-- new best val loss\n",
      "   324 | 0.44279182 | 0.44279182 | 0.44877763 | 0.44877763 | 0.00050046 <-- new best val loss\n",
      "   325 | 0.44267476 | 0.44267476 | 0.44906851 | 0.44906851 | 0.00049597 \n",
      "   326 | 0.44257631 | 0.44257631 | 0.44848377 | 0.44848377 | 0.00049148 <-- new best val loss\n",
      "   327 | 0.44243298 | 0.44243298 | 0.44804636 | 0.44804636 | 0.00048702 <-- new best val loss\n",
      "   328 | 0.44225380 | 0.44225380 | 0.44873616 | 0.44873616 | 0.00048253 \n",
      "   329 | 0.44219205 | 0.44219205 | 0.44871133 | 0.44871133 | 0.00047805 \n",
      "   330 | 0.44217648 | 0.44217648 | 0.44813293 | 0.44813293 | 0.00047357 \n",
      "   331 | 0.44170716 | 0.44170716 | 0.44663072 | 0.44663072 | 0.00046909 <-- new best val loss\n",
      "   332 | 0.44155860 | 0.44155860 | 0.44739850 | 0.44739850 | 0.00046461 \n",
      "   333 | 0.44146426 | 0.44146426 | 0.44857079 | 0.44857079 | 0.00046013 \n",
      "   334 | 0.44134488 | 0.44134488 | 0.44743637 | 0.44743637 | 0.00045566 \n",
      "   335 | 0.44112772 | 0.44112772 | 0.44671032 | 0.44671032 | 0.00045119 \n",
      "   336 | 0.44114046 | 0.44114046 | 0.44794380 | 0.44794380 | 0.00044673 \n",
      "   337 | 0.44068353 | 0.44068353 | 0.44774330 | 0.44774330 | 0.00044227 \n",
      "   338 | 0.44087917 | 0.44087917 | 0.44671044 | 0.44671044 | 0.00043781 \n",
      "   339 | 0.44074210 | 0.44074210 | 0.44573069 | 0.44573069 | 0.00043336 <-- new best val loss\n",
      "   340 | 0.44056281 | 0.44056281 | 0.44781216 | 0.44781216 | 0.00042892 \n",
      "   341 | 0.44031354 | 0.44031354 | 0.44659139 | 0.44659139 | 0.0004245  \n",
      "   342 | 0.44015518 | 0.44015518 | 0.44632436 | 0.44632436 | 0.00042007 \n",
      "   343 | 0.44011639 | 0.44011639 | 0.44547960 | 0.44547960 | 0.00041564 <-- new best val loss\n",
      "   344 | 0.43997071 | 0.43997071 | 0.44509258 | 0.44509258 | 0.00041122 <-- new best val loss\n",
      "   345 | 0.43974255 | 0.43974255 | 0.44496447 | 0.44496447 | 0.00040681 <-- new best val loss\n",
      "   346 | 0.43968506 | 0.43968506 | 0.44542915 | 0.44542915 | 0.0004024  \n",
      "   347 | 0.43949862 | 0.43949862 | 0.44637866 | 0.44637866 | 0.000398   \n",
      "   348 | 0.43933547 | 0.43933547 | 0.44513733 | 0.44513733 | 0.00039361 \n",
      "   349 | 0.43913813 | 0.43913813 | 0.44577181 | 0.44577181 | 0.00038923 \n",
      "   350 | 0.43908323 | 0.43908323 | 0.44431666 | 0.44431666 | 0.00038486 <-- new best val loss\n",
      "   351 | 0.43892097 | 0.43892097 | 0.44457065 | 0.44457065 | 0.0003805  \n",
      "   352 | 0.43862083 | 0.43862083 | 0.44445311 | 0.44445311 | 0.00037617 \n",
      "   353 | 0.43868115 | 0.43868115 | 0.44422012 | 0.44422012 | 0.00037182 <-- new best val loss\n",
      "   354 | 0.43840853 | 0.43840853 | 0.44437765 | 0.44437765 | 0.00036749 \n",
      "   355 | 0.43829186 | 0.43829186 | 0.44401347 | 0.44401347 | 0.00036317 <-- new best val loss\n",
      "   356 | 0.43804077 | 0.43804077 | 0.44387838 | 0.44387838 | 0.00035886 <-- new best val loss\n",
      "   357 | 0.43808947 | 0.43808947 | 0.44410434 | 0.44410434 | 0.00035456 \n",
      "   358 | 0.43785237 | 0.43785237 | 0.44314683 | 0.44314683 | 0.00035027 <-- new best val loss\n",
      "   359 | 0.43782887 | 0.43782887 | 0.44331886 | 0.44331886 | 0.000346   \n",
      "   360 | 0.43761026 | 0.43761026 | 0.44310965 | 0.44310965 | 0.00034173 <-- new best val loss\n",
      "   361 | 0.43752541 | 0.43752541 | 0.44350611 | 0.44350611 | 0.00033748 \n",
      "   362 | 0.43736205 | 0.43736205 | 0.44327105 | 0.44327105 | 0.00033324 \n",
      "   363 | 0.43707658 | 0.43707658 | 0.44220393 | 0.44220393 | 0.00032904 <-- new best val loss\n",
      "   364 | 0.43692086 | 0.43692086 | 0.44225181 | 0.44225181 | 0.00032483 \n",
      "   365 | 0.43700850 | 0.43700850 | 0.44225680 | 0.44225680 | 0.00032063 \n",
      "   366 | 0.43667387 | 0.43667387 | 0.44167372 | 0.44167372 | 0.00031645 <-- new best val loss\n",
      "   367 | 0.43648803 | 0.43648803 | 0.44150186 | 0.44150186 | 0.00031228 <-- new best val loss\n",
      "   368 | 0.43661500 | 0.43661500 | 0.44113112 | 0.44113112 | 0.00030813 <-- new best val loss\n",
      "   369 | 0.43629070 | 0.43629070 | 0.44144194 | 0.44144194 | 0.000304   \n",
      "   370 | 0.43626687 | 0.43626687 | 0.44093374 | 0.44093374 | 0.00029987 <-- new best val loss\n",
      "   371 | 0.43606741 | 0.43606741 | 0.44126958 | 0.44126958 | 0.00029577 \n",
      "   372 | 0.43582404 | 0.43582404 | 0.44129931 | 0.44129931 | 0.00029168 \n",
      "   373 | 0.43572971 | 0.43572971 | 0.44082606 | 0.44082606 | 0.00028761 <-- new best val loss\n",
      "   374 | 0.43554337 | 0.43554337 | 0.44042469 | 0.44042469 | 0.00028358 <-- new best val loss\n",
      "   375 | 0.43530325 | 0.43530325 | 0.44032442 | 0.44032442 | 0.00027954 <-- new best val loss\n",
      "   376 | 0.43527690 | 0.43527690 | 0.44037393 | 0.44037393 | 0.00027552 \n",
      "   377 | 0.43513334 | 0.43513334 | 0.44037702 | 0.44037702 | 0.00027152 \n",
      "   378 | 0.43492475 | 0.43492475 | 0.44071510 | 0.44071510 | 0.00026754 \n",
      "   379 | 0.43494523 | 0.43494523 | 0.43946716 | 0.43946716 | 0.00026357 <-- new best val loss\n",
      "   380 | 0.43471966 | 0.43471966 | 0.43996348 | 0.43996348 | 0.00025963 \n",
      "   381 | 0.43450521 | 0.43450521 | 0.43931904 | 0.43931904 | 0.0002557  <-- new best val loss\n",
      "   382 | 0.43448955 | 0.43448955 | 0.43951622 | 0.43951622 | 0.0002518  \n",
      "   383 | 0.43412748 | 0.43412748 | 0.43949732 | 0.43949732 | 0.00024791 \n",
      "   384 | 0.43402909 | 0.43402909 | 0.43947148 | 0.43947148 | 0.00024404 \n",
      "   385 | 0.43398147 | 0.43398147 | 0.43896511 | 0.43896511 | 0.00024022 <-- new best val loss\n",
      "   386 | 0.43372868 | 0.43372868 | 0.43901803 | 0.43901803 | 0.0002364  \n",
      "   387 | 0.43366762 | 0.43366762 | 0.43902005 | 0.43902005 | 0.00023259 \n",
      "   388 | 0.43357737 | 0.43357737 | 0.43879538 | 0.43879538 | 0.00022881 <-- new best val loss\n",
      "   389 | 0.43348692 | 0.43348692 | 0.43856169 | 0.43856169 | 0.00022505 <-- new best val loss\n",
      "   390 | 0.43316747 | 0.43316747 | 0.43833188 | 0.43833188 | 0.00022131 <-- new best val loss\n",
      "   391 | 0.43317823 | 0.43317823 | 0.43801961 | 0.43801961 | 0.0002176  <-- new best val loss\n",
      "   392 | 0.43298521 | 0.43298521 | 0.43729245 | 0.43729245 | 0.00021391 <-- new best val loss\n",
      "   393 | 0.43275270 | 0.43275270 | 0.43827165 | 0.43827165 | 0.00021024 \n",
      "   394 | 0.43274787 | 0.43274787 | 0.43712402 | 0.43712402 | 0.00020659 <-- new best val loss\n",
      "   395 | 0.43263131 | 0.43263131 | 0.43721096 | 0.43721096 | 0.00020297 \n",
      "   396 | 0.43251501 | 0.43251501 | 0.43749739 | 0.43749739 | 0.00019939 \n",
      "   397 | 0.43216669 | 0.43216669 | 0.43765757 | 0.43765757 | 0.00019582 \n",
      "   398 | 0.43224980 | 0.43224980 | 0.43725401 | 0.43725401 | 0.00019227 \n",
      "   399 | 0.43198951 | 0.43198951 | 0.43692170 | 0.43692170 | 0.00018874 <-- new best val loss\n",
      "   400 | 0.43197198 | 0.43197198 | 0.43678839 | 0.43678839 | 0.00018524 <-- new best val loss\n",
      "   401 | 0.43163848 | 0.43163848 | 0.43684844 | 0.43684844 | 0.00018177 \n",
      "   402 | 0.43161074 | 0.43161074 | 0.43680730 | 0.43680730 | 0.00017832 \n",
      "   403 | 0.43146961 | 0.43146961 | 0.43649531 | 0.43649531 | 0.0001749  <-- new best val loss\n",
      "   404 | 0.43131929 | 0.43131929 | 0.43614246 | 0.43614246 | 0.0001715  <-- new best val loss\n",
      "   405 | 0.43116893 | 0.43116893 | 0.43631859 | 0.43631859 | 0.00016813 \n",
      "   406 | 0.43098660 | 0.43098660 | 0.43618903 | 0.43618903 | 0.00016479 \n",
      "   407 | 0.43108391 | 0.43108391 | 0.43607643 | 0.43607643 | 0.00016149 <-- new best val loss\n",
      "   408 | 0.43081925 | 0.43081925 | 0.43561914 | 0.43561914 | 0.00015821 <-- new best val loss\n",
      "   409 | 0.43066705 | 0.43066705 | 0.43549266 | 0.43549266 | 0.00015495 <-- new best val loss\n",
      "   410 | 0.43059552 | 0.43059552 | 0.43596708 | 0.43596708 | 0.00015172 \n",
      "   411 | 0.43046173 | 0.43046173 | 0.43549437 | 0.43549437 | 0.00014851 \n",
      "   412 | 0.43029545 | 0.43029545 | 0.43543437 | 0.43543437 | 0.00014533 <-- new best val loss\n",
      "   413 | 0.43027540 | 0.43027540 | 0.43504108 | 0.43504108 | 0.00014219 <-- new best val loss\n",
      "   414 | 0.43016782 | 0.43016782 | 0.43559137 | 0.43559137 | 0.00013907 \n",
      "   415 | 0.42990924 | 0.42990924 | 0.43519052 | 0.43519052 | 0.00013597 \n",
      "   416 | 0.42985944 | 0.42985944 | 0.43485005 | 0.43485005 | 0.00013291 <-- new best val loss\n",
      "   417 | 0.42962676 | 0.42962676 | 0.43493588 | 0.43493588 | 0.00012988 \n",
      "   418 | 0.42963565 | 0.42963565 | 0.43463479 | 0.43463479 | 0.00012688 <-- new best val loss\n",
      "   419 | 0.42949423 | 0.42949423 | 0.43461661 | 0.43461661 | 0.00012391 <-- new best val loss\n",
      "   420 | 0.42933524 | 0.42933524 | 0.43443227 | 0.43443227 | 0.00012096 <-- new best val loss\n",
      "   421 | 0.42927841 | 0.42927841 | 0.43457132 | 0.43457132 | 0.00011805 \n",
      "   422 | 0.42922753 | 0.42922753 | 0.43436922 | 0.43436922 | 0.00011517 <-- new best val loss\n",
      "   423 | 0.42917314 | 0.42917314 | 0.43413413 | 0.43413413 | 0.00011232 <-- new best val loss\n",
      "   424 | 0.42889137 | 0.42889137 | 0.43426473 | 0.43426473 | 0.0001095  \n",
      "   425 | 0.42879860 | 0.42879860 | 0.43421286 | 0.43421286 | 0.00010672 \n",
      "   426 | 0.42866689 | 0.42866689 | 0.43408587 | 0.43408587 | 0.00010396 <-- new best val loss\n",
      "   427 | 0.42848504 | 0.42848504 | 0.43395028 | 0.43395028 | 0.00010124 <-- new best val loss\n",
      "   428 | 0.42846485 | 0.42846485 | 0.43356952 | 0.43356952 | 9.8545e-05 <-- new best val loss\n",
      "   429 | 0.42819816 | 0.42819816 | 0.43350978 | 0.43350978 | 9.5899e-05 <-- new best val loss\n",
      "   430 | 0.42816215 | 0.42816215 | 0.43351430 | 0.43351430 | 9.3273e-05 \n",
      "   431 | 0.42799683 | 0.42799683 | 0.43354748 | 0.43354748 | 9.0679e-05 \n",
      "   432 | 0.42798950 | 0.42798950 | 0.43338638 | 0.43338638 | 8.8131e-05 <-- new best val loss\n",
      "   433 | 0.42801488 | 0.42801488 | 0.43336426 | 0.43336426 | 8.5603e-05 <-- new best val loss\n",
      "   434 | 0.42778564 | 0.42778564 | 0.43354120 | 0.43354120 | 8.3109e-05 \n",
      "   435 | 0.42761591 | 0.42761591 | 0.43331012 | 0.43331012 | 8.0648e-05 <-- new best val loss\n",
      "   436 | 0.42768068 | 0.42768068 | 0.43322064 | 0.43322064 | 7.822e-05  <-- new best val loss\n",
      "   437 | 0.42758411 | 0.42758411 | 0.43312722 | 0.43312722 | 7.5827e-05 <-- new best val loss\n",
      "   438 | 0.42742200 | 0.42742200 | 0.43302609 | 0.43302609 | 7.3468e-05 <-- new best val loss\n",
      "   439 | 0.42729575 | 0.42729575 | 0.43284560 | 0.43284560 | 7.1144e-05 <-- new best val loss\n",
      "   440 | 0.42710781 | 0.42710781 | 0.43249769 | 0.43249769 | 6.8854e-05 <-- new best val loss\n",
      "   441 | 0.42713046 | 0.42713046 | 0.43262920 | 0.43262920 | 6.6598e-05 \n",
      "   442 | 0.42703196 | 0.42703196 | 0.43267622 | 0.43267622 | 6.4378e-05 \n",
      "   443 | 0.42685693 | 0.42685693 | 0.43265131 | 0.43265131 | 6.2193e-05 \n",
      "   444 | 0.42686424 | 0.42686424 | 0.43268175 | 0.43268175 | 6.0043e-05 \n",
      "   445 | 0.42668815 | 0.42668815 | 0.43252903 | 0.43252903 | 5.7928e-05 \n",
      "   446 | 0.42661671 | 0.42661671 | 0.43262222 | 0.43262222 | 5.5849e-05 \n",
      "   447 | 0.42658534 | 0.42658534 | 0.43246907 | 0.43246907 | 5.3806e-05 <-- new best val loss\n",
      "   448 | 0.42642515 | 0.42642515 | 0.43247097 | 0.43247097 | 5.1809e-05 \n",
      "   449 | 0.42634080 | 0.42634080 | 0.43224016 | 0.43224016 | 4.9838e-05 <-- new best val loss\n",
      "   450 | 0.42638629 | 0.42638629 | 0.43213798 | 0.43213798 | 4.7903e-05 <-- new best val loss\n",
      "   451 | 0.42618372 | 0.42618372 | 0.43215939 | 0.43215939 | 4.6004e-05 \n",
      "   452 | 0.42611042 | 0.42611042 | 0.43212394 | 0.43212394 | 4.4142e-05 <-- new best val loss\n",
      "   453 | 0.42608593 | 0.42608593 | 0.43207029 | 0.43207029 | 4.2317e-05 <-- new best val loss\n",
      "   454 | 0.42599411 | 0.42599411 | 0.43190960 | 0.43190960 | 4.0528e-05 <-- new best val loss\n",
      "   455 | 0.42597531 | 0.42597531 | 0.43190009 | 0.43190009 | 3.8777e-05 <-- new best val loss\n",
      "   456 | 0.42588405 | 0.42588405 | 0.43200167 | 0.43200167 | 3.7063e-05 \n",
      "   457 | 0.42595457 | 0.42595457 | 0.43162505 | 0.43162505 | 3.5386e-05 <-- new best val loss\n",
      "   458 | 0.42579264 | 0.42579264 | 0.43165051 | 0.43165051 | 3.3746e-05 \n",
      "   459 | 0.42560710 | 0.42560710 | 0.43171125 | 0.43171125 | 3.2144e-05 \n",
      "   460 | 0.42563835 | 0.42563835 | 0.43173026 | 0.43173026 | 3.058e-05  \n",
      "   461 | 0.42558548 | 0.42558548 | 0.43166934 | 0.43166934 | 2.9054e-05 \n",
      "   462 | 0.42549646 | 0.42549646 | 0.43174110 | 0.43174110 | 2.7565e-05 \n",
      "   463 | 0.42546971 | 0.42546971 | 0.43149729 | 0.43149729 | 2.6115e-05 <-- new best val loss\n",
      "   464 | 0.42540261 | 0.42540261 | 0.43135255 | 0.43135255 | 2.4703e-05 <-- new best val loss\n",
      "   465 | 0.42543369 | 0.42543369 | 0.43165945 | 0.43165945 | 2.3329e-05 \n",
      "   466 | 0.42525322 | 0.42525322 | 0.43154331 | 0.43154331 | 2.1993e-05 \n",
      "   467 | 0.42525820 | 0.42525820 | 0.43158713 | 0.43158713 | 2.0696e-05 \n",
      "   468 | 0.42518533 | 0.42518533 | 0.43130333 | 0.43130333 | 1.9438e-05 <-- new best val loss\n",
      "   469 | 0.42506423 | 0.42506423 | 0.43152964 | 0.43152964 | 1.8224e-05 \n",
      "   470 | 0.42516317 | 0.42516317 | 0.43145475 | 0.43145475 | 1.7043e-05 \n",
      "   471 | 0.42502070 | 0.42502070 | 0.43147203 | 0.43147203 | 1.5901e-05 \n",
      "   472 | 0.42495384 | 0.42495384 | 0.43129330 | 0.43129330 | 1.4798e-05 <-- new best val loss\n",
      "   473 | 0.42491593 | 0.42491593 | 0.43151640 | 0.43151640 | 1.3734e-05 \n",
      "   474 | 0.42482439 | 0.42482439 | 0.43125955 | 0.43125955 | 1.2709e-05 <-- new best val loss\n",
      "   475 | 0.42499696 | 0.42499696 | 0.43116684 | 0.43116684 | 1.1723e-05 <-- new best val loss\n",
      "   476 | 0.42486115 | 0.42486115 | 0.43152222 | 0.43152222 | 1.0777e-05 \n",
      "   477 | 0.42482323 | 0.42482323 | 0.43166326 | 0.43166326 | 9.8699e-06 \n",
      "   478 | 0.42475769 | 0.42475769 | 0.43122097 | 0.43122097 | 9.0025e-06 \n",
      "   479 | 0.42485465 | 0.42485465 | 0.43115334 | 0.43115334 | 8.1788e-06 <-- new best val loss\n",
      "   480 | 0.42464506 | 0.42464506 | 0.43147810 | 0.43147810 | 7.3904e-06 \n",
      "   481 | 0.42470444 | 0.42470444 | 0.43111066 | 0.43111066 | 6.6417e-06 <-- new best val loss\n",
      "   482 | 0.42477181 | 0.42477181 | 0.43109212 | 0.43109212 | 5.9327e-06 <-- new best val loss\n",
      "   483 | 0.42453100 | 0.42453100 | 0.43111473 | 0.43111473 | 5.2635e-06 \n",
      "   484 | 0.42458177 | 0.42458177 | 0.43142892 | 0.43142892 | 4.6342e-06 \n",
      "   485 | 0.42457612 | 0.42457612 | 0.43106744 | 0.43106744 | 4.0448e-06 <-- new best val loss\n",
      "   486 | 0.42465751 | 0.42465751 | 0.43136548 | 0.43136548 | 3.4954e-06 \n",
      "   487 | 0.42448533 | 0.42448533 | 0.43117238 | 0.43117238 | 2.9859e-06 \n",
      "   488 | 0.42460311 | 0.42460311 | 0.43122437 | 0.43122437 | 2.5165e-06 \n",
      "   489 | 0.42445175 | 0.42445175 | 0.43103313 | 0.43103313 | 2.0872e-06 <-- new best val loss\n",
      "   490 | 0.42447430 | 0.42447430 | 0.43136056 | 0.43136056 | 1.698e-06  \n",
      "   491 | 0.42451328 | 0.42451328 | 0.43135289 | 0.43135289 | 1.3507e-06 \n",
      "   492 | 0.42447451 | 0.42447451 | 0.43115304 | 0.43115304 | 1.0416e-06 \n",
      "   493 | 0.42450521 | 0.42450521 | 0.43111931 | 0.43111931 | 7.7272e-07 \n",
      "   494 | 0.42458068 | 0.42458068 | 0.43110460 | 0.43110460 | 5.4406e-07 \n",
      "   495 | 0.42441895 | 0.42441895 | 0.43100262 | 0.43100262 | 3.5565e-07 <-- new best val loss\n",
      "   496 | 0.42444850 | 0.42444850 | 0.43107003 | 0.43107003 | 2.0749e-07 \n",
      "   497 | 0.42436014 | 0.42436014 | 0.43120642 | 0.43120642 | 9.9594e-08 \n",
      "   498 | 0.42451625 | 0.42451625 | 0.43100785 | 0.43100785 | 3.1978e-08 \n",
      "   499 | 0.42454015 | 0.42454015 | 0.43125096 | 0.43125096 | 4.6423e-09 \n"
     ]
    }
   ],
   "source": [
    "# Model Initialization\n",
    "model = AutoEncoder(inp_dim = sc_mix_d[\"train\"].shape[1], **autoencoder_kwargs)\n",
    "model = model.to(device)\n",
    "model.apply(initialize_weights)\n",
    "print(model)\n",
    "\n",
    "\n",
    "results_history_out = train(\n",
    "    model,\n",
    "    model_folder,\n",
    "    dataloader_source_train,\n",
    "    dataloader_source_val,\n",
    "    epochs=INITIAL_TRAIN_EPOCHS,\n",
    "    **adamw_kwargs\n",
    ")\n",
    "\n",
    "best_checkpoint = torch.load(os.path.join(model_folder, f\"final_model.pth\"))\n",
    "best_epoch = best_checkpoint[\"epoch\"]\n",
    "\n",
    "train_history_running = results_history_out[\"train\"][\"running\"]\n",
    "val_history = results_history_out[\"val\"][\"avg\"]\n",
    "plot_results(train_history_running, val_history, best_epoch, save_folder=model_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T01:32:31.874439Z",
     "iopub.status.busy": "2022-12-14T01:32:31.874218Z",
     "iopub.status.idle": "2022-12-14T01:32:32.790660Z",
     "shell.execute_reply": "2022-12-14T01:32:32.789655Z"
    }
   },
   "outputs": [],
   "source": [
    "model = best_checkpoint[\"model\"]\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    avg_results_train, running_results_train = run_epoch(dataloader_source_train, model)\n",
    "    avg_results_val, running_results_val = run_epoch(dataloader_source_val, model)\n",
    "    avg_results_test, running_results_test = run_epoch(dataloader_source_test, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000304"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.std(sc_mix_d[\"train\"], axis=0) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T01:32:32.795972Z",
     "iopub.status.busy": "2022-12-14T01:32:32.795756Z",
     "iopub.status.idle": "2022-12-14T01:32:32.974176Z",
     "shell.execute_reply": "2022-12-14T01:32:32.973683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             loss       mse        bs\n",
      "train 0  0.389375  0.389375  511.4368\n",
      "val   0  0.431003  0.431003  506.9120\n",
      "test  0  0.422636  0.422636  506.9120\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    pd.concat(\n",
    "        [\n",
    "            pd.Series(avg_results_train).to_frame().T,\n",
    "            pd.Series(avg_results_val).to_frame().T,\n",
    "            pd.Series(avg_results_test).to_frame().T,\n",
    "        ],\n",
    "        keys=[\"train\", \"val\", \"test\"],\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('agreda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c8a91b640e5c43bacdcbf87782ad770b561ed71a46153862bbc20bda0bebf44f"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "05036004ba764a4e96bc25846d732f66": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1b9be84e19b64b2d9ffb6e234b7e386b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b88f993a2cd5498abdb0307be37984be",
       "placeholder": "​",
       "style": "IPY_MODEL_256cb45cc7494b16a578df1527ed374d",
       "value": "Epochs: 100%"
      }
     },
     "1dca961603174ea09bba4982d916fafb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1f441e546df54a558e1e3aebe1813e19",
       "max": 200,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c061781ace784a7dbb7456055d80e84e",
       "value": 200
      }
     },
     "1f441e546df54a558e1e3aebe1813e19": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "256cb45cc7494b16a578df1527ed374d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "307c0c01330a48f2969d66d7f9ba3445": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "32c8a5e48cfe42d3a5e846a71a8c8365": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4650c8367dcc42c2b7f9d9e1ecd2bd22": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4faf1a3e8aed4153952d7da4a81e0ea6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_32c8a5e48cfe42d3a5e846a71a8c8365",
       "placeholder": "​",
       "style": "IPY_MODEL_05036004ba764a4e96bc25846d732f66",
       "value": "Batch:  70%"
      }
     },
     "65958bd4f85242ba951e0cb57c1dd705": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4faf1a3e8aed4153952d7da4a81e0ea6",
        "IPY_MODEL_ded882eb46cd4a5b8d4e510a68eb2bcb",
        "IPY_MODEL_b1de2e2208424d118d50fa1b264819a9"
       ],
       "layout": "IPY_MODEL_a5b031b31b7e47e0a5b94a2de50b2282"
      }
     },
     "76dcd54c15af46cf8bf558a0eec12ca7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a5b031b31b7e47e0a5b94a2de50b2282": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ad0e514330704bdd997a07f229f4f26c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1b9be84e19b64b2d9ffb6e234b7e386b",
        "IPY_MODEL_1dca961603174ea09bba4982d916fafb",
        "IPY_MODEL_c8be6994fa504786b23d15658856ebe3"
       ],
       "layout": "IPY_MODEL_76dcd54c15af46cf8bf558a0eec12ca7"
      }
     },
     "b1de2e2208424d118d50fa1b264819a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4650c8367dcc42c2b7f9d9e1ecd2bd22",
       "placeholder": "​",
       "style": "IPY_MODEL_307c0c01330a48f2969d66d7f9ba3445",
       "value": " 28/40 [00:00&lt;00:00, 72.85it/s]"
      }
     },
     "b5a0f430b4e24ad483d7bd7216952d62": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b84cd75a178f4e6e9a82d83c11c7fd72": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b88f993a2cd5498abdb0307be37984be": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c061781ace784a7dbb7456055d80e84e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c1e326eb8fb7453e9708e90c00372428": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c8be6994fa504786b23d15658856ebe3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b5a0f430b4e24ad483d7bd7216952d62",
       "placeholder": "​",
       "style": "IPY_MODEL_f2c09b121549484f8038896c2aec431b",
       "value": " 200/200 [02:52&lt;00:00,  1.10it/s]"
      }
     },
     "ded882eb46cd4a5b8d4e510a68eb2bcb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b84cd75a178f4e6e9a82d83c11c7fd72",
       "max": 40,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c1e326eb8fb7453e9708e90c00372428",
       "value": 28
      }
     },
     "f2c09b121549484f8038896c2aec431b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
